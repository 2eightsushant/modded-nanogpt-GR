import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
import copy
import glob
from dataclasses import dataclass
from functools import lru_cache, partial # Added partial for hook registration
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
#torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params: list[Tensor] = [*params]
        param_groups = []
        for size in {p.numel() for p in params}:
            b = torch.empty(world_size, size, dtype=torch.bfloat16, device="cuda")
            group = dict(params=[p for p in params if p.numel() == size],
                         update_buffer=b, update_buffer_views=[b[i] for i in range(world_size)])
            param_groups.append(group)
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            update_buffer: Tensor = group["update_buffer"]
            update_buffer_views: list[Tensor] = group["update_buffer_views"]
            # generate weight updates in distributed fashion
            params: list[Tensor] = group["params"]
            handle = None
            params_world = None
            def update_prev(): # optimized Muon implementation contributed by @YouJiacheng
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffer_views):
                    p_world.add_(g_world.view_as(p_world),
                                 alpha=-group["lr"] * max(1, p_world.size(-2) / p_world.size(-1))**0.5)
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if "momentum_buffer" not in state:
                        state["momentum_buffer"] = torch.zeros_like(g)
                    buf: Tensor = state["momentum_buffer"]
                    buf.lerp_(g, 1 - group["momentum"])
                    g = g.lerp_(buf, group["momentum"]) if group["nesterov"] else buf
                    g = zeropower_via_newtonschulz5(g, steps=group["ns_steps"]).flatten()
                else:
                    g = update_buffer_views[self.rank]
                if base_i > 0:
                    update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=0.12).transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, block_mask: BlockMask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, max_seq_len, i) for i in range(num_layers)])
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128),
                                    use_fp8=True, x_s=(model_dim**0.5)/448, w_s=24/448, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        self.skip_weights = nn.Parameter(torch.ones(num_layers//2))

    def create_blockmasks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        docs = (input_seq == 50256).cumsum(0)

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_blockmask: Tensor):
            num_blocks = dense_blockmask.sum(dim=-1, dtype=torch.int32)
            indices = dense_blockmask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
        causal_blockmask_any = block_idx[:, None] >= block_idx
        causal_blockmask_all = block_idx[:, None] > block_idx
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
        document_blockmask_any = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
        document_blockmask_all = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
        blockmask_any = causal_blockmask_any & document_blockmask_any
        blockmask_all = causal_blockmask_all & document_blockmask_all
        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(blockmask_any & ~blockmask_all)
        full_kv_num_blocks, full_kv_indices = dense_to_ordered(blockmask_all)
        def build_bm(window_size_blocks: Tensor) -> BlockMask:
            return BlockMask.from_kv_blocks(
                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(window_size_blocks - full_kv_num_blocks, 1)),
                partial_kv_indices,
                torch.clamp_max(full_kv_num_blocks, window_size_blocks - 1),
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        long_bm, short_bm = self.create_blockmasks(input_seq, sliding_window_num_blocks)
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, long_bm, short_bm, short_bm, short_bm, long_bm]
        assert len(block_masks) == len(self.blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977

        # U-net design by @brendanh0gan
        skip_connections = []
        n = len(self.skip_weights)
        for i in range(len(self.blocks)):
            if i >= n:
                x = x + self.skip_weights[i - n] * skip_connections.pop()
            x = self.blocks[i](x, ve[i], x0, block_masks[i])
            if i < n:
                skip_connections.append(x)

        x = norm(x)
        logits = self.lm_head(x).float()
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / (7.5 * x.size(-1)**0.5))
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq, reduction='sum' if self.training else 'mean')
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn't helpful.
        pos += batch_size
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_seq_len = 48*1024 # FlexAttention sequence length
    val_seq_len = 4*64*1024 # FlexAttention sequence length for validation
    # optimization
    num_iterations = 1770 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    # architecture
    vocab_size = 50257
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert world_size == 8 # this code is designed for 8xH100
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs("logs_bucket", exist_ok=True)
    logfile = f"logs_bucket/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

########################################
#    Construct model and optimizer     #
########################################

model: nn.Module = GPT(vocab_size=args.vocab_size, num_layers=12, num_heads=6, model_dim=768,
                       max_seq_len=max(args.train_seq_len, args.val_seq_len)).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
adam_params = [dict(params=head_params, lr=0.22), dict(params=embed_params, lr=0.6), dict(params=scalar_params, lr=0.04)]
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = torch.optim.Adam(adam_params, betas=(0.8, 0.95), eps=1e-10, fused=True)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then decay
def get_lr(step: int):
    x = step / args.num_iterations # progress in training
    assert 0 <= x < 1
    if x < 1 - args.cooldown_frac:
        return 1.0
    else:
        w = (1 - x) / args.cooldown_frac
        return w * 1.0 + (1 - w) * 0.1

# attention window size schedule: linearly increase
@lru_cache(1)
def get_window_size_blocks_helper(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)
def get_window_size_blocks(step: int):
    x = step / args.num_iterations # progress in training
    assert 0 <= x <= 1
    # Linearly increase the block-wise sliding window size over training 128 -> 1792
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * x, n=128)
    return get_window_size_blocks_helper(window_size)

model: nn.Module = torch.compile(model, dynamic=False)

########################################
#     Warmup kernel and Layerwise Bucketing   #
########################################

# Track the gradient readiness order during warmup
gradient_ready_order = []

def record_grad_order_hook(param):
    def hook(*_):
        if param.grad is not None and all(param is not p for p in gradient_ready_order):
            gradient_ready_order.append(param)
    return hook

# Register temp hooks to track when each parameter's gradient is ready
tmp_handles = []
for p in model.parameters():
    if p.requires_grad:
        handle = p.register_post_accumulate_grad_hook(record_grad_order_hook(p))
        tmp_handles.append(handle)

warmup_steps = 10
initial_state = dict(
    model=copy.deepcopy(model.state_dict()),
    optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]
)

for step in range(warmup_steps):
    # Create a local generator with fixed, step-dependent seed
    g = torch.Generator(device="cuda")
    g.manual_seed(1337 + step)

    # Use this generator to sample inputs
    inputs = targets = torch.randint(
        low=0,
        high=args.vocab_size,
        size=(args.train_seq_len,),
        device="cuda",
        dtype=torch.long,
        generator=g,  # <- Local generator ensures deterministic and isolated RNG
    )
    model(inputs.to(torch.int32), targets, get_window_size_blocks(0)).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)

# Remove hooks after warmup
for h in tmp_handles:
    h.remove()

# Restore model/optimizer state to ensure a clean start
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del initial_state

print0(f"Collected {len(gradient_ready_order)} parameters in gradient-ready order")

########################################
#      Overlap Communication Setup     #
########################################

# Layer-aware bucketing using gradient-ready order
def create_buckets_layerwise(ordered_params, bucket_size_mb=25):
    """Group parameters into buckets of ~bucket_size_mb MB, in gradient ready order"""
    buckets = []
    current_bucket = []
    current_size = 0

    for param in ordered_params:
        if not param.requires_grad:
            continue
        param_size_mb = param.numel() * param.element_size() / (1024 * 1024)

        if current_size + param_size_mb > bucket_size_mb and current_bucket:
            buckets.append(current_bucket)
            current_bucket = [param]
            current_size = param_size_mb
        else:
            current_bucket.append(param)
            current_size += param_size_mb

    if current_bucket:
        buckets.append(current_bucket)

    return buckets

# Create buckets for all parameters
all_params = [p for p in model.parameters() if p.requires_grad]
param_buckets = create_buckets_layerwise(gradient_ready_order)
print0(f"Created {len(param_buckets)} gradient buckets")
for i, bucket in enumerate(param_buckets):
    total_size = sum(p.numel() * p.element_size() for p in bucket) / (1024 * 1024)
    print0(f"Bucket {i}: {len(bucket)} params, {total_size:.1f} MB")

# Bucket state tracking
bucket_ready_count = [0] * len(param_buckets)
bucket_handles = [None] * len(param_buckets)
param_to_bucket = {}

# Map each parameter to its bucket index
for bucket_idx, bucket in enumerate(param_buckets):
    for param in bucket:
        param_to_bucket[param] = bucket_idx

def _gradient_hook(param: Tensor):
    """Called when a parameter's gradient is ready"""
    if param.grad is None:
        return

    bucket_idx = param_to_bucket[param]
    bucket_ready_count[bucket_idx] += 1

    # Check if all parameters in this bucket are ready
    if bucket_ready_count[bucket_idx] == len(param_buckets[bucket_idx]):
        # All-reduce this bucket
        bucket_grads = [p.grad for p in param_buckets[bucket_idx]]

        # For multi-tensor operations, we can reduce them together
        if len(bucket_grads) == 1:
            handle = dist.all_reduce(bucket_grads[0], op=dist.ReduceOp.AVG, async_op=True)
        else:
            # Use multi-tensor all-reduce for efficiency
            handle = dist.all_reduce_coalesced(bucket_grads, op=dist.ReduceOp.AVG, async_op=True)

        bucket_handles[bucket_idx] = handle

# Register hooks for all parameters
print0("Registering bucketed gradient hooks...")
for param in all_params:
    param.register_post_accumulate_grad_hook(_gradient_hook)

def wait_for_gradients():
    """Wait for all gradient reductions to complete and reset bucket state"""
    for handle in bucket_handles:
        if handle is not None:
            handle.wait()

    # Reset state for next iteration
    for i in range(len(bucket_ready_count)):
        bucket_ready_count[i] = 0
        bucket_handles[i] = None

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, world_size * args.train_seq_len, rank, world_size)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_batch_size = world_size * args.val_seq_len
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        val_loader = distributed_data_generator(args.val_files, val_batch_size, rank, world_size)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets = next(val_loader)
                val_loss += model(inputs, targets, get_window_size_blocks(step))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs_bucket/{run_id}", exist_ok=True)
            torch.save(log, f"logs_bucket/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    inputs, targets = next(train_loader)
    model(inputs, targets, get_window_size_blocks(step)).backward()
    #for param in model.parameters():
    #    dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
    wait_for_gradients() # does the same thing as commented two lines above, but faster

    # set optimization hyperparameters
    for opt in optimizers:
        for group in opt.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)
    for group in optimizer2.param_groups:
        frac = min(step / 300, 1) # momentum warmup for muon
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers
    for opt in optimizers:
        opt.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Feb  4 2025, 14:57:36) [GCC 11.4.0]
Running PyTorch 2.8.0.dev20250612+cu126 compiled for CUDA 12.6
Fri Jun 13 04:49:01 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.08             Driver Version: 550.127.08     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   40C    P0            127W /  700W |    5824MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   33C    P0            125W /  700W |    1514MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   31C    P0            121W /  700W |    1514MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   37C    P0            123W /  700W |    1514MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   38C    P0            120W /  700W |    1514MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   31C    P0            119W /  700W |    1514MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   37C    P0            124W /  700W |    1514MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   30C    P0            114W /  700W |    1514MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
Collected 75 parameters in gradient-ready order
Created 21 gradient buckets
Bucket 0: 1 params, 73.6 MB
Bucket 1: 1 params, 73.6 MB
Bucket 2: 4 params, 15.8 MB
Bucket 3: 1 params, 147.4 MB
Bucket 4: 4 params, 20.3 MB
Bucket 5: 4 params, 24.8 MB
Bucket 6: 4 params, 18.0 MB
Bucket 7: 5 params, 18.0 MB
Bucket 8: 4 params, 20.3 MB
Bucket 9: 5 params, 24.8 MB
Bucket 10: 4 params, 20.3 MB
Bucket 11: 4 params, 24.8 MB
Bucket 12: 5 params, 18.0 MB
Bucket 13: 5 params, 20.3 MB
Bucket 14: 5 params, 18.0 MB
Bucket 15: 4 params, 20.3 MB
Bucket 16: 5 params, 24.8 MB
Bucket 17: 4 params, 24.8 MB
Bucket 18: 4 params, 2.3 MB
Bucket 19: 1 params, 73.6 MB
Bucket 20: 1 params, 73.6 MB
Registering bucketed gradient hooks...
step:0/1770 val_loss:10.8258 train_time:0ms step_avg:0.02ms
step:1/1770 train_time:65ms step_avg:64.94ms
step:2/1770 train_time:140ms step_avg:70.09ms
step:3/1770 train_time:228ms step_avg:76.12ms
step:4/1770 train_time:321ms step_avg:80.35ms
step:5/1770 train_time:415ms step_avg:83.04ms
step:6/1770 train_time:509ms step_avg:84.85ms
step:7/1770 train_time:603ms step_avg:86.17ms
step:8/1770 train_time:697ms step_avg:87.14ms
step:9/1770 train_time:792ms step_avg:87.95ms
step:10/1770 train_time:886ms step_avg:88.57ms
step:11/1770 train_time:980ms step_avg:89.06ms
step:12/1770 train_time:1077ms step_avg:89.77ms
step:13/1770 train_time:1174ms step_avg:90.32ms
step:14/1770 train_time:1270ms step_avg:90.69ms
step:15/1770 train_time:1364ms step_avg:90.92ms
step:16/1770 train_time:1458ms step_avg:91.15ms
step:17/1770 train_time:1552ms step_avg:91.32ms
step:18/1770 train_time:1646ms step_avg:91.46ms
step:19/1770 train_time:1740ms step_avg:91.59ms
step:20/1770 train_time:1835ms step_avg:91.75ms
step:21/1770 train_time:1930ms step_avg:91.88ms
step:22/1770 train_time:2023ms step_avg:91.98ms
step:23/1770 train_time:2119ms step_avg:92.13ms
step:24/1770 train_time:2214ms step_avg:92.27ms
step:25/1770 train_time:2309ms step_avg:92.37ms
step:26/1770 train_time:2404ms step_avg:92.46ms
step:27/1770 train_time:2498ms step_avg:92.53ms
step:28/1770 train_time:2593ms step_avg:92.61ms
step:29/1770 train_time:2688ms step_avg:92.68ms
step:30/1770 train_time:2782ms step_avg:92.74ms
step:31/1770 train_time:2876ms step_avg:92.78ms
step:32/1770 train_time:2971ms step_avg:92.84ms
step:33/1770 train_time:3066ms step_avg:92.91ms
step:34/1770 train_time:3161ms step_avg:92.96ms
step:35/1770 train_time:3256ms step_avg:93.01ms
step:36/1770 train_time:3351ms step_avg:93.08ms
step:37/1770 train_time:3446ms step_avg:93.13ms
step:38/1770 train_time:3540ms step_avg:93.15ms
step:39/1770 train_time:3634ms step_avg:93.18ms
step:40/1770 train_time:3729ms step_avg:93.22ms
step:41/1770 train_time:3823ms step_avg:93.25ms
step:42/1770 train_time:3918ms step_avg:93.28ms
step:43/1770 train_time:4013ms step_avg:93.32ms
step:44/1770 train_time:4108ms step_avg:93.37ms
step:45/1770 train_time:4202ms step_avg:93.39ms
step:46/1770 train_time:4297ms step_avg:93.41ms
step:47/1770 train_time:4393ms step_avg:93.46ms
step:48/1770 train_time:4488ms step_avg:93.50ms
step:49/1770 train_time:4582ms step_avg:93.52ms
step:50/1770 train_time:4677ms step_avg:93.53ms
step:51/1770 train_time:4772ms step_avg:93.57ms
step:52/1770 train_time:4867ms step_avg:93.59ms
step:53/1770 train_time:4962ms step_avg:93.62ms
step:54/1770 train_time:5056ms step_avg:93.63ms
step:55/1770 train_time:5151ms step_avg:93.65ms
step:56/1770 train_time:5244ms step_avg:93.65ms
step:57/1770 train_time:5338ms step_avg:93.65ms
step:58/1770 train_time:5433ms step_avg:93.67ms
step:59/1770 train_time:5527ms step_avg:93.68ms
step:60/1770 train_time:5621ms step_avg:93.69ms
step:61/1770 train_time:5716ms step_avg:93.70ms
step:62/1770 train_time:5811ms step_avg:93.72ms
step:63/1770 train_time:5906ms step_avg:93.74ms
step:64/1770 train_time:6000ms step_avg:93.74ms
step:65/1770 train_time:6094ms step_avg:93.76ms
step:66/1770 train_time:6188ms step_avg:93.76ms
step:67/1770 train_time:6283ms step_avg:93.78ms
step:68/1770 train_time:6378ms step_avg:93.80ms
step:69/1770 train_time:6473ms step_avg:93.81ms
step:70/1770 train_time:6569ms step_avg:93.84ms
step:71/1770 train_time:6663ms step_avg:93.85ms
step:72/1770 train_time:6758ms step_avg:93.87ms
step:73/1770 train_time:6854ms step_avg:93.89ms
step:74/1770 train_time:6948ms step_avg:93.89ms
step:75/1770 train_time:7042ms step_avg:93.90ms
step:76/1770 train_time:7137ms step_avg:93.90ms
step:77/1770 train_time:7231ms step_avg:93.91ms
step:78/1770 train_time:7326ms step_avg:93.92ms
step:79/1770 train_time:7421ms step_avg:93.94ms
step:80/1770 train_time:7516ms step_avg:93.95ms
step:81/1770 train_time:7611ms step_avg:93.96ms
step:82/1770 train_time:7706ms step_avg:93.97ms
step:83/1770 train_time:7801ms step_avg:93.99ms
step:84/1770 train_time:7895ms step_avg:93.99ms
step:85/1770 train_time:7990ms step_avg:94.00ms
step:86/1770 train_time:8085ms step_avg:94.01ms
step:87/1770 train_time:8178ms step_avg:94.00ms
step:88/1770 train_time:8273ms step_avg:94.01ms
step:89/1770 train_time:8368ms step_avg:94.02ms
step:90/1770 train_time:8463ms step_avg:94.03ms
step:91/1770 train_time:8557ms step_avg:94.03ms
step:92/1770 train_time:8652ms step_avg:94.04ms
step:93/1770 train_time:8747ms step_avg:94.06ms
step:94/1770 train_time:8842ms step_avg:94.06ms
step:95/1770 train_time:8936ms step_avg:94.07ms
step:96/1770 train_time:9031ms step_avg:94.07ms
step:97/1770 train_time:9126ms step_avg:94.08ms
step:98/1770 train_time:9220ms step_avg:94.08ms
step:99/1770 train_time:9314ms step_avg:94.09ms
step:100/1770 train_time:9409ms step_avg:94.09ms
step:101/1770 train_time:9503ms step_avg:94.09ms
step:102/1770 train_time:9597ms step_avg:94.09ms
step:103/1770 train_time:9692ms step_avg:94.10ms
step:104/1770 train_time:9787ms step_avg:94.11ms
step:105/1770 train_time:9882ms step_avg:94.12ms
step:106/1770 train_time:9977ms step_avg:94.12ms
step:107/1770 train_time:10071ms step_avg:94.12ms
step:108/1770 train_time:10166ms step_avg:94.13ms
step:109/1770 train_time:10260ms step_avg:94.13ms
step:110/1770 train_time:10355ms step_avg:94.14ms
step:111/1770 train_time:10450ms step_avg:94.15ms
step:112/1770 train_time:10544ms step_avg:94.15ms
step:113/1770 train_time:10638ms step_avg:94.14ms
step:114/1770 train_time:10733ms step_avg:94.14ms
step:115/1770 train_time:10829ms step_avg:94.16ms
step:116/1770 train_time:10923ms step_avg:94.17ms
step:117/1770 train_time:11019ms step_avg:94.18ms
step:118/1770 train_time:11113ms step_avg:94.18ms
step:119/1770 train_time:11207ms step_avg:94.18ms
step:120/1770 train_time:11302ms step_avg:94.18ms
step:121/1770 train_time:11396ms step_avg:94.18ms
step:122/1770 train_time:11492ms step_avg:94.20ms
step:123/1770 train_time:11587ms step_avg:94.20ms
step:124/1770 train_time:11681ms step_avg:94.20ms
step:125/1770 train_time:11775ms step_avg:94.20ms
step:125/1770 val_loss:4.6432 train_time:11869ms step_avg:94.96ms
step:126/1770 train_time:11889ms step_avg:94.36ms
step:127/1770 train_time:11972ms step_avg:94.27ms
step:128/1770 train_time:12073ms step_avg:94.32ms
step:129/1770 train_time:12170ms step_avg:94.34ms
step:130/1770 train_time:12264ms step_avg:94.34ms
step:131/1770 train_time:12358ms step_avg:94.34ms
step:132/1770 train_time:12451ms step_avg:94.33ms
step:133/1770 train_time:12545ms step_avg:94.32ms
step:134/1770 train_time:12639ms step_avg:94.32ms
step:135/1770 train_time:12733ms step_avg:94.32ms
step:136/1770 train_time:12828ms step_avg:94.32ms
step:137/1770 train_time:12925ms step_avg:94.35ms
step:138/1770 train_time:13022ms step_avg:94.36ms
step:139/1770 train_time:13119ms step_avg:94.38ms
step:140/1770 train_time:13214ms step_avg:94.39ms
step:141/1770 train_time:13310ms step_avg:94.40ms
step:142/1770 train_time:13405ms step_avg:94.40ms
step:143/1770 train_time:13499ms step_avg:94.40ms
step:144/1770 train_time:13595ms step_avg:94.41ms
step:145/1770 train_time:13690ms step_avg:94.41ms
step:146/1770 train_time:13784ms step_avg:94.41ms
step:147/1770 train_time:13879ms step_avg:94.42ms
step:148/1770 train_time:13975ms step_avg:94.43ms
step:149/1770 train_time:14071ms step_avg:94.43ms
step:150/1770 train_time:14167ms step_avg:94.44ms
step:151/1770 train_time:14262ms step_avg:94.45ms
step:152/1770 train_time:14357ms step_avg:94.46ms
step:153/1770 train_time:14452ms step_avg:94.46ms
step:154/1770 train_time:14548ms step_avg:94.47ms
step:155/1770 train_time:14643ms step_avg:94.47ms
step:156/1770 train_time:14738ms step_avg:94.47ms
step:157/1770 train_time:14833ms step_avg:94.48ms
step:158/1770 train_time:14928ms step_avg:94.48ms
step:159/1770 train_time:15024ms step_avg:94.49ms
step:160/1770 train_time:15119ms step_avg:94.49ms
step:161/1770 train_time:15215ms step_avg:94.50ms
step:162/1770 train_time:15310ms step_avg:94.51ms
step:163/1770 train_time:15405ms step_avg:94.51ms
step:164/1770 train_time:15500ms step_avg:94.51ms
step:165/1770 train_time:15595ms step_avg:94.52ms
step:166/1770 train_time:15691ms step_avg:94.52ms
step:167/1770 train_time:15787ms step_avg:94.53ms
step:168/1770 train_time:15882ms step_avg:94.53ms
step:169/1770 train_time:15976ms step_avg:94.53ms
step:170/1770 train_time:16072ms step_avg:94.54ms
step:171/1770 train_time:16168ms step_avg:94.55ms
step:172/1770 train_time:16263ms step_avg:94.55ms
step:173/1770 train_time:16357ms step_avg:94.55ms
step:174/1770 train_time:16453ms step_avg:94.56ms
step:175/1770 train_time:16548ms step_avg:94.56ms
step:176/1770 train_time:16643ms step_avg:94.56ms
step:177/1770 train_time:16738ms step_avg:94.57ms
step:178/1770 train_time:16834ms step_avg:94.57ms
step:179/1770 train_time:16930ms step_avg:94.58ms
step:180/1770 train_time:17025ms step_avg:94.58ms
step:181/1770 train_time:17120ms step_avg:94.58ms
step:182/1770 train_time:17216ms step_avg:94.59ms
step:183/1770 train_time:17311ms step_avg:94.60ms
step:184/1770 train_time:17406ms step_avg:94.60ms
step:185/1770 train_time:17501ms step_avg:94.60ms
step:186/1770 train_time:17596ms step_avg:94.60ms
step:187/1770 train_time:17691ms step_avg:94.61ms
step:188/1770 train_time:17786ms step_avg:94.61ms
step:189/1770 train_time:17881ms step_avg:94.61ms
step:190/1770 train_time:17976ms step_avg:94.61ms
step:191/1770 train_time:18072ms step_avg:94.62ms
step:192/1770 train_time:18167ms step_avg:94.62ms
step:193/1770 train_time:18263ms step_avg:94.63ms
step:194/1770 train_time:18358ms step_avg:94.63ms
step:195/1770 train_time:18453ms step_avg:94.63ms
step:196/1770 train_time:18549ms step_avg:94.64ms
step:197/1770 train_time:18644ms step_avg:94.64ms
step:198/1770 train_time:18740ms step_avg:94.64ms
step:199/1770 train_time:18834ms step_avg:94.64ms
step:200/1770 train_time:18930ms step_avg:94.65ms
step:201/1770 train_time:19025ms step_avg:94.65ms
step:202/1770 train_time:19120ms step_avg:94.65ms
step:203/1770 train_time:19216ms step_avg:94.66ms
step:204/1770 train_time:19311ms step_avg:94.66ms
step:205/1770 train_time:19407ms step_avg:94.67ms
step:206/1770 train_time:19502ms step_avg:94.67ms
step:207/1770 train_time:19597ms step_avg:94.67ms
step:208/1770 train_time:19692ms step_avg:94.67ms
step:209/1770 train_time:19787ms step_avg:94.67ms
step:210/1770 train_time:19882ms step_avg:94.67ms
step:211/1770 train_time:19977ms step_avg:94.68ms
step:212/1770 train_time:20072ms step_avg:94.68ms
step:213/1770 train_time:20168ms step_avg:94.69ms
step:214/1770 train_time:20263ms step_avg:94.69ms
step:215/1770 train_time:20359ms step_avg:94.69ms
step:216/1770 train_time:20454ms step_avg:94.70ms
step:217/1770 train_time:20550ms step_avg:94.70ms
step:218/1770 train_time:20645ms step_avg:94.70ms
step:219/1770 train_time:20740ms step_avg:94.70ms
step:220/1770 train_time:20835ms step_avg:94.70ms
step:221/1770 train_time:20931ms step_avg:94.71ms
step:222/1770 train_time:21026ms step_avg:94.71ms
step:223/1770 train_time:21121ms step_avg:94.71ms
step:224/1770 train_time:21216ms step_avg:94.72ms
step:225/1770 train_time:21312ms step_avg:94.72ms
step:226/1770 train_time:21408ms step_avg:94.72ms
step:227/1770 train_time:21503ms step_avg:94.73ms
step:228/1770 train_time:21598ms step_avg:94.73ms
step:229/1770 train_time:21693ms step_avg:94.73ms
step:230/1770 train_time:21789ms step_avg:94.74ms
step:231/1770 train_time:21885ms step_avg:94.74ms
step:232/1770 train_time:21980ms step_avg:94.74ms
step:233/1770 train_time:22074ms step_avg:94.74ms
step:234/1770 train_time:22170ms step_avg:94.74ms
step:235/1770 train_time:22266ms step_avg:94.75ms
step:236/1770 train_time:22361ms step_avg:94.75ms
step:237/1770 train_time:22457ms step_avg:94.75ms
step:238/1770 train_time:22553ms step_avg:94.76ms
step:239/1770 train_time:22649ms step_avg:94.77ms
step:240/1770 train_time:22745ms step_avg:94.77ms
step:241/1770 train_time:22840ms step_avg:94.77ms
step:242/1770 train_time:22935ms step_avg:94.77ms
step:243/1770 train_time:23031ms step_avg:94.78ms
step:244/1770 train_time:23126ms step_avg:94.78ms
step:245/1770 train_time:23222ms step_avg:94.78ms
step:246/1770 train_time:23316ms step_avg:94.78ms
step:247/1770 train_time:23412ms step_avg:94.79ms
step:248/1770 train_time:23508ms step_avg:94.79ms
step:249/1770 train_time:23603ms step_avg:94.79ms
step:250/1770 train_time:23698ms step_avg:94.79ms
step:250/1770 val_loss:4.1085 train_time:23792ms step_avg:95.17ms
step:251/1770 train_time:23810ms step_avg:94.86ms
step:252/1770 train_time:23896ms step_avg:94.83ms
step:253/1770 train_time:23997ms step_avg:94.85ms
step:254/1770 train_time:24092ms step_avg:94.85ms
step:255/1770 train_time:24188ms step_avg:94.86ms
step:256/1770 train_time:24284ms step_avg:94.86ms
step:257/1770 train_time:24379ms step_avg:94.86ms
step:258/1770 train_time:24473ms step_avg:94.86ms
step:259/1770 train_time:24568ms step_avg:94.86ms
step:260/1770 train_time:24662ms step_avg:94.85ms
step:261/1770 train_time:24756ms step_avg:94.85ms
step:262/1770 train_time:24852ms step_avg:94.86ms
step:263/1770 train_time:24949ms step_avg:94.86ms
step:264/1770 train_time:25046ms step_avg:94.87ms
step:265/1770 train_time:25143ms step_avg:94.88ms
step:266/1770 train_time:25238ms step_avg:94.88ms
step:267/1770 train_time:25333ms step_avg:94.88ms
step:268/1770 train_time:25429ms step_avg:94.89ms
step:269/1770 train_time:25525ms step_avg:94.89ms
step:270/1770 train_time:25620ms step_avg:94.89ms
step:271/1770 train_time:25715ms step_avg:94.89ms
step:272/1770 train_time:25812ms step_avg:94.90ms
step:273/1770 train_time:25908ms step_avg:94.90ms
step:274/1770 train_time:26005ms step_avg:94.91ms
step:275/1770 train_time:26101ms step_avg:94.91ms
step:276/1770 train_time:26197ms step_avg:94.92ms
step:277/1770 train_time:26293ms step_avg:94.92ms
step:278/1770 train_time:26388ms step_avg:94.92ms
step:279/1770 train_time:26484ms step_avg:94.92ms
step:280/1770 train_time:26579ms step_avg:94.93ms
step:281/1770 train_time:26674ms step_avg:94.93ms
step:282/1770 train_time:26770ms step_avg:94.93ms
step:283/1770 train_time:26866ms step_avg:94.93ms
step:284/1770 train_time:26962ms step_avg:94.94ms
step:285/1770 train_time:27058ms step_avg:94.94ms
step:286/1770 train_time:27154ms step_avg:94.94ms
step:287/1770 train_time:27250ms step_avg:94.95ms
step:288/1770 train_time:27346ms step_avg:94.95ms
step:289/1770 train_time:27442ms step_avg:94.95ms
step:290/1770 train_time:27538ms step_avg:94.96ms
step:291/1770 train_time:27633ms step_avg:94.96ms
step:292/1770 train_time:27729ms step_avg:94.96ms
step:293/1770 train_time:27824ms step_avg:94.96ms
step:294/1770 train_time:27919ms step_avg:94.96ms
step:295/1770 train_time:28015ms step_avg:94.97ms
step:296/1770 train_time:28111ms step_avg:94.97ms
step:297/1770 train_time:28207ms step_avg:94.97ms
step:298/1770 train_time:28302ms step_avg:94.97ms
step:299/1770 train_time:28398ms step_avg:94.98ms
step:300/1770 train_time:28493ms step_avg:94.98ms
step:301/1770 train_time:28589ms step_avg:94.98ms
step:302/1770 train_time:28685ms step_avg:94.98ms
step:303/1770 train_time:28781ms step_avg:94.99ms
step:304/1770 train_time:28876ms step_avg:94.99ms
step:305/1770 train_time:28972ms step_avg:94.99ms
step:306/1770 train_time:29068ms step_avg:94.99ms
step:307/1770 train_time:29165ms step_avg:95.00ms
step:308/1770 train_time:29261ms step_avg:95.00ms
step:309/1770 train_time:29356ms step_avg:95.00ms
step:310/1770 train_time:29451ms step_avg:95.00ms
step:311/1770 train_time:29547ms step_avg:95.01ms
step:312/1770 train_time:29642ms step_avg:95.01ms
step:313/1770 train_time:29738ms step_avg:95.01ms
step:314/1770 train_time:29834ms step_avg:95.01ms
step:315/1770 train_time:29929ms step_avg:95.01ms
step:316/1770 train_time:30026ms step_avg:95.02ms
step:317/1770 train_time:30121ms step_avg:95.02ms
step:318/1770 train_time:30217ms step_avg:95.02ms
step:319/1770 train_time:30313ms step_avg:95.02ms
step:320/1770 train_time:30409ms step_avg:95.03ms
step:321/1770 train_time:30506ms step_avg:95.03ms
step:322/1770 train_time:30602ms step_avg:95.04ms
step:323/1770 train_time:30697ms step_avg:95.04ms
step:324/1770 train_time:30793ms step_avg:95.04ms
step:325/1770 train_time:30889ms step_avg:95.04ms
step:326/1770 train_time:30984ms step_avg:95.04ms
step:327/1770 train_time:31081ms step_avg:95.05ms
step:328/1770 train_time:31176ms step_avg:95.05ms
step:329/1770 train_time:31271ms step_avg:95.05ms
step:330/1770 train_time:31368ms step_avg:95.05ms
step:331/1770 train_time:31463ms step_avg:95.05ms
step:332/1770 train_time:31558ms step_avg:95.05ms
step:333/1770 train_time:31654ms step_avg:95.06ms
step:334/1770 train_time:31749ms step_avg:95.06ms
step:335/1770 train_time:31845ms step_avg:95.06ms
step:336/1770 train_time:31941ms step_avg:95.06ms
step:337/1770 train_time:32038ms step_avg:95.07ms
step:338/1770 train_time:32134ms step_avg:95.07ms
step:339/1770 train_time:32230ms step_avg:95.07ms
step:340/1770 train_time:32325ms step_avg:95.07ms
step:341/1770 train_time:32421ms step_avg:95.07ms
step:342/1770 train_time:32516ms step_avg:95.08ms
step:343/1770 train_time:32611ms step_avg:95.08ms
step:344/1770 train_time:32708ms step_avg:95.08ms
step:345/1770 train_time:32803ms step_avg:95.08ms
step:346/1770 train_time:32898ms step_avg:95.08ms
step:347/1770 train_time:32993ms step_avg:95.08ms
step:348/1770 train_time:33089ms step_avg:95.08ms
step:349/1770 train_time:33186ms step_avg:95.09ms
step:350/1770 train_time:33282ms step_avg:95.09ms
step:351/1770 train_time:33378ms step_avg:95.09ms
step:352/1770 train_time:33473ms step_avg:95.09ms
step:353/1770 train_time:33569ms step_avg:95.10ms
step:354/1770 train_time:33665ms step_avg:95.10ms
step:355/1770 train_time:33761ms step_avg:95.10ms
step:356/1770 train_time:33857ms step_avg:95.10ms
step:357/1770 train_time:33953ms step_avg:95.11ms
step:358/1770 train_time:34049ms step_avg:95.11ms
step:359/1770 train_time:34146ms step_avg:95.11ms
step:360/1770 train_time:34242ms step_avg:95.12ms
step:361/1770 train_time:34337ms step_avg:95.12ms
step:362/1770 train_time:34433ms step_avg:95.12ms
step:363/1770 train_time:34529ms step_avg:95.12ms
step:364/1770 train_time:34624ms step_avg:95.12ms
step:365/1770 train_time:34720ms step_avg:95.12ms
step:366/1770 train_time:34815ms step_avg:95.12ms
step:367/1770 train_time:34912ms step_avg:95.13ms
step:368/1770 train_time:35008ms step_avg:95.13ms
step:369/1770 train_time:35103ms step_avg:95.13ms
step:370/1770 train_time:35199ms step_avg:95.13ms
step:371/1770 train_time:35294ms step_avg:95.13ms
step:372/1770 train_time:35390ms step_avg:95.13ms
step:373/1770 train_time:35486ms step_avg:95.14ms
step:374/1770 train_time:35582ms step_avg:95.14ms
step:375/1770 train_time:35677ms step_avg:95.14ms
step:375/1770 val_loss:3.8965 train_time:35772ms step_avg:95.39ms
step:376/1770 train_time:35790ms step_avg:95.19ms
step:377/1770 train_time:35876ms step_avg:95.16ms
step:378/1770 train_time:35973ms step_avg:95.17ms
step:379/1770 train_time:36069ms step_avg:95.17ms
step:380/1770 train_time:36164ms step_avg:95.17ms
step:381/1770 train_time:36258ms step_avg:95.17ms
step:382/1770 train_time:36353ms step_avg:95.17ms
step:383/1770 train_time:36449ms step_avg:95.17ms
step:384/1770 train_time:36544ms step_avg:95.17ms
step:385/1770 train_time:36639ms step_avg:95.17ms
step:386/1770 train_time:36734ms step_avg:95.17ms
step:387/1770 train_time:36831ms step_avg:95.17ms
step:388/1770 train_time:36928ms step_avg:95.18ms
step:389/1770 train_time:37024ms step_avg:95.18ms
step:390/1770 train_time:37120ms step_avg:95.18ms
step:391/1770 train_time:37215ms step_avg:95.18ms
step:392/1770 train_time:37311ms step_avg:95.18ms
step:393/1770 train_time:37406ms step_avg:95.18ms
step:394/1770 train_time:37501ms step_avg:95.18ms
step:395/1770 train_time:37596ms step_avg:95.18ms
step:396/1770 train_time:37694ms step_avg:95.19ms
step:397/1770 train_time:37793ms step_avg:95.20ms
step:398/1770 train_time:37892ms step_avg:95.21ms
step:399/1770 train_time:37990ms step_avg:95.21ms
step:400/1770 train_time:38089ms step_avg:95.22ms
step:401/1770 train_time:38187ms step_avg:95.23ms
step:402/1770 train_time:38284ms step_avg:95.23ms
step:403/1770 train_time:38382ms step_avg:95.24ms
step:404/1770 train_time:38479ms step_avg:95.24ms
step:405/1770 train_time:38576ms step_avg:95.25ms
step:406/1770 train_time:38674ms step_avg:95.26ms
step:407/1770 train_time:38772ms step_avg:95.26ms
step:408/1770 train_time:38870ms step_avg:95.27ms
step:409/1770 train_time:38968ms step_avg:95.28ms
step:410/1770 train_time:39066ms step_avg:95.28ms
step:411/1770 train_time:39164ms step_avg:95.29ms
step:412/1770 train_time:39262ms step_avg:95.30ms
step:413/1770 train_time:39360ms step_avg:95.30ms
step:414/1770 train_time:39458ms step_avg:95.31ms
step:415/1770 train_time:39556ms step_avg:95.31ms
step:416/1770 train_time:39654ms step_avg:95.32ms
step:417/1770 train_time:39751ms step_avg:95.33ms
step:418/1770 train_time:39849ms step_avg:95.33ms
step:419/1770 train_time:39947ms step_avg:95.34ms
step:420/1770 train_time:40045ms step_avg:95.35ms
step:421/1770 train_time:40143ms step_avg:95.35ms
step:422/1770 train_time:40241ms step_avg:95.36ms
step:423/1770 train_time:40338ms step_avg:95.36ms
step:424/1770 train_time:40436ms step_avg:95.37ms
step:425/1770 train_time:40533ms step_avg:95.37ms
step:426/1770 train_time:40631ms step_avg:95.38ms
step:427/1770 train_time:40729ms step_avg:95.38ms
step:428/1770 train_time:40826ms step_avg:95.39ms
step:429/1770 train_time:40924ms step_avg:95.39ms
step:430/1770 train_time:41022ms step_avg:95.40ms
step:431/1770 train_time:41120ms step_avg:95.41ms
step:432/1770 train_time:41218ms step_avg:95.41ms
step:433/1770 train_time:41316ms step_avg:95.42ms
step:434/1770 train_time:41414ms step_avg:95.42ms
step:435/1770 train_time:41512ms step_avg:95.43ms
step:436/1770 train_time:41609ms step_avg:95.43ms
step:437/1770 train_time:41706ms step_avg:95.44ms
step:438/1770 train_time:41804ms step_avg:95.44ms
step:439/1770 train_time:41902ms step_avg:95.45ms
step:440/1770 train_time:42000ms step_avg:95.45ms
step:441/1770 train_time:42098ms step_avg:95.46ms
step:442/1770 train_time:42196ms step_avg:95.47ms
step:443/1770 train_time:42295ms step_avg:95.47ms
step:444/1770 train_time:42393ms step_avg:95.48ms
step:445/1770 train_time:42491ms step_avg:95.49ms
step:446/1770 train_time:42589ms step_avg:95.49ms
step:447/1770 train_time:42687ms step_avg:95.50ms
step:448/1770 train_time:42784ms step_avg:95.50ms
step:449/1770 train_time:42882ms step_avg:95.51ms
step:450/1770 train_time:42979ms step_avg:95.51ms
step:451/1770 train_time:43078ms step_avg:95.52ms
step:452/1770 train_time:43176ms step_avg:95.52ms
step:453/1770 train_time:43275ms step_avg:95.53ms
step:454/1770 train_time:43373ms step_avg:95.53ms
step:455/1770 train_time:43471ms step_avg:95.54ms
step:456/1770 train_time:43568ms step_avg:95.54ms
step:457/1770 train_time:43666ms step_avg:95.55ms
step:458/1770 train_time:43765ms step_avg:95.56ms
step:459/1770 train_time:43863ms step_avg:95.56ms
step:460/1770 train_time:43961ms step_avg:95.57ms
step:461/1770 train_time:44059ms step_avg:95.57ms
step:462/1770 train_time:44157ms step_avg:95.58ms
step:463/1770 train_time:44254ms step_avg:95.58ms
step:464/1770 train_time:44352ms step_avg:95.59ms
step:465/1770 train_time:44450ms step_avg:95.59ms
step:466/1770 train_time:44548ms step_avg:95.60ms
step:467/1770 train_time:44646ms step_avg:95.60ms
step:468/1770 train_time:44744ms step_avg:95.61ms
step:469/1770 train_time:44842ms step_avg:95.61ms
step:470/1770 train_time:44940ms step_avg:95.62ms
step:471/1770 train_time:45038ms step_avg:95.62ms
step:472/1770 train_time:45135ms step_avg:95.63ms
step:473/1770 train_time:45233ms step_avg:95.63ms
step:474/1770 train_time:45331ms step_avg:95.63ms
step:475/1770 train_time:45429ms step_avg:95.64ms
step:476/1770 train_time:45526ms step_avg:95.64ms
step:477/1770 train_time:45624ms step_avg:95.65ms
step:478/1770 train_time:45721ms step_avg:95.65ms
step:479/1770 train_time:45819ms step_avg:95.66ms
step:480/1770 train_time:45918ms step_avg:95.66ms
step:481/1770 train_time:46016ms step_avg:95.67ms
step:482/1770 train_time:46114ms step_avg:95.67ms
step:483/1770 train_time:46213ms step_avg:95.68ms
step:484/1770 train_time:46311ms step_avg:95.68ms
step:485/1770 train_time:46408ms step_avg:95.69ms
step:486/1770 train_time:46506ms step_avg:95.69ms
step:487/1770 train_time:46603ms step_avg:95.70ms
step:488/1770 train_time:46701ms step_avg:95.70ms
step:489/1770 train_time:46799ms step_avg:95.70ms
step:490/1770 train_time:46897ms step_avg:95.71ms
step:491/1770 train_time:46995ms step_avg:95.71ms
step:492/1770 train_time:47093ms step_avg:95.72ms
step:493/1770 train_time:47191ms step_avg:95.72ms
step:494/1770 train_time:47289ms step_avg:95.73ms
step:495/1770 train_time:47387ms step_avg:95.73ms
step:496/1770 train_time:47484ms step_avg:95.73ms
step:497/1770 train_time:47582ms step_avg:95.74ms
step:498/1770 train_time:47679ms step_avg:95.74ms
step:499/1770 train_time:47777ms step_avg:95.75ms
step:500/1770 train_time:47875ms step_avg:95.75ms
step:500/1770 val_loss:3.7502 train_time:47973ms step_avg:95.95ms
step:501/1770 train_time:47992ms step_avg:95.79ms
step:502/1770 train_time:48077ms step_avg:95.77ms
step:503/1770 train_time:48178ms step_avg:95.78ms
step:504/1770 train_time:48276ms step_avg:95.79ms
step:505/1770 train_time:48374ms step_avg:95.79ms
step:506/1770 train_time:48471ms step_avg:95.79ms
step:507/1770 train_time:48568ms step_avg:95.80ms
step:508/1770 train_time:48666ms step_avg:95.80ms
step:509/1770 train_time:48763ms step_avg:95.80ms
step:510/1770 train_time:48860ms step_avg:95.80ms
step:511/1770 train_time:48959ms step_avg:95.81ms
step:512/1770 train_time:49059ms step_avg:95.82ms
step:513/1770 train_time:49158ms step_avg:95.82ms
step:514/1770 train_time:49257ms step_avg:95.83ms
step:515/1770 train_time:49355ms step_avg:95.83ms
step:516/1770 train_time:49452ms step_avg:95.84ms
step:517/1770 train_time:49550ms step_avg:95.84ms
step:518/1770 train_time:49647ms step_avg:95.84ms
step:519/1770 train_time:49744ms step_avg:95.85ms
step:520/1770 train_time:49841ms step_avg:95.85ms
step:521/1770 train_time:49940ms step_avg:95.85ms
step:522/1770 train_time:50039ms step_avg:95.86ms
step:523/1770 train_time:50138ms step_avg:95.87ms
step:524/1770 train_time:50236ms step_avg:95.87ms
step:525/1770 train_time:50335ms step_avg:95.88ms
step:526/1770 train_time:50433ms step_avg:95.88ms
step:527/1770 train_time:50531ms step_avg:95.88ms
step:528/1770 train_time:50628ms step_avg:95.89ms
step:529/1770 train_time:50726ms step_avg:95.89ms
step:530/1770 train_time:50823ms step_avg:95.89ms
step:531/1770 train_time:50921ms step_avg:95.90ms
step:532/1770 train_time:51020ms step_avg:95.90ms
step:533/1770 train_time:51118ms step_avg:95.91ms
step:534/1770 train_time:51217ms step_avg:95.91ms
step:535/1770 train_time:51317ms step_avg:95.92ms
step:536/1770 train_time:51416ms step_avg:95.93ms
step:537/1770 train_time:51515ms step_avg:95.93ms
step:538/1770 train_time:51614ms step_avg:95.94ms
step:539/1770 train_time:51713ms step_avg:95.94ms
step:540/1770 train_time:51812ms step_avg:95.95ms
step:541/1770 train_time:51910ms step_avg:95.95ms
step:542/1770 train_time:52009ms step_avg:95.96ms
step:543/1770 train_time:52108ms step_avg:95.96ms
step:544/1770 train_time:52206ms step_avg:95.97ms
step:545/1770 train_time:52305ms step_avg:95.97ms
step:546/1770 train_time:52403ms step_avg:95.98ms
step:547/1770 train_time:52502ms step_avg:95.98ms
step:548/1770 train_time:52600ms step_avg:95.99ms
step:549/1770 train_time:52699ms step_avg:95.99ms
step:550/1770 train_time:52798ms step_avg:96.00ms
step:551/1770 train_time:52896ms step_avg:96.00ms
step:552/1770 train_time:52996ms step_avg:96.01ms
step:553/1770 train_time:53096ms step_avg:96.01ms
step:554/1770 train_time:53194ms step_avg:96.02ms
step:555/1770 train_time:53292ms step_avg:96.02ms
step:556/1770 train_time:53391ms step_avg:96.03ms
step:557/1770 train_time:53490ms step_avg:96.03ms
step:558/1770 train_time:53588ms step_avg:96.04ms
step:559/1770 train_time:53686ms step_avg:96.04ms
step:560/1770 train_time:53784ms step_avg:96.04ms
step:561/1770 train_time:53882ms step_avg:96.05ms
step:562/1770 train_time:53981ms step_avg:96.05ms
step:563/1770 train_time:54080ms step_avg:96.06ms
step:564/1770 train_time:54179ms step_avg:96.06ms
step:565/1770 train_time:54278ms step_avg:96.07ms
step:566/1770 train_time:54377ms step_avg:96.07ms
step:567/1770 train_time:54475ms step_avg:96.08ms
step:568/1770 train_time:54574ms step_avg:96.08ms
step:569/1770 train_time:54673ms step_avg:96.09ms
step:570/1770 train_time:54772ms step_avg:96.09ms
step:571/1770 train_time:54871ms step_avg:96.10ms
step:572/1770 train_time:54969ms step_avg:96.10ms
step:573/1770 train_time:55068ms step_avg:96.10ms
step:574/1770 train_time:55167ms step_avg:96.11ms
step:575/1770 train_time:55266ms step_avg:96.11ms
step:576/1770 train_time:55365ms step_avg:96.12ms
step:577/1770 train_time:55463ms step_avg:96.12ms
step:578/1770 train_time:55562ms step_avg:96.13ms
step:579/1770 train_time:55661ms step_avg:96.13ms
step:580/1770 train_time:55759ms step_avg:96.14ms
step:581/1770 train_time:55857ms step_avg:96.14ms
step:582/1770 train_time:55956ms step_avg:96.14ms
step:583/1770 train_time:56054ms step_avg:96.15ms
step:584/1770 train_time:56152ms step_avg:96.15ms
step:585/1770 train_time:56251ms step_avg:96.16ms
step:586/1770 train_time:56349ms step_avg:96.16ms
step:587/1770 train_time:56448ms step_avg:96.16ms
step:588/1770 train_time:56546ms step_avg:96.17ms
step:589/1770 train_time:56645ms step_avg:96.17ms
step:590/1770 train_time:56744ms step_avg:96.18ms
step:591/1770 train_time:56842ms step_avg:96.18ms
step:592/1770 train_time:56941ms step_avg:96.18ms
step:593/1770 train_time:57040ms step_avg:96.19ms
step:594/1770 train_time:57139ms step_avg:96.19ms
step:595/1770 train_time:57237ms step_avg:96.20ms
step:596/1770 train_time:57336ms step_avg:96.20ms
step:597/1770 train_time:57436ms step_avg:96.21ms
step:598/1770 train_time:57536ms step_avg:96.21ms
step:599/1770 train_time:57636ms step_avg:96.22ms
step:600/1770 train_time:57735ms step_avg:96.23ms
step:601/1770 train_time:57834ms step_avg:96.23ms
step:602/1770 train_time:57934ms step_avg:96.24ms
step:603/1770 train_time:58033ms step_avg:96.24ms
step:604/1770 train_time:58131ms step_avg:96.24ms
step:605/1770 train_time:58230ms step_avg:96.25ms
step:606/1770 train_time:58328ms step_avg:96.25ms
step:607/1770 train_time:58427ms step_avg:96.25ms
step:608/1770 train_time:58525ms step_avg:96.26ms
step:609/1770 train_time:58623ms step_avg:96.26ms
step:610/1770 train_time:58722ms step_avg:96.26ms
step:611/1770 train_time:58820ms step_avg:96.27ms
step:612/1770 train_time:58919ms step_avg:96.27ms
step:613/1770 train_time:59017ms step_avg:96.28ms
step:614/1770 train_time:59116ms step_avg:96.28ms
step:615/1770 train_time:59214ms step_avg:96.28ms
step:616/1770 train_time:59314ms step_avg:96.29ms
step:617/1770 train_time:59412ms step_avg:96.29ms
step:618/1770 train_time:59511ms step_avg:96.30ms
step:619/1770 train_time:59609ms step_avg:96.30ms
step:620/1770 train_time:59708ms step_avg:96.30ms
step:621/1770 train_time:59806ms step_avg:96.31ms
step:622/1770 train_time:59905ms step_avg:96.31ms
step:623/1770 train_time:60003ms step_avg:96.31ms
step:624/1770 train_time:60101ms step_avg:96.32ms
step:625/1770 train_time:60200ms step_avg:96.32ms
step:625/1770 val_loss:3.6602 train_time:60299ms step_avg:96.48ms
step:626/1770 train_time:60316ms step_avg:96.35ms
step:627/1770 train_time:60406ms step_avg:96.34ms
step:628/1770 train_time:60508ms step_avg:96.35ms
step:629/1770 train_time:60607ms step_avg:96.35ms
step:630/1770 train_time:60705ms step_avg:96.36ms
step:631/1770 train_time:60803ms step_avg:96.36ms
step:632/1770 train_time:60902ms step_avg:96.36ms
step:633/1770 train_time:60999ms step_avg:96.36ms
step:634/1770 train_time:61097ms step_avg:96.37ms
step:635/1770 train_time:61194ms step_avg:96.37ms
step:636/1770 train_time:61292ms step_avg:96.37ms
step:637/1770 train_time:61392ms step_avg:96.38ms
step:638/1770 train_time:61491ms step_avg:96.38ms
step:639/1770 train_time:61590ms step_avg:96.39ms
step:640/1770 train_time:61689ms step_avg:96.39ms
step:641/1770 train_time:61787ms step_avg:96.39ms
step:642/1770 train_time:61885ms step_avg:96.39ms
step:643/1770 train_time:61983ms step_avg:96.40ms
step:644/1770 train_time:62081ms step_avg:96.40ms
step:645/1770 train_time:62180ms step_avg:96.40ms
step:646/1770 train_time:62279ms step_avg:96.41ms
step:647/1770 train_time:62378ms step_avg:96.41ms
step:648/1770 train_time:62477ms step_avg:96.42ms
step:649/1770 train_time:62576ms step_avg:96.42ms
step:650/1770 train_time:62675ms step_avg:96.42ms
step:651/1770 train_time:62775ms step_avg:96.43ms
step:652/1770 train_time:62874ms step_avg:96.43ms
step:653/1770 train_time:62973ms step_avg:96.44ms
step:654/1770 train_time:63071ms step_avg:96.44ms
step:655/1770 train_time:63170ms step_avg:96.44ms
step:656/1770 train_time:63268ms step_avg:96.45ms
step:657/1770 train_time:63367ms step_avg:96.45ms
step:658/1770 train_time:63468ms step_avg:96.46ms
step:659/1770 train_time:63569ms step_avg:96.46ms
step:660/1770 train_time:63671ms step_avg:96.47ms
step:661/1770 train_time:63772ms step_avg:96.48ms
step:662/1770 train_time:63873ms step_avg:96.49ms
step:663/1770 train_time:63973ms step_avg:96.49ms
step:664/1770 train_time:64073ms step_avg:96.50ms
step:665/1770 train_time:64173ms step_avg:96.50ms
step:666/1770 train_time:64273ms step_avg:96.51ms
step:667/1770 train_time:64373ms step_avg:96.51ms
step:668/1770 train_time:64473ms step_avg:96.52ms
step:669/1770 train_time:64574ms step_avg:96.52ms
step:670/1770 train_time:64674ms step_avg:96.53ms
step:671/1770 train_time:64774ms step_avg:96.53ms
step:672/1770 train_time:64875ms step_avg:96.54ms
step:673/1770 train_time:64975ms step_avg:96.55ms
step:674/1770 train_time:65075ms step_avg:96.55ms
step:675/1770 train_time:65174ms step_avg:96.55ms
step:676/1770 train_time:65275ms step_avg:96.56ms
step:677/1770 train_time:65375ms step_avg:96.57ms
step:678/1770 train_time:65474ms step_avg:96.57ms
step:679/1770 train_time:65574ms step_avg:96.57ms
step:680/1770 train_time:65674ms step_avg:96.58ms
step:681/1770 train_time:65775ms step_avg:96.59ms
step:682/1770 train_time:65875ms step_avg:96.59ms
step:683/1770 train_time:65975ms step_avg:96.60ms
step:684/1770 train_time:66074ms step_avg:96.60ms
step:685/1770 train_time:66174ms step_avg:96.60ms
step:686/1770 train_time:66274ms step_avg:96.61ms
step:687/1770 train_time:66374ms step_avg:96.61ms
step:688/1770 train_time:66473ms step_avg:96.62ms
step:689/1770 train_time:66574ms step_avg:96.62ms
step:690/1770 train_time:66674ms step_avg:96.63ms
step:691/1770 train_time:66774ms step_avg:96.63ms
step:692/1770 train_time:66874ms step_avg:96.64ms
step:693/1770 train_time:66974ms step_avg:96.64ms
step:694/1770 train_time:67075ms step_avg:96.65ms
step:695/1770 train_time:67175ms step_avg:96.65ms
step:696/1770 train_time:67275ms step_avg:96.66ms
step:697/1770 train_time:67375ms step_avg:96.66ms
step:698/1770 train_time:67474ms step_avg:96.67ms
step:699/1770 train_time:67575ms step_avg:96.67ms
step:700/1770 train_time:67674ms step_avg:96.68ms
step:701/1770 train_time:67775ms step_avg:96.68ms
step:702/1770 train_time:67874ms step_avg:96.69ms
step:703/1770 train_time:67974ms step_avg:96.69ms
step:704/1770 train_time:68075ms step_avg:96.70ms
step:705/1770 train_time:68175ms step_avg:96.70ms
step:706/1770 train_time:68275ms step_avg:96.71ms
step:707/1770 train_time:68375ms step_avg:96.71ms
step:708/1770 train_time:68474ms step_avg:96.71ms
step:709/1770 train_time:68574ms step_avg:96.72ms
step:710/1770 train_time:68674ms step_avg:96.72ms
step:711/1770 train_time:68774ms step_avg:96.73ms
step:712/1770 train_time:68874ms step_avg:96.73ms
step:713/1770 train_time:68974ms step_avg:96.74ms
step:714/1770 train_time:69074ms step_avg:96.74ms
step:715/1770 train_time:69174ms step_avg:96.75ms
step:716/1770 train_time:69274ms step_avg:96.75ms
step:717/1770 train_time:69374ms step_avg:96.76ms
step:718/1770 train_time:69474ms step_avg:96.76ms
step:719/1770 train_time:69573ms step_avg:96.76ms
step:720/1770 train_time:69673ms step_avg:96.77ms
step:721/1770 train_time:69773ms step_avg:96.77ms
step:722/1770 train_time:69874ms step_avg:96.78ms
step:723/1770 train_time:69975ms step_avg:96.78ms
step:724/1770 train_time:70075ms step_avg:96.79ms
step:725/1770 train_time:70175ms step_avg:96.79ms
step:726/1770 train_time:70275ms step_avg:96.80ms
step:727/1770 train_time:70374ms step_avg:96.80ms
step:728/1770 train_time:70475ms step_avg:96.81ms
step:729/1770 train_time:70575ms step_avg:96.81ms
step:730/1770 train_time:70674ms step_avg:96.81ms
step:731/1770 train_time:70774ms step_avg:96.82ms
step:732/1770 train_time:70874ms step_avg:96.82ms
step:733/1770 train_time:70974ms step_avg:96.83ms
step:734/1770 train_time:71074ms step_avg:96.83ms
step:735/1770 train_time:71175ms step_avg:96.84ms
step:736/1770 train_time:71275ms step_avg:96.84ms
step:737/1770 train_time:71375ms step_avg:96.85ms
step:738/1770 train_time:71475ms step_avg:96.85ms
step:739/1770 train_time:71574ms step_avg:96.85ms
step:740/1770 train_time:71674ms step_avg:96.86ms
step:741/1770 train_time:71774ms step_avg:96.86ms
step:742/1770 train_time:71874ms step_avg:96.87ms
step:743/1770 train_time:71974ms step_avg:96.87ms
step:744/1770 train_time:72074ms step_avg:96.87ms
step:745/1770 train_time:72174ms step_avg:96.88ms
step:746/1770 train_time:72275ms step_avg:96.88ms
step:747/1770 train_time:72374ms step_avg:96.89ms
step:748/1770 train_time:72474ms step_avg:96.89ms
step:749/1770 train_time:72574ms step_avg:96.89ms
step:750/1770 train_time:72673ms step_avg:96.90ms
step:750/1770 val_loss:3.5986 train_time:72772ms step_avg:97.03ms
step:751/1770 train_time:72790ms step_avg:96.92ms
step:752/1770 train_time:72879ms step_avg:96.91ms
step:753/1770 train_time:72981ms step_avg:96.92ms
step:754/1770 train_time:73081ms step_avg:96.92ms
step:755/1770 train_time:73180ms step_avg:96.93ms
step:756/1770 train_time:73280ms step_avg:96.93ms
step:757/1770 train_time:73379ms step_avg:96.93ms
step:758/1770 train_time:73478ms step_avg:96.94ms
step:759/1770 train_time:73578ms step_avg:96.94ms
step:760/1770 train_time:73678ms step_avg:96.95ms
step:761/1770 train_time:73780ms step_avg:96.95ms
step:762/1770 train_time:73882ms step_avg:96.96ms
step:763/1770 train_time:73984ms step_avg:96.96ms
step:764/1770 train_time:74084ms step_avg:96.97ms
step:765/1770 train_time:74183ms step_avg:96.97ms
step:766/1770 train_time:74283ms step_avg:96.98ms
step:767/1770 train_time:74383ms step_avg:96.98ms
step:768/1770 train_time:74482ms step_avg:96.98ms
step:769/1770 train_time:74582ms step_avg:96.99ms
step:770/1770 train_time:74683ms step_avg:96.99ms
step:771/1770 train_time:74782ms step_avg:96.99ms
step:772/1770 train_time:74883ms step_avg:97.00ms
step:773/1770 train_time:74983ms step_avg:97.00ms
step:774/1770 train_time:75084ms step_avg:97.01ms
step:775/1770 train_time:75183ms step_avg:97.01ms
step:776/1770 train_time:75283ms step_avg:97.01ms
step:777/1770 train_time:75383ms step_avg:97.02ms
step:778/1770 train_time:75482ms step_avg:97.02ms
step:779/1770 train_time:75581ms step_avg:97.02ms
step:780/1770 train_time:75681ms step_avg:97.03ms
step:781/1770 train_time:75781ms step_avg:97.03ms
step:782/1770 train_time:75882ms step_avg:97.04ms
step:783/1770 train_time:75983ms step_avg:97.04ms
step:784/1770 train_time:76083ms step_avg:97.04ms
step:785/1770 train_time:76183ms step_avg:97.05ms
step:786/1770 train_time:76284ms step_avg:97.05ms
step:787/1770 train_time:76384ms step_avg:97.06ms
step:788/1770 train_time:76485ms step_avg:97.06ms
step:789/1770 train_time:76585ms step_avg:97.07ms
step:790/1770 train_time:76685ms step_avg:97.07ms
step:791/1770 train_time:76786ms step_avg:97.07ms
step:792/1770 train_time:76887ms step_avg:97.08ms
step:793/1770 train_time:76987ms step_avg:97.08ms
step:794/1770 train_time:77088ms step_avg:97.09ms
step:795/1770 train_time:77189ms step_avg:97.09ms
step:796/1770 train_time:77290ms step_avg:97.10ms
step:797/1770 train_time:77390ms step_avg:97.10ms
step:798/1770 train_time:77490ms step_avg:97.11ms
step:799/1770 train_time:77591ms step_avg:97.11ms
step:800/1770 train_time:77693ms step_avg:97.12ms
step:801/1770 train_time:77794ms step_avg:97.12ms
step:802/1770 train_time:77895ms step_avg:97.13ms
step:803/1770 train_time:77996ms step_avg:97.13ms
step:804/1770 train_time:78097ms step_avg:97.14ms
step:805/1770 train_time:78198ms step_avg:97.14ms
step:806/1770 train_time:78299ms step_avg:97.15ms
step:807/1770 train_time:78400ms step_avg:97.15ms
step:808/1770 train_time:78502ms step_avg:97.16ms
step:809/1770 train_time:78603ms step_avg:97.16ms
step:810/1770 train_time:78704ms step_avg:97.17ms
step:811/1770 train_time:78805ms step_avg:97.17ms
step:812/1770 train_time:78905ms step_avg:97.17ms
step:813/1770 train_time:79006ms step_avg:97.18ms
step:814/1770 train_time:79107ms step_avg:97.18ms
step:815/1770 train_time:79208ms step_avg:97.19ms
step:816/1770 train_time:79309ms step_avg:97.19ms
step:817/1770 train_time:79410ms step_avg:97.20ms
step:818/1770 train_time:79511ms step_avg:97.20ms
step:819/1770 train_time:79611ms step_avg:97.20ms
step:820/1770 train_time:79711ms step_avg:97.21ms
step:821/1770 train_time:79811ms step_avg:97.21ms
step:822/1770 train_time:79913ms step_avg:97.22ms
step:823/1770 train_time:80015ms step_avg:97.22ms
step:824/1770 train_time:80116ms step_avg:97.23ms
step:825/1770 train_time:80217ms step_avg:97.23ms
step:826/1770 train_time:80318ms step_avg:97.24ms
step:827/1770 train_time:80419ms step_avg:97.24ms
step:828/1770 train_time:80520ms step_avg:97.25ms
step:829/1770 train_time:80620ms step_avg:97.25ms
step:830/1770 train_time:80721ms step_avg:97.25ms
step:831/1770 train_time:80823ms step_avg:97.26ms
step:832/1770 train_time:80923ms step_avg:97.26ms
step:833/1770 train_time:81024ms step_avg:97.27ms
step:834/1770 train_time:81125ms step_avg:97.27ms
step:835/1770 train_time:81225ms step_avg:97.28ms
step:836/1770 train_time:81326ms step_avg:97.28ms
step:837/1770 train_time:81427ms step_avg:97.28ms
step:838/1770 train_time:81528ms step_avg:97.29ms
step:839/1770 train_time:81628ms step_avg:97.29ms
step:840/1770 train_time:81729ms step_avg:97.30ms
step:841/1770 train_time:81829ms step_avg:97.30ms
step:842/1770 train_time:81930ms step_avg:97.30ms
step:843/1770 train_time:82031ms step_avg:97.31ms
step:844/1770 train_time:82133ms step_avg:97.31ms
step:845/1770 train_time:82235ms step_avg:97.32ms
step:846/1770 train_time:82335ms step_avg:97.32ms
step:847/1770 train_time:82437ms step_avg:97.33ms
step:848/1770 train_time:82537ms step_avg:97.33ms
step:849/1770 train_time:82637ms step_avg:97.33ms
step:850/1770 train_time:82739ms step_avg:97.34ms
step:851/1770 train_time:82840ms step_avg:97.34ms
step:852/1770 train_time:82941ms step_avg:97.35ms
step:853/1770 train_time:83042ms step_avg:97.35ms
step:854/1770 train_time:83142ms step_avg:97.36ms
step:855/1770 train_time:83243ms step_avg:97.36ms
step:856/1770 train_time:83343ms step_avg:97.36ms
step:857/1770 train_time:83444ms step_avg:97.37ms
step:858/1770 train_time:83545ms step_avg:97.37ms
step:859/1770 train_time:83646ms step_avg:97.38ms
step:860/1770 train_time:83747ms step_avg:97.38ms
step:861/1770 train_time:83847ms step_avg:97.38ms
step:862/1770 train_time:83948ms step_avg:97.39ms
step:863/1770 train_time:84049ms step_avg:97.39ms
step:864/1770 train_time:84149ms step_avg:97.39ms
step:865/1770 train_time:84249ms step_avg:97.40ms
step:866/1770 train_time:84350ms step_avg:97.40ms
step:867/1770 train_time:84450ms step_avg:97.41ms
step:868/1770 train_time:84551ms step_avg:97.41ms
step:869/1770 train_time:84651ms step_avg:97.41ms
step:870/1770 train_time:84752ms step_avg:97.42ms
step:871/1770 train_time:84853ms step_avg:97.42ms
step:872/1770 train_time:84955ms step_avg:97.43ms
step:873/1770 train_time:85055ms step_avg:97.43ms
step:874/1770 train_time:85156ms step_avg:97.43ms
step:875/1770 train_time:85257ms step_avg:97.44ms
step:875/1770 val_loss:3.5482 train_time:85357ms step_avg:97.55ms
step:876/1770 train_time:85375ms step_avg:97.46ms
step:877/1770 train_time:85469ms step_avg:97.46ms
step:878/1770 train_time:85572ms step_avg:97.46ms
step:879/1770 train_time:85675ms step_avg:97.47ms
step:880/1770 train_time:85774ms step_avg:97.47ms
step:881/1770 train_time:85874ms step_avg:97.47ms
step:882/1770 train_time:85974ms step_avg:97.48ms
step:883/1770 train_time:86073ms step_avg:97.48ms
step:884/1770 train_time:86173ms step_avg:97.48ms
step:885/1770 train_time:86273ms step_avg:97.48ms
step:886/1770 train_time:86376ms step_avg:97.49ms
step:887/1770 train_time:86478ms step_avg:97.50ms
step:888/1770 train_time:86579ms step_avg:97.50ms
step:889/1770 train_time:86680ms step_avg:97.50ms
step:890/1770 train_time:86781ms step_avg:97.51ms
step:891/1770 train_time:86881ms step_avg:97.51ms
step:892/1770 train_time:86982ms step_avg:97.51ms
step:893/1770 train_time:87081ms step_avg:97.52ms
step:894/1770 train_time:87182ms step_avg:97.52ms
step:895/1770 train_time:87283ms step_avg:97.52ms
step:896/1770 train_time:87385ms step_avg:97.53ms
step:897/1770 train_time:87486ms step_avg:97.53ms
step:898/1770 train_time:87587ms step_avg:97.54ms
step:899/1770 train_time:87689ms step_avg:97.54ms
step:900/1770 train_time:87790ms step_avg:97.54ms
step:901/1770 train_time:87891ms step_avg:97.55ms
step:902/1770 train_time:87993ms step_avg:97.55ms
step:903/1770 train_time:88093ms step_avg:97.56ms
step:904/1770 train_time:88194ms step_avg:97.56ms
step:905/1770 train_time:88295ms step_avg:97.56ms
step:906/1770 train_time:88397ms step_avg:97.57ms
step:907/1770 train_time:88498ms step_avg:97.57ms
step:908/1770 train_time:88600ms step_avg:97.58ms
step:909/1770 train_time:88701ms step_avg:97.58ms
step:910/1770 train_time:88802ms step_avg:97.58ms
step:911/1770 train_time:88903ms step_avg:97.59ms
step:912/1770 train_time:89003ms step_avg:97.59ms
step:913/1770 train_time:89104ms step_avg:97.60ms
step:914/1770 train_time:89205ms step_avg:97.60ms
step:915/1770 train_time:89307ms step_avg:97.60ms
step:916/1770 train_time:89409ms step_avg:97.61ms
step:917/1770 train_time:89511ms step_avg:97.61ms
step:918/1770 train_time:89612ms step_avg:97.62ms
step:919/1770 train_time:89713ms step_avg:97.62ms
step:920/1770 train_time:89816ms step_avg:97.63ms
step:921/1770 train_time:89919ms step_avg:97.63ms
step:922/1770 train_time:90020ms step_avg:97.64ms
step:923/1770 train_time:90121ms step_avg:97.64ms
step:924/1770 train_time:90223ms step_avg:97.64ms
step:925/1770 train_time:90325ms step_avg:97.65ms
step:926/1770 train_time:90428ms step_avg:97.65ms
step:927/1770 train_time:90532ms step_avg:97.66ms
step:928/1770 train_time:90634ms step_avg:97.67ms
step:929/1770 train_time:90735ms step_avg:97.67ms
step:930/1770 train_time:90838ms step_avg:97.67ms
step:931/1770 train_time:90939ms step_avg:97.68ms
step:932/1770 train_time:91041ms step_avg:97.68ms
step:933/1770 train_time:91142ms step_avg:97.69ms
step:934/1770 train_time:91243ms step_avg:97.69ms
step:935/1770 train_time:91346ms step_avg:97.70ms
step:936/1770 train_time:91449ms step_avg:97.70ms
step:937/1770 train_time:91552ms step_avg:97.71ms
step:938/1770 train_time:91655ms step_avg:97.71ms
step:939/1770 train_time:91759ms step_avg:97.72ms
step:940/1770 train_time:91860ms step_avg:97.72ms
step:941/1770 train_time:91961ms step_avg:97.73ms
step:942/1770 train_time:92063ms step_avg:97.73ms
step:943/1770 train_time:92164ms step_avg:97.73ms
step:944/1770 train_time:92266ms step_avg:97.74ms
step:945/1770 train_time:92367ms step_avg:97.74ms
step:946/1770 train_time:92470ms step_avg:97.75ms
step:947/1770 train_time:92574ms step_avg:97.76ms
step:948/1770 train_time:92677ms step_avg:97.76ms
step:949/1770 train_time:92779ms step_avg:97.77ms
step:950/1770 train_time:92881ms step_avg:97.77ms
step:951/1770 train_time:92982ms step_avg:97.77ms
step:952/1770 train_time:93084ms step_avg:97.78ms
step:953/1770 train_time:93186ms step_avg:97.78ms
step:954/1770 train_time:93289ms step_avg:97.79ms
step:955/1770 train_time:93392ms step_avg:97.79ms
step:956/1770 train_time:93494ms step_avg:97.80ms
step:957/1770 train_time:93596ms step_avg:97.80ms
step:958/1770 train_time:93698ms step_avg:97.81ms
step:959/1770 train_time:93799ms step_avg:97.81ms
step:960/1770 train_time:93901ms step_avg:97.81ms
step:961/1770 train_time:94003ms step_avg:97.82ms
step:962/1770 train_time:94107ms step_avg:97.82ms
step:963/1770 train_time:94208ms step_avg:97.83ms
step:964/1770 train_time:94311ms step_avg:97.83ms
step:965/1770 train_time:94413ms step_avg:97.84ms
step:966/1770 train_time:94515ms step_avg:97.84ms
step:967/1770 train_time:94617ms step_avg:97.85ms
step:968/1770 train_time:94720ms step_avg:97.85ms
step:969/1770 train_time:94821ms step_avg:97.85ms
step:970/1770 train_time:94923ms step_avg:97.86ms
step:971/1770 train_time:95024ms step_avg:97.86ms
step:972/1770 train_time:95127ms step_avg:97.87ms
step:973/1770 train_time:95229ms step_avg:97.87ms
step:974/1770 train_time:95332ms step_avg:97.88ms
step:975/1770 train_time:95434ms step_avg:97.88ms
step:976/1770 train_time:95536ms step_avg:97.89ms
step:977/1770 train_time:95639ms step_avg:97.89ms
step:978/1770 train_time:95741ms step_avg:97.89ms
step:979/1770 train_time:95842ms step_avg:97.90ms
step:980/1770 train_time:95944ms step_avg:97.90ms
step:981/1770 train_time:96046ms step_avg:97.91ms
step:982/1770 train_time:96147ms step_avg:97.91ms
step:983/1770 train_time:96250ms step_avg:97.91ms
step:984/1770 train_time:96353ms step_avg:97.92ms
step:985/1770 train_time:96455ms step_avg:97.92ms
step:986/1770 train_time:96557ms step_avg:97.93ms
step:987/1770 train_time:96660ms step_avg:97.93ms
step:988/1770 train_time:96761ms step_avg:97.94ms
step:989/1770 train_time:96865ms step_avg:97.94ms
step:990/1770 train_time:96966ms step_avg:97.95ms
step:991/1770 train_time:97067ms step_avg:97.95ms
step:992/1770 train_time:97169ms step_avg:97.95ms
step:993/1770 train_time:97271ms step_avg:97.96ms
step:994/1770 train_time:97374ms step_avg:97.96ms
step:995/1770 train_time:97476ms step_avg:97.97ms
step:996/1770 train_time:97577ms step_avg:97.97ms
step:997/1770 train_time:97679ms step_avg:97.97ms
step:998/1770 train_time:97780ms step_avg:97.98ms
step:999/1770 train_time:97882ms step_avg:97.98ms
step:1000/1770 train_time:97984ms step_avg:97.98ms
step:1000/1770 val_loss:3.5116 train_time:98085ms step_avg:98.08ms
step:1001/1770 train_time:98102ms step_avg:98.00ms
step:1002/1770 train_time:98200ms step_avg:98.00ms
step:1003/1770 train_time:98302ms step_avg:98.01ms
step:1004/1770 train_time:98403ms step_avg:98.01ms
step:1005/1770 train_time:98504ms step_avg:98.01ms
step:1006/1770 train_time:98605ms step_avg:98.02ms
step:1007/1770 train_time:98705ms step_avg:98.02ms
step:1008/1770 train_time:98806ms step_avg:98.02ms
step:1009/1770 train_time:98907ms step_avg:98.02ms
step:1010/1770 train_time:99009ms step_avg:98.03ms
step:1011/1770 train_time:99117ms step_avg:98.04ms
step:1012/1770 train_time:99220ms step_avg:98.04ms
step:1013/1770 train_time:99321ms step_avg:98.05ms
step:1014/1770 train_time:99423ms step_avg:98.05ms
step:1015/1770 train_time:99524ms step_avg:98.05ms
step:1016/1770 train_time:99625ms step_avg:98.06ms
step:1017/1770 train_time:99727ms step_avg:98.06ms
step:1018/1770 train_time:99828ms step_avg:98.06ms
step:1019/1770 train_time:99930ms step_avg:98.07ms
step:1020/1770 train_time:100033ms step_avg:98.07ms
step:1021/1770 train_time:100137ms step_avg:98.08ms
step:1022/1770 train_time:100239ms step_avg:98.08ms
step:1023/1770 train_time:100341ms step_avg:98.09ms
step:1024/1770 train_time:100443ms step_avg:98.09ms
step:1025/1770 train_time:100544ms step_avg:98.09ms
step:1026/1770 train_time:100646ms step_avg:98.10ms
step:1027/1770 train_time:100748ms step_avg:98.10ms
step:1028/1770 train_time:100849ms step_avg:98.10ms
step:1029/1770 train_time:100951ms step_avg:98.11ms
step:1030/1770 train_time:101056ms step_avg:98.11ms
step:1031/1770 train_time:101159ms step_avg:98.12ms
step:1032/1770 train_time:101262ms step_avg:98.12ms
step:1033/1770 train_time:101365ms step_avg:98.13ms
step:1034/1770 train_time:101467ms step_avg:98.13ms
step:1035/1770 train_time:101569ms step_avg:98.13ms
step:1036/1770 train_time:101670ms step_avg:98.14ms
step:1037/1770 train_time:101773ms step_avg:98.14ms
step:1038/1770 train_time:101875ms step_avg:98.15ms
step:1039/1770 train_time:101977ms step_avg:98.15ms
step:1040/1770 train_time:102079ms step_avg:98.15ms
step:1041/1770 train_time:102180ms step_avg:98.16ms
step:1042/1770 train_time:102283ms step_avg:98.16ms
step:1043/1770 train_time:102385ms step_avg:98.16ms
step:1044/1770 train_time:102487ms step_avg:98.17ms
step:1045/1770 train_time:102589ms step_avg:98.17ms
step:1046/1770 train_time:102691ms step_avg:98.18ms
step:1047/1770 train_time:102794ms step_avg:98.18ms
step:1048/1770 train_time:102895ms step_avg:98.18ms
step:1049/1770 train_time:102997ms step_avg:98.19ms
step:1050/1770 train_time:103100ms step_avg:98.19ms
step:1051/1770 train_time:103202ms step_avg:98.19ms
step:1052/1770 train_time:103305ms step_avg:98.20ms
step:1053/1770 train_time:103407ms step_avg:98.20ms
step:1054/1770 train_time:103510ms step_avg:98.21ms
step:1055/1770 train_time:103613ms step_avg:98.21ms
step:1056/1770 train_time:103714ms step_avg:98.21ms
step:1057/1770 train_time:103816ms step_avg:98.22ms
step:1058/1770 train_time:103918ms step_avg:98.22ms
step:1059/1770 train_time:104020ms step_avg:98.22ms
step:1060/1770 train_time:104122ms step_avg:98.23ms
step:1061/1770 train_time:104224ms step_avg:98.23ms
step:1062/1770 train_time:104327ms step_avg:98.24ms
step:1063/1770 train_time:104430ms step_avg:98.24ms
step:1064/1770 train_time:104534ms step_avg:98.25ms
step:1065/1770 train_time:104635ms step_avg:98.25ms
step:1066/1770 train_time:104738ms step_avg:98.25ms
step:1067/1770 train_time:104841ms step_avg:98.26ms
step:1068/1770 train_time:104942ms step_avg:98.26ms
step:1069/1770 train_time:105045ms step_avg:98.26ms
step:1070/1770 train_time:105147ms step_avg:98.27ms
step:1071/1770 train_time:105248ms step_avg:98.27ms
step:1072/1770 train_time:105350ms step_avg:98.27ms
step:1073/1770 train_time:105452ms step_avg:98.28ms
step:1074/1770 train_time:105555ms step_avg:98.28ms
step:1075/1770 train_time:105657ms step_avg:98.29ms
step:1076/1770 train_time:105761ms step_avg:98.29ms
step:1077/1770 train_time:105862ms step_avg:98.29ms
step:1078/1770 train_time:105964ms step_avg:98.30ms
step:1079/1770 train_time:106066ms step_avg:98.30ms
step:1080/1770 train_time:106168ms step_avg:98.30ms
step:1081/1770 train_time:106270ms step_avg:98.31ms
step:1082/1770 train_time:106372ms step_avg:98.31ms
step:1083/1770 train_time:106474ms step_avg:98.31ms
step:1084/1770 train_time:106577ms step_avg:98.32ms
step:1085/1770 train_time:106680ms step_avg:98.32ms
step:1086/1770 train_time:106781ms step_avg:98.33ms
step:1087/1770 train_time:106883ms step_avg:98.33ms
step:1088/1770 train_time:106985ms step_avg:98.33ms
step:1089/1770 train_time:107087ms step_avg:98.34ms
step:1090/1770 train_time:107190ms step_avg:98.34ms
step:1091/1770 train_time:107292ms step_avg:98.34ms
step:1092/1770 train_time:107395ms step_avg:98.35ms
step:1093/1770 train_time:107496ms step_avg:98.35ms
step:1094/1770 train_time:107598ms step_avg:98.35ms
step:1095/1770 train_time:107700ms step_avg:98.36ms
step:1096/1770 train_time:107802ms step_avg:98.36ms
step:1097/1770 train_time:107905ms step_avg:98.36ms
step:1098/1770 train_time:108007ms step_avg:98.37ms
step:1099/1770 train_time:108109ms step_avg:98.37ms
step:1100/1770 train_time:108212ms step_avg:98.37ms
step:1101/1770 train_time:108315ms step_avg:98.38ms
step:1102/1770 train_time:108416ms step_avg:98.38ms
step:1103/1770 train_time:108519ms step_avg:98.39ms
step:1104/1770 train_time:108621ms step_avg:98.39ms
step:1105/1770 train_time:108723ms step_avg:98.39ms
step:1106/1770 train_time:108825ms step_avg:98.39ms
step:1107/1770 train_time:108926ms step_avg:98.40ms
step:1108/1770 train_time:109028ms step_avg:98.40ms
step:1109/1770 train_time:109130ms step_avg:98.40ms
step:1110/1770 train_time:109232ms step_avg:98.41ms
step:1111/1770 train_time:109335ms step_avg:98.41ms
step:1112/1770 train_time:109438ms step_avg:98.42ms
step:1113/1770 train_time:109540ms step_avg:98.42ms
step:1114/1770 train_time:109642ms step_avg:98.42ms
step:1115/1770 train_time:109743ms step_avg:98.42ms
step:1116/1770 train_time:109845ms step_avg:98.43ms
step:1117/1770 train_time:109947ms step_avg:98.43ms
step:1118/1770 train_time:110049ms step_avg:98.43ms
step:1119/1770 train_time:110150ms step_avg:98.44ms
step:1120/1770 train_time:110253ms step_avg:98.44ms
step:1121/1770 train_time:110356ms step_avg:98.44ms
step:1122/1770 train_time:110459ms step_avg:98.45ms
step:1123/1770 train_time:110561ms step_avg:98.45ms
step:1124/1770 train_time:110663ms step_avg:98.45ms
step:1125/1770 train_time:110765ms step_avg:98.46ms
step:1125/1770 val_loss:3.4716 train_time:110865ms step_avg:98.55ms
step:1126/1770 train_time:110885ms step_avg:98.48ms
step:1127/1770 train_time:110976ms step_avg:98.47ms
step:1128/1770 train_time:111078ms step_avg:98.47ms
step:1129/1770 train_time:111180ms step_avg:98.48ms
step:1130/1770 train_time:111281ms step_avg:98.48ms
step:1131/1770 train_time:111382ms step_avg:98.48ms
step:1132/1770 train_time:111484ms step_avg:98.48ms
step:1133/1770 train_time:111585ms step_avg:98.49ms
step:1134/1770 train_time:111687ms step_avg:98.49ms
step:1135/1770 train_time:111790ms step_avg:98.49ms
step:1136/1770 train_time:111894ms step_avg:98.50ms
step:1137/1770 train_time:111998ms step_avg:98.50ms
step:1138/1770 train_time:112099ms step_avg:98.51ms
step:1139/1770 train_time:112202ms step_avg:98.51ms
step:1140/1770 train_time:112304ms step_avg:98.51ms
step:1141/1770 train_time:112406ms step_avg:98.52ms
step:1142/1770 train_time:112507ms step_avg:98.52ms
step:1143/1770 train_time:112609ms step_avg:98.52ms
step:1144/1770 train_time:112712ms step_avg:98.52ms
step:1145/1770 train_time:112815ms step_avg:98.53ms
step:1146/1770 train_time:112919ms step_avg:98.53ms
step:1147/1770 train_time:113022ms step_avg:98.54ms
step:1148/1770 train_time:113124ms step_avg:98.54ms
step:1149/1770 train_time:113227ms step_avg:98.54ms
step:1150/1770 train_time:113330ms step_avg:98.55ms
step:1151/1770 train_time:113432ms step_avg:98.55ms
step:1152/1770 train_time:113534ms step_avg:98.55ms
step:1153/1770 train_time:113636ms step_avg:98.56ms
step:1154/1770 train_time:113737ms step_avg:98.56ms
step:1155/1770 train_time:113840ms step_avg:98.56ms
step:1156/1770 train_time:113943ms step_avg:98.57ms
step:1157/1770 train_time:114046ms step_avg:98.57ms
step:1158/1770 train_time:114149ms step_avg:98.57ms
step:1159/1770 train_time:114252ms step_avg:98.58ms
step:1160/1770 train_time:114354ms step_avg:98.58ms
step:1161/1770 train_time:114455ms step_avg:98.58ms
step:1162/1770 train_time:114558ms step_avg:98.59ms
step:1163/1770 train_time:114660ms step_avg:98.59ms
step:1164/1770 train_time:114762ms step_avg:98.59ms
step:1165/1770 train_time:114863ms step_avg:98.60ms
step:1166/1770 train_time:114966ms step_avg:98.60ms
step:1167/1770 train_time:115069ms step_avg:98.60ms
step:1168/1770 train_time:115171ms step_avg:98.61ms
step:1169/1770 train_time:115274ms step_avg:98.61ms
step:1170/1770 train_time:115376ms step_avg:98.61ms
step:1171/1770 train_time:115479ms step_avg:98.62ms
step:1172/1770 train_time:115580ms step_avg:98.62ms
step:1173/1770 train_time:115682ms step_avg:98.62ms
step:1174/1770 train_time:115785ms step_avg:98.62ms
step:1175/1770 train_time:115886ms step_avg:98.63ms
step:1176/1770 train_time:115988ms step_avg:98.63ms
step:1177/1770 train_time:116090ms step_avg:98.63ms
step:1178/1770 train_time:116193ms step_avg:98.64ms
step:1179/1770 train_time:116295ms step_avg:98.64ms
step:1180/1770 train_time:116397ms step_avg:98.64ms
step:1181/1770 train_time:116499ms step_avg:98.64ms
step:1182/1770 train_time:116601ms step_avg:98.65ms
step:1183/1770 train_time:116704ms step_avg:98.65ms
step:1184/1770 train_time:116808ms step_avg:98.66ms
step:1185/1770 train_time:116911ms step_avg:98.66ms
step:1186/1770 train_time:117016ms step_avg:98.66ms
step:1187/1770 train_time:117121ms step_avg:98.67ms
step:1188/1770 train_time:117225ms step_avg:98.67ms
step:1189/1770 train_time:117330ms step_avg:98.68ms
step:1190/1770 train_time:117433ms step_avg:98.68ms
step:1191/1770 train_time:117536ms step_avg:98.69ms
step:1192/1770 train_time:117640ms step_avg:98.69ms
step:1193/1770 train_time:117742ms step_avg:98.69ms
step:1194/1770 train_time:117846ms step_avg:98.70ms
step:1195/1770 train_time:117950ms step_avg:98.70ms
step:1196/1770 train_time:118056ms step_avg:98.71ms
step:1197/1770 train_time:118159ms step_avg:98.71ms
step:1198/1770 train_time:118262ms step_avg:98.72ms
step:1199/1770 train_time:118366ms step_avg:98.72ms
step:1200/1770 train_time:118470ms step_avg:98.73ms
step:1201/1770 train_time:118575ms step_avg:98.73ms
step:1202/1770 train_time:118678ms step_avg:98.73ms
step:1203/1770 train_time:118781ms step_avg:98.74ms
step:1204/1770 train_time:118884ms step_avg:98.74ms
step:1205/1770 train_time:118987ms step_avg:98.74ms
step:1206/1770 train_time:119092ms step_avg:98.75ms
step:1207/1770 train_time:119196ms step_avg:98.75ms
step:1208/1770 train_time:119299ms step_avg:98.76ms
step:1209/1770 train_time:119402ms step_avg:98.76ms
step:1210/1770 train_time:119506ms step_avg:98.76ms
step:1211/1770 train_time:119611ms step_avg:98.77ms
step:1212/1770 train_time:119716ms step_avg:98.78ms
step:1213/1770 train_time:119819ms step_avg:98.78ms
step:1214/1770 train_time:119922ms step_avg:98.78ms
step:1215/1770 train_time:120026ms step_avg:98.79ms
step:1216/1770 train_time:120131ms step_avg:98.79ms
step:1217/1770 train_time:120234ms step_avg:98.80ms
step:1218/1770 train_time:120338ms step_avg:98.80ms
step:1219/1770 train_time:120441ms step_avg:98.80ms
step:1220/1770 train_time:120544ms step_avg:98.81ms
step:1221/1770 train_time:120648ms step_avg:98.81ms
step:1222/1770 train_time:120753ms step_avg:98.82ms
step:1223/1770 train_time:120857ms step_avg:98.82ms
step:1224/1770 train_time:120960ms step_avg:98.82ms
step:1225/1770 train_time:121065ms step_avg:98.83ms
step:1226/1770 train_time:121169ms step_avg:98.83ms
step:1227/1770 train_time:121274ms step_avg:98.84ms
step:1228/1770 train_time:121379ms step_avg:98.84ms
step:1229/1770 train_time:121482ms step_avg:98.85ms
step:1230/1770 train_time:121586ms step_avg:98.85ms
step:1231/1770 train_time:121689ms step_avg:98.85ms
step:1232/1770 train_time:121793ms step_avg:98.86ms
step:1233/1770 train_time:121896ms step_avg:98.86ms
step:1234/1770 train_time:122000ms step_avg:98.87ms
step:1235/1770 train_time:122104ms step_avg:98.87ms
step:1236/1770 train_time:122208ms step_avg:98.87ms
step:1237/1770 train_time:122312ms step_avg:98.88ms
step:1238/1770 train_time:122415ms step_avg:98.88ms
step:1239/1770 train_time:122519ms step_avg:98.89ms
step:1240/1770 train_time:122622ms step_avg:98.89ms
step:1241/1770 train_time:122727ms step_avg:98.89ms
step:1242/1770 train_time:122831ms step_avg:98.90ms
step:1243/1770 train_time:122935ms step_avg:98.90ms
step:1244/1770 train_time:123038ms step_avg:98.91ms
step:1245/1770 train_time:123142ms step_avg:98.91ms
step:1246/1770 train_time:123246ms step_avg:98.91ms
step:1247/1770 train_time:123349ms step_avg:98.92ms
step:1248/1770 train_time:123454ms step_avg:98.92ms
step:1249/1770 train_time:123557ms step_avg:98.93ms
step:1250/1770 train_time:123660ms step_avg:98.93ms
step:1250/1770 val_loss:3.4243 train_time:123764ms step_avg:99.01ms
step:1251/1770 train_time:123782ms step_avg:98.95ms
step:1252/1770 train_time:123878ms step_avg:98.94ms
step:1253/1770 train_time:123981ms step_avg:98.95ms
step:1254/1770 train_time:124084ms step_avg:98.95ms
step:1255/1770 train_time:124189ms step_avg:98.96ms
step:1256/1770 train_time:124292ms step_avg:98.96ms
step:1257/1770 train_time:124394ms step_avg:98.96ms
step:1258/1770 train_time:124497ms step_avg:98.96ms
step:1259/1770 train_time:124601ms step_avg:98.97ms
step:1260/1770 train_time:124703ms step_avg:98.97ms
step:1261/1770 train_time:124808ms step_avg:98.98ms
step:1262/1770 train_time:124914ms step_avg:98.98ms
step:1263/1770 train_time:125018ms step_avg:98.98ms
step:1264/1770 train_time:125121ms step_avg:98.99ms
step:1265/1770 train_time:125224ms step_avg:98.99ms
step:1266/1770 train_time:125328ms step_avg:98.99ms
step:1267/1770 train_time:125432ms step_avg:99.00ms
step:1268/1770 train_time:125535ms step_avg:99.00ms
step:1269/1770 train_time:125638ms step_avg:99.01ms
step:1270/1770 train_time:125742ms step_avg:99.01ms
step:1271/1770 train_time:125846ms step_avg:99.01ms
step:1272/1770 train_time:125949ms step_avg:99.02ms
step:1273/1770 train_time:126054ms step_avg:99.02ms
step:1274/1770 train_time:126158ms step_avg:99.02ms
step:1275/1770 train_time:126261ms step_avg:99.03ms
step:1276/1770 train_time:126365ms step_avg:99.03ms
step:1277/1770 train_time:126468ms step_avg:99.03ms
step:1278/1770 train_time:126573ms step_avg:99.04ms
step:1279/1770 train_time:126676ms step_avg:99.04ms
step:1280/1770 train_time:126779ms step_avg:99.05ms
step:1281/1770 train_time:126883ms step_avg:99.05ms
step:1282/1770 train_time:126987ms step_avg:99.05ms
step:1283/1770 train_time:127092ms step_avg:99.06ms
step:1284/1770 train_time:127197ms step_avg:99.06ms
step:1285/1770 train_time:127300ms step_avg:99.07ms
step:1286/1770 train_time:127403ms step_avg:99.07ms
step:1287/1770 train_time:127509ms step_avg:99.07ms
step:1288/1770 train_time:127612ms step_avg:99.08ms
step:1289/1770 train_time:127715ms step_avg:99.08ms
step:1290/1770 train_time:127819ms step_avg:99.08ms
step:1291/1770 train_time:127922ms step_avg:99.09ms
step:1292/1770 train_time:128025ms step_avg:99.09ms
step:1293/1770 train_time:128129ms step_avg:99.09ms
step:1294/1770 train_time:128233ms step_avg:99.10ms
step:1295/1770 train_time:128338ms step_avg:99.10ms
step:1296/1770 train_time:128441ms step_avg:99.11ms
step:1297/1770 train_time:128546ms step_avg:99.11ms
step:1298/1770 train_time:128649ms step_avg:99.11ms
step:1299/1770 train_time:128753ms step_avg:99.12ms
step:1300/1770 train_time:128856ms step_avg:99.12ms
step:1301/1770 train_time:128960ms step_avg:99.12ms
step:1302/1770 train_time:129063ms step_avg:99.13ms
step:1303/1770 train_time:129166ms step_avg:99.13ms
step:1304/1770 train_time:129268ms step_avg:99.13ms
step:1305/1770 train_time:129374ms step_avg:99.14ms
step:1306/1770 train_time:129478ms step_avg:99.14ms
step:1307/1770 train_time:129581ms step_avg:99.14ms
step:1308/1770 train_time:129684ms step_avg:99.15ms
step:1309/1770 train_time:129786ms step_avg:99.15ms
step:1310/1770 train_time:129890ms step_avg:99.15ms
step:1311/1770 train_time:129994ms step_avg:99.16ms
step:1312/1770 train_time:130097ms step_avg:99.16ms
step:1313/1770 train_time:130199ms step_avg:99.16ms
step:1314/1770 train_time:130304ms step_avg:99.17ms
step:1315/1770 train_time:130407ms step_avg:99.17ms
step:1316/1770 train_time:130512ms step_avg:99.17ms
step:1317/1770 train_time:130616ms step_avg:99.18ms
step:1318/1770 train_time:130721ms step_avg:99.18ms
step:1319/1770 train_time:130824ms step_avg:99.18ms
step:1320/1770 train_time:130928ms step_avg:99.19ms
step:1321/1770 train_time:131032ms step_avg:99.19ms
step:1322/1770 train_time:131136ms step_avg:99.20ms
step:1323/1770 train_time:131240ms step_avg:99.20ms
step:1324/1770 train_time:131344ms step_avg:99.20ms
step:1325/1770 train_time:131449ms step_avg:99.21ms
step:1326/1770 train_time:131552ms step_avg:99.21ms
step:1327/1770 train_time:131658ms step_avg:99.21ms
step:1328/1770 train_time:131761ms step_avg:99.22ms
step:1329/1770 train_time:131864ms step_avg:99.22ms
step:1330/1770 train_time:131967ms step_avg:99.22ms
step:1331/1770 train_time:132071ms step_avg:99.23ms
step:1332/1770 train_time:132175ms step_avg:99.23ms
step:1333/1770 train_time:132278ms step_avg:99.23ms
step:1334/1770 train_time:132380ms step_avg:99.24ms
step:1335/1770 train_time:132484ms step_avg:99.24ms
step:1336/1770 train_time:132588ms step_avg:99.24ms
step:1337/1770 train_time:132692ms step_avg:99.25ms
step:1338/1770 train_time:132795ms step_avg:99.25ms
step:1339/1770 train_time:132899ms step_avg:99.25ms
step:1340/1770 train_time:133002ms step_avg:99.26ms
step:1341/1770 train_time:133107ms step_avg:99.26ms
step:1342/1770 train_time:133212ms step_avg:99.26ms
step:1343/1770 train_time:133316ms step_avg:99.27ms
step:1344/1770 train_time:133419ms step_avg:99.27ms
step:1345/1770 train_time:133522ms step_avg:99.27ms
step:1346/1770 train_time:133626ms step_avg:99.28ms
step:1347/1770 train_time:133729ms step_avg:99.28ms
step:1348/1770 train_time:133834ms step_avg:99.28ms
step:1349/1770 train_time:133937ms step_avg:99.29ms
step:1350/1770 train_time:134041ms step_avg:99.29ms
step:1351/1770 train_time:134147ms step_avg:99.29ms
step:1352/1770 train_time:134250ms step_avg:99.30ms
step:1353/1770 train_time:134354ms step_avg:99.30ms
step:1354/1770 train_time:134458ms step_avg:99.30ms
step:1355/1770 train_time:134561ms step_avg:99.31ms
step:1356/1770 train_time:134665ms step_avg:99.31ms
step:1357/1770 train_time:134769ms step_avg:99.31ms
step:1358/1770 train_time:134873ms step_avg:99.32ms
step:1359/1770 train_time:134977ms step_avg:99.32ms
step:1360/1770 train_time:135082ms step_avg:99.32ms
step:1361/1770 train_time:135186ms step_avg:99.33ms
step:1362/1770 train_time:135290ms step_avg:99.33ms
step:1363/1770 train_time:135393ms step_avg:99.33ms
step:1364/1770 train_time:135498ms step_avg:99.34ms
step:1365/1770 train_time:135601ms step_avg:99.34ms
step:1366/1770 train_time:135704ms step_avg:99.34ms
step:1367/1770 train_time:135809ms step_avg:99.35ms
step:1368/1770 train_time:135913ms step_avg:99.35ms
step:1369/1770 train_time:136017ms step_avg:99.35ms
step:1370/1770 train_time:136121ms step_avg:99.36ms
step:1371/1770 train_time:136225ms step_avg:99.36ms
step:1372/1770 train_time:136328ms step_avg:99.36ms
step:1373/1770 train_time:136432ms step_avg:99.37ms
step:1374/1770 train_time:136537ms step_avg:99.37ms
step:1375/1770 train_time:136641ms step_avg:99.38ms
step:1375/1770 val_loss:3.3803 train_time:136743ms step_avg:99.45ms
step:1376/1770 train_time:136761ms step_avg:99.39ms
step:1377/1770 train_time:136859ms step_avg:99.39ms
step:1378/1770 train_time:136964ms step_avg:99.39ms
step:1379/1770 train_time:137067ms step_avg:99.40ms
step:1380/1770 train_time:137170ms step_avg:99.40ms
step:1381/1770 train_time:137272ms step_avg:99.40ms
step:1382/1770 train_time:137376ms step_avg:99.40ms
step:1383/1770 train_time:137479ms step_avg:99.41ms
step:1384/1770 train_time:137582ms step_avg:99.41ms
step:1385/1770 train_time:137684ms step_avg:99.41ms
step:1386/1770 train_time:137790ms step_avg:99.42ms
step:1387/1770 train_time:137896ms step_avg:99.42ms
step:1388/1770 train_time:138001ms step_avg:99.42ms
step:1389/1770 train_time:138106ms step_avg:99.43ms
step:1390/1770 train_time:138209ms step_avg:99.43ms
step:1391/1770 train_time:138312ms step_avg:99.43ms
step:1392/1770 train_time:138415ms step_avg:99.44ms
step:1393/1770 train_time:138517ms step_avg:99.44ms
step:1394/1770 train_time:138620ms step_avg:99.44ms
step:1395/1770 train_time:138724ms step_avg:99.44ms
step:1396/1770 train_time:138829ms step_avg:99.45ms
step:1397/1770 train_time:138934ms step_avg:99.45ms
step:1398/1770 train_time:139038ms step_avg:99.46ms
step:1399/1770 train_time:139143ms step_avg:99.46ms
step:1400/1770 train_time:139247ms step_avg:99.46ms
step:1401/1770 train_time:139350ms step_avg:99.46ms
step:1402/1770 train_time:139453ms step_avg:99.47ms
step:1403/1770 train_time:139556ms step_avg:99.47ms
step:1404/1770 train_time:139660ms step_avg:99.47ms
step:1405/1770 train_time:139764ms step_avg:99.48ms
step:1406/1770 train_time:139868ms step_avg:99.48ms
step:1407/1770 train_time:139972ms step_avg:99.48ms
step:1408/1770 train_time:140077ms step_avg:99.49ms
step:1409/1770 train_time:140182ms step_avg:99.49ms
step:1410/1770 train_time:140286ms step_avg:99.49ms
step:1411/1770 train_time:140389ms step_avg:99.50ms
step:1412/1770 train_time:140491ms step_avg:99.50ms
step:1413/1770 train_time:140595ms step_avg:99.50ms
step:1414/1770 train_time:140699ms step_avg:99.50ms
step:1415/1770 train_time:140803ms step_avg:99.51ms
step:1416/1770 train_time:140908ms step_avg:99.51ms
step:1417/1770 train_time:141012ms step_avg:99.51ms
step:1418/1770 train_time:141115ms step_avg:99.52ms
step:1419/1770 train_time:141219ms step_avg:99.52ms
step:1420/1770 train_time:141325ms step_avg:99.52ms
step:1421/1770 train_time:141429ms step_avg:99.53ms
step:1422/1770 train_time:141532ms step_avg:99.53ms
step:1423/1770 train_time:141635ms step_avg:99.53ms
step:1424/1770 train_time:141738ms step_avg:99.54ms
step:1425/1770 train_time:141844ms step_avg:99.54ms
step:1426/1770 train_time:141948ms step_avg:99.54ms
step:1427/1770 train_time:142050ms step_avg:99.54ms
step:1428/1770 train_time:142157ms step_avg:99.55ms
step:1429/1770 train_time:142261ms step_avg:99.55ms
step:1430/1770 train_time:142365ms step_avg:99.56ms
step:1431/1770 train_time:142469ms step_avg:99.56ms
step:1432/1770 train_time:142572ms step_avg:99.56ms
step:1433/1770 train_time:142675ms step_avg:99.56ms
step:1434/1770 train_time:142778ms step_avg:99.57ms
step:1435/1770 train_time:142882ms step_avg:99.57ms
step:1436/1770 train_time:142988ms step_avg:99.57ms
step:1437/1770 train_time:143092ms step_avg:99.58ms
step:1438/1770 train_time:143195ms step_avg:99.58ms
step:1439/1770 train_time:143298ms step_avg:99.58ms
step:1440/1770 train_time:143402ms step_avg:99.58ms
step:1441/1770 train_time:143508ms step_avg:99.59ms
step:1442/1770 train_time:143612ms step_avg:99.59ms
step:1443/1770 train_time:143715ms step_avg:99.59ms
step:1444/1770 train_time:143818ms step_avg:99.60ms
step:1445/1770 train_time:143923ms step_avg:99.60ms
step:1446/1770 train_time:144028ms step_avg:99.60ms
step:1447/1770 train_time:144132ms step_avg:99.61ms
step:1448/1770 train_time:144236ms step_avg:99.61ms
step:1449/1770 train_time:144342ms step_avg:99.61ms
step:1450/1770 train_time:144447ms step_avg:99.62ms
step:1451/1770 train_time:144552ms step_avg:99.62ms
step:1452/1770 train_time:144656ms step_avg:99.63ms
step:1453/1770 train_time:144760ms step_avg:99.63ms
step:1454/1770 train_time:144866ms step_avg:99.63ms
step:1455/1770 train_time:144972ms step_avg:99.64ms
step:1456/1770 train_time:145077ms step_avg:99.64ms
step:1457/1770 train_time:145182ms step_avg:99.64ms
step:1458/1770 train_time:145287ms step_avg:99.65ms
step:1459/1770 train_time:145392ms step_avg:99.65ms
step:1460/1770 train_time:145496ms step_avg:99.66ms
step:1461/1770 train_time:145601ms step_avg:99.66ms
step:1462/1770 train_time:145707ms step_avg:99.66ms
step:1463/1770 train_time:145811ms step_avg:99.67ms
step:1464/1770 train_time:145917ms step_avg:99.67ms
step:1465/1770 train_time:146021ms step_avg:99.67ms
step:1466/1770 train_time:146126ms step_avg:99.68ms
step:1467/1770 train_time:146231ms step_avg:99.68ms
step:1468/1770 train_time:146336ms step_avg:99.68ms
step:1469/1770 train_time:146440ms step_avg:99.69ms
step:1470/1770 train_time:146545ms step_avg:99.69ms
step:1471/1770 train_time:146650ms step_avg:99.69ms
step:1472/1770 train_time:146755ms step_avg:99.70ms
step:1473/1770 train_time:146860ms step_avg:99.70ms
step:1474/1770 train_time:146967ms step_avg:99.71ms
step:1475/1770 train_time:147072ms step_avg:99.71ms
step:1476/1770 train_time:147176ms step_avg:99.71ms
step:1477/1770 train_time:147282ms step_avg:99.72ms
step:1478/1770 train_time:147387ms step_avg:99.72ms
step:1479/1770 train_time:147492ms step_avg:99.72ms
step:1480/1770 train_time:147596ms step_avg:99.73ms
step:1481/1770 train_time:147704ms step_avg:99.73ms
step:1482/1770 train_time:147808ms step_avg:99.74ms
step:1483/1770 train_time:147914ms step_avg:99.74ms
step:1484/1770 train_time:148019ms step_avg:99.74ms
step:1485/1770 train_time:148124ms step_avg:99.75ms
step:1486/1770 train_time:148229ms step_avg:99.75ms
step:1487/1770 train_time:148332ms step_avg:99.75ms
step:1488/1770 train_time:148437ms step_avg:99.76ms
step:1489/1770 train_time:148542ms step_avg:99.76ms
step:1490/1770 train_time:148647ms step_avg:99.76ms
step:1491/1770 train_time:148752ms step_avg:99.77ms
step:1492/1770 train_time:148859ms step_avg:99.77ms
step:1493/1770 train_time:148967ms step_avg:99.78ms
step:1494/1770 train_time:149073ms step_avg:99.78ms
step:1495/1770 train_time:149177ms step_avg:99.78ms
step:1496/1770 train_time:149281ms step_avg:99.79ms
step:1497/1770 train_time:149386ms step_avg:99.79ms
step:1498/1770 train_time:149490ms step_avg:99.79ms
step:1499/1770 train_time:149593ms step_avg:99.80ms
step:1500/1770 train_time:149697ms step_avg:99.80ms
step:1500/1770 val_loss:3.3425 train_time:149802ms step_avg:99.87ms
step:1501/1770 train_time:149820ms step_avg:99.81ms
step:1502/1770 train_time:149917ms step_avg:99.81ms
step:1503/1770 train_time:150022ms step_avg:99.81ms
step:1504/1770 train_time:150127ms step_avg:99.82ms
step:1505/1770 train_time:150233ms step_avg:99.82ms
step:1506/1770 train_time:150338ms step_avg:99.83ms
step:1507/1770 train_time:150442ms step_avg:99.83ms
step:1508/1770 train_time:150547ms step_avg:99.83ms
step:1509/1770 train_time:150650ms step_avg:99.83ms
step:1510/1770 train_time:150754ms step_avg:99.84ms
step:1511/1770 train_time:150860ms step_avg:99.84ms
step:1512/1770 train_time:150967ms step_avg:99.85ms
step:1513/1770 train_time:151072ms step_avg:99.85ms
step:1514/1770 train_time:151176ms step_avg:99.85ms
step:1515/1770 train_time:151281ms step_avg:99.86ms
step:1516/1770 train_time:151386ms step_avg:99.86ms
step:1517/1770 train_time:151492ms step_avg:99.86ms
step:1518/1770 train_time:151597ms step_avg:99.87ms
step:1519/1770 train_time:151700ms step_avg:99.87ms
step:1520/1770 train_time:151806ms step_avg:99.87ms
step:1521/1770 train_time:151911ms step_avg:99.88ms
step:1522/1770 train_time:152017ms step_avg:99.88ms
step:1523/1770 train_time:152123ms step_avg:99.88ms
step:1524/1770 train_time:152227ms step_avg:99.89ms
step:1525/1770 train_time:152331ms step_avg:99.89ms
step:1526/1770 train_time:152436ms step_avg:99.89ms
step:1527/1770 train_time:152540ms step_avg:99.90ms
step:1528/1770 train_time:152646ms step_avg:99.90ms
step:1529/1770 train_time:152750ms step_avg:99.90ms
step:1530/1770 train_time:152856ms step_avg:99.91ms
step:1531/1770 train_time:152959ms step_avg:99.91ms
step:1532/1770 train_time:153066ms step_avg:99.91ms
step:1533/1770 train_time:153171ms step_avg:99.92ms
step:1534/1770 train_time:153277ms step_avg:99.92ms
step:1535/1770 train_time:153381ms step_avg:99.92ms
step:1536/1770 train_time:153486ms step_avg:99.93ms
step:1537/1770 train_time:153591ms step_avg:99.93ms
step:1538/1770 train_time:153698ms step_avg:99.93ms
step:1539/1770 train_time:153802ms step_avg:99.94ms
step:1540/1770 train_time:153908ms step_avg:99.94ms
step:1541/1770 train_time:154015ms step_avg:99.94ms
step:1542/1770 train_time:154120ms step_avg:99.95ms
step:1543/1770 train_time:154225ms step_avg:99.95ms
step:1544/1770 train_time:154330ms step_avg:99.95ms
step:1545/1770 train_time:154435ms step_avg:99.96ms
step:1546/1770 train_time:154540ms step_avg:99.96ms
step:1547/1770 train_time:154645ms step_avg:99.96ms
step:1548/1770 train_time:154750ms step_avg:99.97ms
step:1549/1770 train_time:154854ms step_avg:99.97ms
step:1550/1770 train_time:154960ms step_avg:99.97ms
step:1551/1770 train_time:155064ms step_avg:99.98ms
step:1552/1770 train_time:155170ms step_avg:99.98ms
step:1553/1770 train_time:155274ms step_avg:99.98ms
step:1554/1770 train_time:155378ms step_avg:99.99ms
step:1555/1770 train_time:155484ms step_avg:99.99ms
step:1556/1770 train_time:155589ms step_avg:99.99ms
step:1557/1770 train_time:155693ms step_avg:100.00ms
step:1558/1770 train_time:155797ms step_avg:100.00ms
step:1559/1770 train_time:155902ms step_avg:100.00ms
step:1560/1770 train_time:156007ms step_avg:100.00ms
step:1561/1770 train_time:156114ms step_avg:100.01ms
step:1562/1770 train_time:156218ms step_avg:100.01ms
step:1563/1770 train_time:156323ms step_avg:100.01ms
step:1564/1770 train_time:156427ms step_avg:100.02ms
step:1565/1770 train_time:156531ms step_avg:100.02ms
step:1566/1770 train_time:156635ms step_avg:100.02ms
step:1567/1770 train_time:156740ms step_avg:100.03ms
step:1568/1770 train_time:156845ms step_avg:100.03ms
step:1569/1770 train_time:156952ms step_avg:100.03ms
step:1570/1770 train_time:157057ms step_avg:100.04ms
step:1571/1770 train_time:157163ms step_avg:100.04ms
step:1572/1770 train_time:157269ms step_avg:100.04ms
step:1573/1770 train_time:157375ms step_avg:100.05ms
step:1574/1770 train_time:157479ms step_avg:100.05ms
step:1575/1770 train_time:157584ms step_avg:100.05ms
step:1576/1770 train_time:157689ms step_avg:100.06ms
step:1577/1770 train_time:157794ms step_avg:100.06ms
step:1578/1770 train_time:157900ms step_avg:100.06ms
step:1579/1770 train_time:158006ms step_avg:100.07ms
step:1580/1770 train_time:158110ms step_avg:100.07ms
step:1581/1770 train_time:158216ms step_avg:100.07ms
step:1582/1770 train_time:158322ms step_avg:100.08ms
step:1583/1770 train_time:158427ms step_avg:100.08ms
step:1584/1770 train_time:158532ms step_avg:100.08ms
step:1585/1770 train_time:158637ms step_avg:100.09ms
step:1586/1770 train_time:158744ms step_avg:100.09ms
step:1587/1770 train_time:158850ms step_avg:100.09ms
step:1588/1770 train_time:158955ms step_avg:100.10ms
step:1589/1770 train_time:159061ms step_avg:100.10ms
step:1590/1770 train_time:159165ms step_avg:100.10ms
step:1591/1770 train_time:159269ms step_avg:100.11ms
step:1592/1770 train_time:159375ms step_avg:100.11ms
step:1593/1770 train_time:159479ms step_avg:100.11ms
step:1594/1770 train_time:159583ms step_avg:100.11ms
step:1595/1770 train_time:159689ms step_avg:100.12ms
step:1596/1770 train_time:159795ms step_avg:100.12ms
step:1597/1770 train_time:159899ms step_avg:100.12ms
step:1598/1770 train_time:160004ms step_avg:100.13ms
step:1599/1770 train_time:160110ms step_avg:100.13ms
step:1600/1770 train_time:160217ms step_avg:100.14ms
step:1601/1770 train_time:160323ms step_avg:100.14ms
step:1602/1770 train_time:160429ms step_avg:100.14ms
step:1603/1770 train_time:160533ms step_avg:100.15ms
step:1604/1770 train_time:160637ms step_avg:100.15ms
step:1605/1770 train_time:160742ms step_avg:100.15ms
step:1606/1770 train_time:160849ms step_avg:100.15ms
step:1607/1770 train_time:160956ms step_avg:100.16ms
step:1608/1770 train_time:161062ms step_avg:100.16ms
step:1609/1770 train_time:161168ms step_avg:100.17ms
step:1610/1770 train_time:161272ms step_avg:100.17ms
step:1611/1770 train_time:161378ms step_avg:100.17ms
step:1612/1770 train_time:161485ms step_avg:100.18ms
step:1613/1770 train_time:161589ms step_avg:100.18ms
step:1614/1770 train_time:161694ms step_avg:100.18ms
step:1615/1770 train_time:161799ms step_avg:100.19ms
step:1616/1770 train_time:161905ms step_avg:100.19ms
step:1617/1770 train_time:162012ms step_avg:100.19ms
step:1618/1770 train_time:162118ms step_avg:100.20ms
step:1619/1770 train_time:162222ms step_avg:100.20ms
step:1620/1770 train_time:162328ms step_avg:100.20ms
step:1621/1770 train_time:162432ms step_avg:100.20ms
step:1622/1770 train_time:162537ms step_avg:100.21ms
step:1623/1770 train_time:162645ms step_avg:100.21ms
step:1624/1770 train_time:162750ms step_avg:100.22ms
step:1625/1770 train_time:162854ms step_avg:100.22ms
step:1625/1770 val_loss:3.3084 train_time:162959ms step_avg:100.28ms
step:1626/1770 train_time:162977ms step_avg:100.23ms
step:1627/1770 train_time:163070ms step_avg:100.23ms
step:1628/1770 train_time:163174ms step_avg:100.23ms
step:1629/1770 train_time:163278ms step_avg:100.23ms
step:1630/1770 train_time:163382ms step_avg:100.23ms
step:1631/1770 train_time:163486ms step_avg:100.24ms
step:1632/1770 train_time:163589ms step_avg:100.24ms
step:1633/1770 train_time:163694ms step_avg:100.24ms
step:1634/1770 train_time:163798ms step_avg:100.24ms
step:1635/1770 train_time:163903ms step_avg:100.25ms
step:1636/1770 train_time:164010ms step_avg:100.25ms
step:1637/1770 train_time:164117ms step_avg:100.25ms
step:1638/1770 train_time:164221ms step_avg:100.26ms
step:1639/1770 train_time:164326ms step_avg:100.26ms
step:1640/1770 train_time:164431ms step_avg:100.26ms
step:1641/1770 train_time:164535ms step_avg:100.27ms
step:1642/1770 train_time:164639ms step_avg:100.27ms
step:1643/1770 train_time:164743ms step_avg:100.27ms
step:1644/1770 train_time:164849ms step_avg:100.27ms
step:1645/1770 train_time:164953ms step_avg:100.28ms
step:1646/1770 train_time:165059ms step_avg:100.28ms
step:1647/1770 train_time:165165ms step_avg:100.28ms
step:1648/1770 train_time:165269ms step_avg:100.28ms
step:1649/1770 train_time:165374ms step_avg:100.29ms
step:1650/1770 train_time:165479ms step_avg:100.29ms
step:1651/1770 train_time:165582ms step_avg:100.29ms
step:1652/1770 train_time:165687ms step_avg:100.29ms
step:1653/1770 train_time:165791ms step_avg:100.30ms
step:1654/1770 train_time:165898ms step_avg:100.30ms
step:1655/1770 train_time:166006ms step_avg:100.31ms
step:1656/1770 train_time:166110ms step_avg:100.31ms
step:1657/1770 train_time:166218ms step_avg:100.31ms
step:1658/1770 train_time:166323ms step_avg:100.32ms
step:1659/1770 train_time:166429ms step_avg:100.32ms
step:1660/1770 train_time:166535ms step_avg:100.32ms
step:1661/1770 train_time:166642ms step_avg:100.33ms
step:1662/1770 train_time:166746ms step_avg:100.33ms
step:1663/1770 train_time:166850ms step_avg:100.33ms
step:1664/1770 train_time:166955ms step_avg:100.33ms
step:1665/1770 train_time:167060ms step_avg:100.34ms
step:1666/1770 train_time:167166ms step_avg:100.34ms
step:1667/1770 train_time:167271ms step_avg:100.34ms
step:1668/1770 train_time:167375ms step_avg:100.34ms
step:1669/1770 train_time:167480ms step_avg:100.35ms
step:1670/1770 train_time:167585ms step_avg:100.35ms
step:1671/1770 train_time:167691ms step_avg:100.35ms
step:1672/1770 train_time:167796ms step_avg:100.36ms
step:1673/1770 train_time:167902ms step_avg:100.36ms
step:1674/1770 train_time:168007ms step_avg:100.36ms
step:1675/1770 train_time:168111ms step_avg:100.36ms
step:1676/1770 train_time:168215ms step_avg:100.37ms
step:1677/1770 train_time:168324ms step_avg:100.37ms
step:1678/1770 train_time:168428ms step_avg:100.37ms
step:1679/1770 train_time:168532ms step_avg:100.38ms
step:1680/1770 train_time:168637ms step_avg:100.38ms
step:1681/1770 train_time:168742ms step_avg:100.38ms
step:1682/1770 train_time:168850ms step_avg:100.39ms
step:1683/1770 train_time:168954ms step_avg:100.39ms
step:1684/1770 train_time:169058ms step_avg:100.39ms
step:1685/1770 train_time:169163ms step_avg:100.39ms
step:1686/1770 train_time:169269ms step_avg:100.40ms
step:1687/1770 train_time:169377ms step_avg:100.40ms
step:1688/1770 train_time:169481ms step_avg:100.40ms
step:1689/1770 train_time:169585ms step_avg:100.41ms
step:1690/1770 train_time:169689ms step_avg:100.41ms
step:1691/1770 train_time:169793ms step_avg:100.41ms
step:1692/1770 train_time:169898ms step_avg:100.41ms
step:1693/1770 train_time:170004ms step_avg:100.42ms
step:1694/1770 train_time:170109ms step_avg:100.42ms
step:1695/1770 train_time:170213ms step_avg:100.42ms
step:1696/1770 train_time:170320ms step_avg:100.42ms
step:1697/1770 train_time:170427ms step_avg:100.43ms
step:1698/1770 train_time:170531ms step_avg:100.43ms
step:1699/1770 train_time:170637ms step_avg:100.43ms
step:1700/1770 train_time:170741ms step_avg:100.44ms
step:1701/1770 train_time:170846ms step_avg:100.44ms
step:1702/1770 train_time:170952ms step_avg:100.44ms
step:1703/1770 train_time:171057ms step_avg:100.44ms
step:1704/1770 train_time:171162ms step_avg:100.45ms
step:1705/1770 train_time:171266ms step_avg:100.45ms
step:1706/1770 train_time:171370ms step_avg:100.45ms
step:1707/1770 train_time:171477ms step_avg:100.46ms
step:1708/1770 train_time:171582ms step_avg:100.46ms
step:1709/1770 train_time:171689ms step_avg:100.46ms
step:1710/1770 train_time:171799ms step_avg:100.47ms
step:1711/1770 train_time:171907ms step_avg:100.47ms
step:1712/1770 train_time:172014ms step_avg:100.48ms
step:1713/1770 train_time:172118ms step_avg:100.48ms
step:1714/1770 train_time:172223ms step_avg:100.48ms
step:1715/1770 train_time:172328ms step_avg:100.48ms
step:1716/1770 train_time:172434ms step_avg:100.49ms
step:1717/1770 train_time:172541ms step_avg:100.49ms
step:1718/1770 train_time:172646ms step_avg:100.49ms
step:1719/1770 train_time:172753ms step_avg:100.50ms
step:1720/1770 train_time:172861ms step_avg:100.50ms
step:1721/1770 train_time:172967ms step_avg:100.50ms
step:1722/1770 train_time:173074ms step_avg:100.51ms
step:1723/1770 train_time:173181ms step_avg:100.51ms
step:1724/1770 train_time:173289ms step_avg:100.52ms
step:1725/1770 train_time:173396ms step_avg:100.52ms
step:1726/1770 train_time:173504ms step_avg:100.52ms
step:1727/1770 train_time:173609ms step_avg:100.53ms
step:1728/1770 train_time:173716ms step_avg:100.53ms
step:1729/1770 train_time:173822ms step_avg:100.53ms
step:1730/1770 train_time:173928ms step_avg:100.54ms
step:1731/1770 train_time:174035ms step_avg:100.54ms
step:1732/1770 train_time:174140ms step_avg:100.54ms
step:1733/1770 train_time:174247ms step_avg:100.55ms
step:1734/1770 train_time:174353ms step_avg:100.55ms
step:1735/1770 train_time:174460ms step_avg:100.55ms
step:1736/1770 train_time:174565ms step_avg:100.56ms
step:1737/1770 train_time:174671ms step_avg:100.56ms
step:1738/1770 train_time:174777ms step_avg:100.56ms
step:1739/1770 train_time:174883ms step_avg:100.57ms
step:1740/1770 train_time:174988ms step_avg:100.57ms
step:1741/1770 train_time:175095ms step_avg:100.57ms
step:1742/1770 train_time:175203ms step_avg:100.58ms
step:1743/1770 train_time:175308ms step_avg:100.58ms
step:1744/1770 train_time:175414ms step_avg:100.58ms
step:1745/1770 train_time:175519ms step_avg:100.58ms
step:1746/1770 train_time:175628ms step_avg:100.59ms
step:1747/1770 train_time:175732ms step_avg:100.59ms
step:1748/1770 train_time:175840ms step_avg:100.60ms
step:1749/1770 train_time:175946ms step_avg:100.60ms
step:1750/1770 train_time:176052ms step_avg:100.60ms
step:1750/1770 val_loss:3.2818 train_time:176157ms step_avg:100.66ms
step:1751/1770 train_time:176175ms step_avg:100.61ms
step:1752/1770 train_time:176269ms step_avg:100.61ms
step:1753/1770 train_time:176375ms step_avg:100.61ms
step:1754/1770 train_time:176480ms step_avg:100.62ms
step:1755/1770 train_time:176585ms step_avg:100.62ms
step:1756/1770 train_time:176691ms step_avg:100.62ms
step:1757/1770 train_time:176797ms step_avg:100.62ms
step:1758/1770 train_time:176902ms step_avg:100.63ms
step:1759/1770 train_time:177008ms step_avg:100.63ms
step:1760/1770 train_time:177115ms step_avg:100.63ms
step:1761/1770 train_time:177222ms step_avg:100.64ms
step:1762/1770 train_time:177332ms step_avg:100.64ms
step:1763/1770 train_time:177436ms step_avg:100.64ms
step:1764/1770 train_time:177543ms step_avg:100.65ms
step:1765/1770 train_time:177649ms step_avg:100.65ms
step:1766/1770 train_time:177757ms step_avg:100.66ms
step:1767/1770 train_time:177862ms step_avg:100.66ms
step:1768/1770 train_time:177968ms step_avg:100.66ms
step:1769/1770 train_time:178073ms step_avg:100.66ms
step:1770/1770 train_time:178179ms step_avg:100.67ms
step:1770/1770 val_loss:3.2793 train_time:178286ms step_avg:100.73ms
peak memory allocated: 30724 MiB reserved: 46452 MiB
