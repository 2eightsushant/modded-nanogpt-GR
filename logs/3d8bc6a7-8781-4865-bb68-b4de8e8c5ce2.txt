import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
import copy
import glob
from dataclasses import dataclass
from functools import lru_cache, partial # Added partial for hook registration
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
#torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params: list[Tensor] = [*params]
        param_groups = []
        for size in {p.numel() for p in params}:
            b = torch.empty(world_size, size, dtype=torch.bfloat16, device="cuda")
            group = dict(params=[p for p in params if p.numel() == size],
                         update_buffer=b, update_buffer_views=[b[i] for i in range(world_size)])
            param_groups.append(group)
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            update_buffer: Tensor = group["update_buffer"]
            update_buffer_views: list[Tensor] = group["update_buffer_views"]
            # generate weight updates in distributed fashion
            params: list[Tensor] = group["params"]
            handle = None
            params_world = None
            def update_prev(): # optimized Muon implementation contributed by @YouJiacheng
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffer_views):
                    p_world.add_(g_world.view_as(p_world),
                                 alpha=-group["lr"] * max(1, p_world.size(-2) / p_world.size(-1))**0.5)
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if "momentum_buffer" not in state:
                        state["momentum_buffer"] = torch.zeros_like(g)
                    buf: Tensor = state["momentum_buffer"]
                    buf.lerp_(g, 1 - group["momentum"])
                    g = g.lerp_(buf, group["momentum"]) if group["nesterov"] else buf
                    g = zeropower_via_newtonschulz5(g, steps=group["ns_steps"]).flatten()
                else:
                    g = update_buffer_views[self.rank]
                if base_i > 0:
                    update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=0.12).transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, block_mask: BlockMask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, max_seq_len, i) for i in range(num_layers)])
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128),
                                    use_fp8=True, x_s=(model_dim**0.5)/448, w_s=24/448, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        self.skip_weights = nn.Parameter(torch.ones(num_layers//2))

    def create_blockmasks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        docs = (input_seq == 50256).cumsum(0)

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_blockmask: Tensor):
            num_blocks = dense_blockmask.sum(dim=-1, dtype=torch.int32)
            indices = dense_blockmask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
        causal_blockmask_any = block_idx[:, None] >= block_idx
        causal_blockmask_all = block_idx[:, None] > block_idx
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
        document_blockmask_any = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
        document_blockmask_all = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
        blockmask_any = causal_blockmask_any & document_blockmask_any
        blockmask_all = causal_blockmask_all & document_blockmask_all
        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(blockmask_any & ~blockmask_all)
        full_kv_num_blocks, full_kv_indices = dense_to_ordered(blockmask_all)
        def build_bm(window_size_blocks: Tensor) -> BlockMask:
            return BlockMask.from_kv_blocks(
                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(window_size_blocks - full_kv_num_blocks, 1)),
                partial_kv_indices,
                torch.clamp_max(full_kv_num_blocks, window_size_blocks - 1),
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        long_bm, short_bm = self.create_blockmasks(input_seq, sliding_window_num_blocks)
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, long_bm, short_bm, short_bm, short_bm, long_bm]
        assert len(block_masks) == len(self.blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977

        # U-net design by @brendanh0gan
        skip_connections = []
        n = len(self.skip_weights)
        for i in range(len(self.blocks)):
            if i >= n:
                x = x + self.skip_weights[i - n] * skip_connections.pop()
            x = self.blocks[i](x, ve[i], x0, block_masks[i])
            if i < n:
                skip_connections.append(x)

        x = norm(x)
        logits = self.lm_head(x).float()
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / (7.5 * x.size(-1)**0.5))
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq, reduction='sum' if self.training else 'mean')
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn't helpful.
        pos += batch_size
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_seq_len = 48*1024 # FlexAttention sequence length
    val_seq_len = 4*64*1024 # FlexAttention sequence length for validation
    # optimization
    num_iterations = 1770 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    # architecture
    vocab_size = 50257
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert world_size == 8 # this code is designed for 8xH100
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs("logs_bucket", exist_ok=True)
    logfile = f"logs_bucket/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

########################################
#    Construct model and optimizer     #
########################################

model: nn.Module = GPT(vocab_size=args.vocab_size, num_layers=12, num_heads=6, model_dim=768,
                       max_seq_len=max(args.train_seq_len, args.val_seq_len)).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
adam_params = [dict(params=head_params, lr=0.22), dict(params=embed_params, lr=0.6), dict(params=scalar_params, lr=0.04)]
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = torch.optim.Adam(adam_params, betas=(0.8, 0.95), eps=1e-10, fused=True)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then decay
def get_lr(step: int):
    x = step / args.num_iterations # progress in training
    assert 0 <= x < 1
    if x < 1 - args.cooldown_frac:
        return 1.0
    else:
        w = (1 - x) / args.cooldown_frac
        return w * 1.0 + (1 - w) * 0.1

# attention window size schedule: linearly increase
@lru_cache(1)
def get_window_size_blocks_helper(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)
def get_window_size_blocks(step: int):
    x = step / args.num_iterations # progress in training
    assert 0 <= x <= 1
    # Linearly increase the block-wise sliding window size over training 128 -> 1792
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * x, n=128)
    return get_window_size_blocks_helper(window_size)

model: nn.Module = torch.compile(model, dynamic=False)

########################################
#     Warmup kernel and Layerwise Bucketing   #
########################################

# Track the gradient readiness order during warmup
gradient_ready_order = []

def record_grad_order_hook(param):
    def hook(*_):
        if param.grad is not None and all(param is not p for p in gradient_ready_order):
            gradient_ready_order.append(param)
    return hook

# Register temp hooks to track when each parameter's gradient is ready
tmp_handles = []
for p in model.parameters():
    if p.requires_grad:
        handle = p.register_post_accumulate_grad_hook(record_grad_order_hook(p))
        tmp_handles.append(handle)

warmup_steps = 10
initial_state = dict(
    model=copy.deepcopy(model.state_dict()),
    optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]
)

for step in range(warmup_steps):
    # Create a local generator with fixed, step-dependent seed
    g = torch.Generator(device="cuda")
    g.manual_seed(1337 + step)

    # Use this generator to sample inputs
    inputs = targets = torch.randint(
        low=0,
        high=args.vocab_size,
        size=(args.train_seq_len,),
        device="cuda",
        dtype=torch.long,
        generator=g,  # <- Local generator ensures deterministic and isolated RNG
    )
    model(inputs.to(torch.int32), targets, get_window_size_blocks(0)).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)

# Remove hooks after warmup
for h in tmp_handles:
    h.remove()

# Restore model/optimizer state to ensure a clean start
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del initial_state

print0(f"Collected {len(gradient_ready_order)} parameters in gradient-ready order")

########################################
#      Overlap Communication Setup     #
########################################

# Layer-aware bucketing using gradient-ready order
def create_buckets_layerwise(ordered_params, bucket_size_mb=25):
    """Group parameters into buckets of ~bucket_size_mb MB, in gradient ready order"""
    buckets = []
    current_bucket = []
    current_size = 0

    for param in ordered_params:
        if not param.requires_grad:
            continue
        param_size_mb = param.numel() * param.element_size() / (1024 * 1024)

        if current_size + param_size_mb > bucket_size_mb and current_bucket:
            buckets.append(current_bucket)
            current_bucket = [param]
            current_size = param_size_mb
        else:
            current_bucket.append(param)
            current_size += param_size_mb

    if current_bucket:
        buckets.append(current_bucket)

    return buckets

# Create buckets for all parameters
all_params = [p for p in model.parameters() if p.requires_grad]
param_buckets = create_buckets_layerwise(gradient_ready_order)
print0(f"Created {len(param_buckets)} gradient buckets")
for i, bucket in enumerate(param_buckets):
    total_size = sum(p.numel() * p.element_size() for p in bucket) / (1024 * 1024)
    print0(f"Bucket {i}: {len(bucket)} params, {total_size:.1f} MB")

# Bucket state tracking
bucket_ready_count = [0] * len(param_buckets)
bucket_handles = [None] * len(param_buckets)
param_to_bucket = {}

# Map each parameter to its bucket index
for bucket_idx, bucket in enumerate(param_buckets):
    for param in bucket:
        param_to_bucket[param] = bucket_idx

def _gradient_hook(param: Tensor):
    """Called when a parameter's gradient is ready"""
    if param.grad is None:
        return

    bucket_idx = param_to_bucket[param]
    bucket_ready_count[bucket_idx] += 1

    # Check if all parameters in this bucket are ready
    if bucket_ready_count[bucket_idx] == len(param_buckets[bucket_idx]):
        # All-reduce this bucket
        bucket_grads = [p.grad for p in param_buckets[bucket_idx]]

        # For multi-tensor operations, we can reduce them together
        if len(bucket_grads) == 1:
            handle = dist.all_reduce(bucket_grads[0], op=dist.ReduceOp.AVG, async_op=True)
        else:
            # Use multi-tensor all-reduce for efficiency
            handle = dist.all_reduce_coalesced(bucket_grads, op=dist.ReduceOp.AVG, async_op=True)

        bucket_handles[bucket_idx] = handle

# Register hooks for all parameters
print0("Registering bucketed gradient hooks...")
for param in all_params:
    param.register_post_accumulate_grad_hook(_gradient_hook)

def wait_for_gradients():
    """Wait for all gradient reductions to complete and reset bucket state"""
    for handle in bucket_handles:
        if handle is not None:
            handle.wait()

    # Reset state for next iteration
    for i in range(len(bucket_ready_count)):
        bucket_ready_count[i] = 0
        bucket_handles[i] = None

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, world_size * args.train_seq_len, rank, world_size)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_batch_size = world_size * args.val_seq_len
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        val_loader = distributed_data_generator(args.val_files, val_batch_size, rank, world_size)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets = next(val_loader)
                val_loss += model(inputs, targets, get_window_size_blocks(step))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs_bucket/{run_id}", exist_ok=True)
            torch.save(log, f"logs_bucket/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    inputs, targets = next(train_loader)
    model(inputs, targets, get_window_size_blocks(step)).backward()
    #for param in model.parameters():
    #    dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
    wait_for_gradients() # does the same thing as commented two lines above, but faster

    # set optimization hyperparameters
    for opt in optimizers:
        for group in opt.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)
    for group in optimizer2.param_groups:
        frac = min(step / 300, 1) # momentum warmup for muon
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers
    for opt in optimizers:
        opt.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Feb  4 2025, 14:57:36) [GCC 11.4.0]
Running PyTorch 2.8.0.dev20250612+cu126 compiled for CUDA 12.6
Fri Jun 13 05:26:14 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.08             Driver Version: 550.127.08     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   42C    P0            128W /  700W |    5824MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   34C    P0            127W /  700W |    1514MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   32C    P0            121W /  700W |    1514MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   39C    P0            124W /  700W |    1514MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   40C    P0            121W /  700W |    1514MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   32C    P0            119W /  700W |    1514MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   39C    P0            126W /  700W |    1514MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   31C    P0            115W /  700W |    1514MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
Collected 75 parameters in gradient-ready order
Created 21 gradient buckets
Bucket 0: 1 params, 73.6 MB
Bucket 1: 1 params, 73.6 MB
Bucket 2: 4 params, 15.8 MB
Bucket 3: 1 params, 147.4 MB
Bucket 4: 4 params, 20.3 MB
Bucket 5: 4 params, 24.8 MB
Bucket 6: 4 params, 18.0 MB
Bucket 7: 5 params, 18.0 MB
Bucket 8: 4 params, 20.3 MB
Bucket 9: 5 params, 24.8 MB
Bucket 10: 4 params, 20.3 MB
Bucket 11: 4 params, 24.8 MB
Bucket 12: 5 params, 18.0 MB
Bucket 13: 5 params, 20.3 MB
Bucket 14: 5 params, 18.0 MB
Bucket 15: 4 params, 20.3 MB
Bucket 16: 5 params, 24.8 MB
Bucket 17: 4 params, 24.8 MB
Bucket 18: 4 params, 2.3 MB
Bucket 19: 1 params, 73.6 MB
Bucket 20: 1 params, 73.6 MB
Registering bucketed gradient hooks...
step:0/1770 val_loss:10.8258 train_time:0ms step_avg:0.02ms
step:1/1770 train_time:66ms step_avg:66.10ms
step:2/1770 train_time:142ms step_avg:71.00ms
step:3/1770 train_time:230ms step_avg:76.72ms
step:4/1770 train_time:324ms step_avg:80.96ms
step:5/1770 train_time:417ms step_avg:83.45ms
step:6/1770 train_time:511ms step_avg:85.18ms
step:7/1770 train_time:605ms step_avg:86.41ms
step:8/1770 train_time:699ms step_avg:87.34ms
step:9/1770 train_time:793ms step_avg:88.10ms
step:10/1770 train_time:887ms step_avg:88.67ms
step:11/1770 train_time:981ms step_avg:89.19ms
step:12/1770 train_time:1077ms step_avg:89.76ms
step:13/1770 train_time:1175ms step_avg:90.39ms
step:14/1770 train_time:1270ms step_avg:90.72ms
step:15/1770 train_time:1365ms step_avg:90.98ms
step:16/1770 train_time:1460ms step_avg:91.26ms
step:17/1770 train_time:1554ms step_avg:91.41ms
step:18/1770 train_time:1648ms step_avg:91.54ms
step:19/1770 train_time:1743ms step_avg:91.73ms
step:20/1770 train_time:1837ms step_avg:91.83ms
step:21/1770 train_time:1931ms step_avg:91.94ms
step:22/1770 train_time:2027ms step_avg:92.12ms
step:23/1770 train_time:2123ms step_avg:92.30ms
step:24/1770 train_time:2219ms step_avg:92.46ms
step:25/1770 train_time:2314ms step_avg:92.56ms
step:26/1770 train_time:2409ms step_avg:92.64ms
step:27/1770 train_time:2504ms step_avg:92.72ms
step:28/1770 train_time:2598ms step_avg:92.79ms
step:29/1770 train_time:2692ms step_avg:92.82ms
step:30/1770 train_time:2786ms step_avg:92.86ms
step:31/1770 train_time:2880ms step_avg:92.91ms
step:32/1770 train_time:2974ms step_avg:92.95ms
step:33/1770 train_time:3069ms step_avg:93.00ms
step:34/1770 train_time:3166ms step_avg:93.12ms
step:35/1770 train_time:3262ms step_avg:93.19ms
step:36/1770 train_time:3357ms step_avg:93.26ms
step:37/1770 train_time:3450ms step_avg:93.25ms
step:38/1770 train_time:3546ms step_avg:93.32ms
step:39/1770 train_time:3640ms step_avg:93.33ms
step:40/1770 train_time:3734ms step_avg:93.34ms
step:41/1770 train_time:3828ms step_avg:93.36ms
step:42/1770 train_time:3923ms step_avg:93.40ms
step:43/1770 train_time:4017ms step_avg:93.41ms
step:44/1770 train_time:4111ms step_avg:93.43ms
step:45/1770 train_time:4206ms step_avg:93.47ms
step:46/1770 train_time:4302ms step_avg:93.52ms
step:47/1770 train_time:4399ms step_avg:93.59ms
step:48/1770 train_time:4492ms step_avg:93.59ms
step:49/1770 train_time:4586ms step_avg:93.60ms
step:50/1770 train_time:4681ms step_avg:93.62ms
step:51/1770 train_time:4775ms step_avg:93.63ms
step:52/1770 train_time:4869ms step_avg:93.64ms
step:53/1770 train_time:4964ms step_avg:93.67ms
step:54/1770 train_time:5059ms step_avg:93.69ms
step:55/1770 train_time:5153ms step_avg:93.69ms
step:56/1770 train_time:5251ms step_avg:93.76ms
step:57/1770 train_time:5345ms step_avg:93.77ms
step:58/1770 train_time:5441ms step_avg:93.80ms
step:59/1770 train_time:5534ms step_avg:93.80ms
step:60/1770 train_time:5628ms step_avg:93.80ms
step:61/1770 train_time:5723ms step_avg:93.82ms
step:62/1770 train_time:5818ms step_avg:93.83ms
step:63/1770 train_time:5912ms step_avg:93.84ms
step:64/1770 train_time:6006ms step_avg:93.85ms
step:65/1770 train_time:6101ms step_avg:93.87ms
step:66/1770 train_time:6196ms step_avg:93.87ms
step:67/1770 train_time:6290ms step_avg:93.89ms
step:68/1770 train_time:6386ms step_avg:93.91ms
step:69/1770 train_time:6481ms step_avg:93.92ms
step:70/1770 train_time:6575ms step_avg:93.93ms
step:71/1770 train_time:6669ms step_avg:93.93ms
step:72/1770 train_time:6764ms step_avg:93.95ms
step:73/1770 train_time:6860ms step_avg:93.97ms
step:74/1770 train_time:6954ms step_avg:93.97ms
step:75/1770 train_time:7048ms step_avg:93.97ms
step:76/1770 train_time:7143ms step_avg:93.99ms
step:77/1770 train_time:7239ms step_avg:94.01ms
step:78/1770 train_time:7335ms step_avg:94.04ms
step:79/1770 train_time:7429ms step_avg:94.04ms
step:80/1770 train_time:7524ms step_avg:94.05ms
step:81/1770 train_time:7620ms step_avg:94.07ms
step:82/1770 train_time:7713ms step_avg:94.06ms
step:83/1770 train_time:7808ms step_avg:94.07ms
step:84/1770 train_time:7903ms step_avg:94.08ms
step:85/1770 train_time:7997ms step_avg:94.08ms
step:86/1770 train_time:8091ms step_avg:94.08ms
step:87/1770 train_time:8186ms step_avg:94.09ms
step:88/1770 train_time:8281ms step_avg:94.10ms
step:89/1770 train_time:8375ms step_avg:94.10ms
step:90/1770 train_time:8469ms step_avg:94.10ms
step:91/1770 train_time:8564ms step_avg:94.12ms
step:92/1770 train_time:8660ms step_avg:94.13ms
step:93/1770 train_time:8753ms step_avg:94.12ms
step:94/1770 train_time:8847ms step_avg:94.12ms
step:95/1770 train_time:8943ms step_avg:94.14ms
step:96/1770 train_time:9037ms step_avg:94.14ms
step:97/1770 train_time:9131ms step_avg:94.13ms
step:98/1770 train_time:9226ms step_avg:94.14ms
step:99/1770 train_time:9321ms step_avg:94.15ms
step:100/1770 train_time:9415ms step_avg:94.15ms
step:101/1770 train_time:9509ms step_avg:94.15ms
step:102/1770 train_time:9604ms step_avg:94.16ms
step:103/1770 train_time:9700ms step_avg:94.17ms
step:104/1770 train_time:9794ms step_avg:94.18ms
step:105/1770 train_time:9888ms step_avg:94.17ms
step:106/1770 train_time:9984ms step_avg:94.18ms
step:107/1770 train_time:10079ms step_avg:94.19ms
step:108/1770 train_time:10173ms step_avg:94.19ms
step:109/1770 train_time:10267ms step_avg:94.19ms
step:110/1770 train_time:10362ms step_avg:94.20ms
step:111/1770 train_time:10456ms step_avg:94.20ms
step:112/1770 train_time:10550ms step_avg:94.20ms
step:113/1770 train_time:10646ms step_avg:94.21ms
step:114/1770 train_time:10746ms step_avg:94.26ms
step:115/1770 train_time:10835ms step_avg:94.22ms
step:116/1770 train_time:10930ms step_avg:94.22ms
step:117/1770 train_time:11025ms step_avg:94.23ms
step:118/1770 train_time:11121ms step_avg:94.24ms
step:119/1770 train_time:11215ms step_avg:94.24ms
step:120/1770 train_time:11309ms step_avg:94.24ms
step:121/1770 train_time:11404ms step_avg:94.25ms
step:122/1770 train_time:11499ms step_avg:94.26ms
step:123/1770 train_time:11594ms step_avg:94.26ms
step:124/1770 train_time:11688ms step_avg:94.26ms
step:125/1770 train_time:11783ms step_avg:94.26ms
step:125/1770 val_loss:4.6417 train_time:11877ms step_avg:95.02ms
step:126/1770 train_time:11897ms step_avg:94.42ms
step:127/1770 train_time:11983ms step_avg:94.36ms
step:128/1770 train_time:12084ms step_avg:94.41ms
step:129/1770 train_time:12180ms step_avg:94.42ms
step:130/1770 train_time:12276ms step_avg:94.43ms
step:131/1770 train_time:12368ms step_avg:94.41ms
step:132/1770 train_time:12462ms step_avg:94.41ms
step:133/1770 train_time:12556ms step_avg:94.41ms
step:134/1770 train_time:12651ms step_avg:94.41ms
step:135/1770 train_time:12745ms step_avg:94.40ms
step:136/1770 train_time:12839ms step_avg:94.41ms
step:137/1770 train_time:12934ms step_avg:94.41ms
step:138/1770 train_time:13030ms step_avg:94.42ms
step:139/1770 train_time:13126ms step_avg:94.43ms
step:140/1770 train_time:13222ms step_avg:94.44ms
step:141/1770 train_time:13317ms step_avg:94.45ms
step:142/1770 train_time:13412ms step_avg:94.45ms
step:143/1770 train_time:13507ms step_avg:94.45ms
step:144/1770 train_time:13601ms step_avg:94.45ms
step:145/1770 train_time:13696ms step_avg:94.46ms
step:146/1770 train_time:13791ms step_avg:94.46ms
step:147/1770 train_time:13887ms step_avg:94.47ms
step:148/1770 train_time:13981ms step_avg:94.47ms
step:149/1770 train_time:14077ms step_avg:94.48ms
step:150/1770 train_time:14173ms step_avg:94.48ms
step:151/1770 train_time:14268ms step_avg:94.49ms
step:152/1770 train_time:14363ms step_avg:94.50ms
step:153/1770 train_time:14459ms step_avg:94.50ms
step:154/1770 train_time:14554ms step_avg:94.50ms
step:155/1770 train_time:14647ms step_avg:94.50ms
step:156/1770 train_time:14742ms step_avg:94.50ms
step:157/1770 train_time:14837ms step_avg:94.50ms
step:158/1770 train_time:14932ms step_avg:94.51ms
step:159/1770 train_time:15027ms step_avg:94.51ms
step:160/1770 train_time:15123ms step_avg:94.52ms
step:161/1770 train_time:15219ms step_avg:94.53ms
step:162/1770 train_time:15314ms step_avg:94.53ms
step:163/1770 train_time:15408ms step_avg:94.53ms
step:164/1770 train_time:15503ms step_avg:94.53ms
step:165/1770 train_time:15599ms step_avg:94.54ms
step:166/1770 train_time:15694ms step_avg:94.54ms
step:167/1770 train_time:15788ms step_avg:94.54ms
step:168/1770 train_time:15887ms step_avg:94.56ms
step:169/1770 train_time:15979ms step_avg:94.55ms
step:170/1770 train_time:16074ms step_avg:94.55ms
step:171/1770 train_time:16169ms step_avg:94.56ms
step:172/1770 train_time:16264ms step_avg:94.56ms
step:173/1770 train_time:16359ms step_avg:94.56ms
step:174/1770 train_time:16455ms step_avg:94.57ms
step:175/1770 train_time:16549ms step_avg:94.57ms
step:176/1770 train_time:16643ms step_avg:94.56ms
step:177/1770 train_time:16739ms step_avg:94.57ms
step:178/1770 train_time:16834ms step_avg:94.57ms
step:179/1770 train_time:16928ms step_avg:94.57ms
step:180/1770 train_time:17023ms step_avg:94.57ms
step:181/1770 train_time:17119ms step_avg:94.58ms
step:182/1770 train_time:17214ms step_avg:94.58ms
step:183/1770 train_time:17309ms step_avg:94.58ms
step:184/1770 train_time:17404ms step_avg:94.59ms
step:185/1770 train_time:17500ms step_avg:94.60ms
step:186/1770 train_time:17596ms step_avg:94.60ms
step:187/1770 train_time:17690ms step_avg:94.60ms
step:188/1770 train_time:17786ms step_avg:94.61ms
step:189/1770 train_time:17884ms step_avg:94.63ms
step:190/1770 train_time:17976ms step_avg:94.61ms
step:191/1770 train_time:18071ms step_avg:94.62ms
step:192/1770 train_time:18167ms step_avg:94.62ms
step:193/1770 train_time:18262ms step_avg:94.62ms
step:194/1770 train_time:18357ms step_avg:94.63ms
step:195/1770 train_time:18453ms step_avg:94.63ms
step:196/1770 train_time:18547ms step_avg:94.63ms
step:197/1770 train_time:18642ms step_avg:94.63ms
step:198/1770 train_time:18738ms step_avg:94.63ms
step:199/1770 train_time:18833ms step_avg:94.64ms
step:200/1770 train_time:18928ms step_avg:94.64ms
step:201/1770 train_time:19022ms step_avg:94.64ms
step:202/1770 train_time:19118ms step_avg:94.64ms
step:203/1770 train_time:19213ms step_avg:94.64ms
step:204/1770 train_time:19308ms step_avg:94.64ms
step:205/1770 train_time:19403ms step_avg:94.65ms
step:206/1770 train_time:19499ms step_avg:94.65ms
step:207/1770 train_time:19594ms step_avg:94.66ms
step:208/1770 train_time:19689ms step_avg:94.66ms
step:209/1770 train_time:19784ms step_avg:94.66ms
step:210/1770 train_time:19880ms step_avg:94.67ms
step:211/1770 train_time:19976ms step_avg:94.67ms
step:212/1770 train_time:20071ms step_avg:94.68ms
step:213/1770 train_time:20166ms step_avg:94.68ms
step:214/1770 train_time:20262ms step_avg:94.68ms
step:215/1770 train_time:20358ms step_avg:94.69ms
step:216/1770 train_time:20453ms step_avg:94.69ms
step:217/1770 train_time:20547ms step_avg:94.69ms
step:218/1770 train_time:20642ms step_avg:94.69ms
step:219/1770 train_time:20737ms step_avg:94.69ms
step:220/1770 train_time:20831ms step_avg:94.69ms
step:221/1770 train_time:20926ms step_avg:94.69ms
step:222/1770 train_time:21021ms step_avg:94.69ms
step:223/1770 train_time:21117ms step_avg:94.69ms
step:224/1770 train_time:21212ms step_avg:94.70ms
step:225/1770 train_time:21307ms step_avg:94.70ms
step:226/1770 train_time:21402ms step_avg:94.70ms
step:227/1770 train_time:21498ms step_avg:94.70ms
step:228/1770 train_time:21593ms step_avg:94.71ms
step:229/1770 train_time:21688ms step_avg:94.71ms
step:230/1770 train_time:21782ms step_avg:94.71ms
step:231/1770 train_time:21878ms step_avg:94.71ms
step:232/1770 train_time:21973ms step_avg:94.71ms
step:233/1770 train_time:22067ms step_avg:94.71ms
step:234/1770 train_time:22162ms step_avg:94.71ms
step:235/1770 train_time:22258ms step_avg:94.72ms
step:236/1770 train_time:22354ms step_avg:94.72ms
step:237/1770 train_time:22448ms step_avg:94.72ms
step:238/1770 train_time:22543ms step_avg:94.72ms
step:239/1770 train_time:22639ms step_avg:94.72ms
step:240/1770 train_time:22734ms step_avg:94.72ms
step:241/1770 train_time:22828ms step_avg:94.72ms
step:242/1770 train_time:22923ms step_avg:94.72ms
step:243/1770 train_time:23019ms step_avg:94.73ms
step:244/1770 train_time:23115ms step_avg:94.73ms
step:245/1770 train_time:23209ms step_avg:94.73ms
step:246/1770 train_time:23304ms step_avg:94.73ms
step:247/1770 train_time:23400ms step_avg:94.74ms
step:248/1770 train_time:23495ms step_avg:94.74ms
step:249/1770 train_time:23589ms step_avg:94.74ms
step:250/1770 train_time:23684ms step_avg:94.74ms
step:250/1770 val_loss:4.1053 train_time:23779ms step_avg:95.12ms
step:251/1770 train_time:23799ms step_avg:94.82ms
step:252/1770 train_time:23881ms step_avg:94.77ms
step:253/1770 train_time:23982ms step_avg:94.79ms
step:254/1770 train_time:24080ms step_avg:94.80ms
step:255/1770 train_time:24177ms step_avg:94.81ms
step:256/1770 train_time:24272ms step_avg:94.81ms
step:257/1770 train_time:24366ms step_avg:94.81ms
step:258/1770 train_time:24461ms step_avg:94.81ms
step:259/1770 train_time:24556ms step_avg:94.81ms
step:260/1770 train_time:24650ms step_avg:94.81ms
step:261/1770 train_time:24744ms step_avg:94.80ms
step:262/1770 train_time:24840ms step_avg:94.81ms
step:263/1770 train_time:24937ms step_avg:94.82ms
step:264/1770 train_time:25034ms step_avg:94.83ms
step:265/1770 train_time:25131ms step_avg:94.83ms
step:266/1770 train_time:25226ms step_avg:94.83ms
step:267/1770 train_time:25321ms step_avg:94.84ms
step:268/1770 train_time:25418ms step_avg:94.84ms
step:269/1770 train_time:25512ms step_avg:94.84ms
step:270/1770 train_time:25607ms step_avg:94.84ms
step:271/1770 train_time:25703ms step_avg:94.84ms
step:272/1770 train_time:25798ms step_avg:94.84ms
step:273/1770 train_time:25893ms step_avg:94.85ms
step:274/1770 train_time:25989ms step_avg:94.85ms
step:275/1770 train_time:26084ms step_avg:94.85ms
step:276/1770 train_time:26181ms step_avg:94.86ms
step:277/1770 train_time:26277ms step_avg:94.86ms
step:278/1770 train_time:26372ms step_avg:94.87ms
step:279/1770 train_time:26468ms step_avg:94.87ms
step:280/1770 train_time:26563ms step_avg:94.87ms
step:281/1770 train_time:26658ms step_avg:94.87ms
step:282/1770 train_time:26753ms step_avg:94.87ms
step:283/1770 train_time:26848ms step_avg:94.87ms
step:284/1770 train_time:26945ms step_avg:94.88ms
step:285/1770 train_time:27041ms step_avg:94.88ms
step:286/1770 train_time:27137ms step_avg:94.89ms
step:287/1770 train_time:27234ms step_avg:94.89ms
step:288/1770 train_time:27329ms step_avg:94.89ms
step:289/1770 train_time:27425ms step_avg:94.90ms
step:290/1770 train_time:27522ms step_avg:94.90ms
step:291/1770 train_time:27618ms step_avg:94.91ms
step:292/1770 train_time:27713ms step_avg:94.91ms
step:293/1770 train_time:27808ms step_avg:94.91ms
step:294/1770 train_time:27903ms step_avg:94.91ms
step:295/1770 train_time:27999ms step_avg:94.91ms
step:296/1770 train_time:28095ms step_avg:94.92ms
step:297/1770 train_time:28191ms step_avg:94.92ms
step:298/1770 train_time:28286ms step_avg:94.92ms
step:299/1770 train_time:28382ms step_avg:94.92ms
step:300/1770 train_time:28478ms step_avg:94.93ms
step:301/1770 train_time:28574ms step_avg:94.93ms
step:302/1770 train_time:28669ms step_avg:94.93ms
step:303/1770 train_time:28764ms step_avg:94.93ms
step:304/1770 train_time:28860ms step_avg:94.93ms
step:305/1770 train_time:28957ms step_avg:94.94ms
step:306/1770 train_time:29052ms step_avg:94.94ms
step:307/1770 train_time:29148ms step_avg:94.94ms
step:308/1770 train_time:29243ms step_avg:94.94ms
step:309/1770 train_time:29339ms step_avg:94.95ms
step:310/1770 train_time:29435ms step_avg:94.95ms
step:311/1770 train_time:29531ms step_avg:94.95ms
step:312/1770 train_time:29627ms step_avg:94.96ms
step:313/1770 train_time:29722ms step_avg:94.96ms
step:314/1770 train_time:29818ms step_avg:94.96ms
step:315/1770 train_time:29914ms step_avg:94.97ms
step:316/1770 train_time:30009ms step_avg:94.97ms
step:317/1770 train_time:30105ms step_avg:94.97ms
step:318/1770 train_time:30201ms step_avg:94.97ms
step:319/1770 train_time:30296ms step_avg:94.97ms
step:320/1770 train_time:30391ms step_avg:94.97ms
step:321/1770 train_time:30487ms step_avg:94.97ms
step:322/1770 train_time:30583ms step_avg:94.98ms
step:323/1770 train_time:30679ms step_avg:94.98ms
step:324/1770 train_time:30776ms step_avg:94.99ms
step:325/1770 train_time:30871ms step_avg:94.99ms
step:326/1770 train_time:30967ms step_avg:94.99ms
step:327/1770 train_time:31063ms step_avg:94.99ms
step:328/1770 train_time:31159ms step_avg:95.00ms
step:329/1770 train_time:31255ms step_avg:95.00ms
step:330/1770 train_time:31350ms step_avg:95.00ms
step:331/1770 train_time:31446ms step_avg:95.00ms
step:332/1770 train_time:31542ms step_avg:95.01ms
step:333/1770 train_time:31638ms step_avg:95.01ms
step:334/1770 train_time:31734ms step_avg:95.01ms
step:335/1770 train_time:31831ms step_avg:95.02ms
step:336/1770 train_time:31927ms step_avg:95.02ms
step:337/1770 train_time:32023ms step_avg:95.02ms
step:338/1770 train_time:32119ms step_avg:95.03ms
step:339/1770 train_time:32215ms step_avg:95.03ms
step:340/1770 train_time:32310ms step_avg:95.03ms
step:341/1770 train_time:32405ms step_avg:95.03ms
step:342/1770 train_time:32500ms step_avg:95.03ms
step:343/1770 train_time:32597ms step_avg:95.03ms
step:344/1770 train_time:32692ms step_avg:95.04ms
step:345/1770 train_time:32787ms step_avg:95.04ms
step:346/1770 train_time:32883ms step_avg:95.04ms
step:347/1770 train_time:32979ms step_avg:95.04ms
step:348/1770 train_time:33075ms step_avg:95.04ms
step:349/1770 train_time:33171ms step_avg:95.05ms
step:350/1770 train_time:33266ms step_avg:95.05ms
step:351/1770 train_time:33362ms step_avg:95.05ms
step:352/1770 train_time:33458ms step_avg:95.05ms
step:353/1770 train_time:33554ms step_avg:95.05ms
step:354/1770 train_time:33649ms step_avg:95.05ms
step:355/1770 train_time:33744ms step_avg:95.05ms
step:356/1770 train_time:33840ms step_avg:95.06ms
step:357/1770 train_time:33936ms step_avg:95.06ms
step:358/1770 train_time:34032ms step_avg:95.06ms
step:359/1770 train_time:34128ms step_avg:95.06ms
step:360/1770 train_time:34223ms step_avg:95.07ms
step:361/1770 train_time:34320ms step_avg:95.07ms
step:362/1770 train_time:34416ms step_avg:95.07ms
step:363/1770 train_time:34511ms step_avg:95.07ms
step:364/1770 train_time:34605ms step_avg:95.07ms
step:365/1770 train_time:34701ms step_avg:95.07ms
step:366/1770 train_time:34797ms step_avg:95.07ms
step:367/1770 train_time:34893ms step_avg:95.08ms
step:368/1770 train_time:34988ms step_avg:95.08ms
step:369/1770 train_time:35084ms step_avg:95.08ms
step:370/1770 train_time:35179ms step_avg:95.08ms
step:371/1770 train_time:35275ms step_avg:95.08ms
step:372/1770 train_time:35371ms step_avg:95.08ms
step:373/1770 train_time:35466ms step_avg:95.08ms
step:374/1770 train_time:35562ms step_avg:95.08ms
step:375/1770 train_time:35658ms step_avg:95.09ms
step:375/1770 val_loss:3.9005 train_time:35754ms step_avg:95.34ms
step:376/1770 train_time:35772ms step_avg:95.14ms
step:377/1770 train_time:35856ms step_avg:95.11ms
step:378/1770 train_time:35954ms step_avg:95.12ms
step:379/1770 train_time:36050ms step_avg:95.12ms
step:380/1770 train_time:36145ms step_avg:95.12ms
step:381/1770 train_time:36240ms step_avg:95.12ms
step:382/1770 train_time:36336ms step_avg:95.12ms
step:383/1770 train_time:36431ms step_avg:95.12ms
step:384/1770 train_time:36526ms step_avg:95.12ms
step:385/1770 train_time:36620ms step_avg:95.12ms
step:386/1770 train_time:36715ms step_avg:95.12ms
step:387/1770 train_time:36813ms step_avg:95.12ms
step:388/1770 train_time:36911ms step_avg:95.13ms
step:389/1770 train_time:37008ms step_avg:95.14ms
step:390/1770 train_time:37105ms step_avg:95.14ms
step:391/1770 train_time:37200ms step_avg:95.14ms
step:392/1770 train_time:37295ms step_avg:95.14ms
step:393/1770 train_time:37390ms step_avg:95.14ms
step:394/1770 train_time:37486ms step_avg:95.14ms
step:395/1770 train_time:37581ms step_avg:95.14ms
step:396/1770 train_time:37679ms step_avg:95.15ms
step:397/1770 train_time:37776ms step_avg:95.15ms
step:398/1770 train_time:37875ms step_avg:95.16ms
step:399/1770 train_time:37973ms step_avg:95.17ms
step:400/1770 train_time:38072ms step_avg:95.18ms
step:401/1770 train_time:38170ms step_avg:95.19ms
step:402/1770 train_time:38269ms step_avg:95.20ms
step:403/1770 train_time:38367ms step_avg:95.20ms
step:404/1770 train_time:38466ms step_avg:95.21ms
step:405/1770 train_time:38564ms step_avg:95.22ms
step:406/1770 train_time:38663ms step_avg:95.23ms
step:407/1770 train_time:38762ms step_avg:95.24ms
step:408/1770 train_time:38861ms step_avg:95.25ms
step:409/1770 train_time:38959ms step_avg:95.26ms
step:410/1770 train_time:39058ms step_avg:95.26ms
step:411/1770 train_time:39155ms step_avg:95.27ms
step:412/1770 train_time:39253ms step_avg:95.28ms
step:413/1770 train_time:39351ms step_avg:95.28ms
step:414/1770 train_time:39451ms step_avg:95.29ms
step:415/1770 train_time:39548ms step_avg:95.30ms
step:416/1770 train_time:39645ms step_avg:95.30ms
step:417/1770 train_time:39743ms step_avg:95.31ms
step:418/1770 train_time:39841ms step_avg:95.31ms
step:419/1770 train_time:39940ms step_avg:95.32ms
step:420/1770 train_time:40039ms step_avg:95.33ms
step:421/1770 train_time:40136ms step_avg:95.33ms
step:422/1770 train_time:40233ms step_avg:95.34ms
step:423/1770 train_time:40331ms step_avg:95.35ms
step:424/1770 train_time:40429ms step_avg:95.35ms
step:425/1770 train_time:40527ms step_avg:95.36ms
step:426/1770 train_time:40625ms step_avg:95.36ms
step:427/1770 train_time:40723ms step_avg:95.37ms
step:428/1770 train_time:40821ms step_avg:95.38ms
step:429/1770 train_time:40919ms step_avg:95.38ms
step:430/1770 train_time:41017ms step_avg:95.39ms
step:431/1770 train_time:41115ms step_avg:95.39ms
step:432/1770 train_time:41213ms step_avg:95.40ms
step:433/1770 train_time:41311ms step_avg:95.41ms
step:434/1770 train_time:41408ms step_avg:95.41ms
step:435/1770 train_time:41506ms step_avg:95.42ms
step:436/1770 train_time:41604ms step_avg:95.42ms
step:437/1770 train_time:41702ms step_avg:95.43ms
step:438/1770 train_time:41800ms step_avg:95.43ms
step:439/1770 train_time:41897ms step_avg:95.44ms
step:440/1770 train_time:41996ms step_avg:95.44ms
step:441/1770 train_time:42094ms step_avg:95.45ms
step:442/1770 train_time:42192ms step_avg:95.46ms
step:443/1770 train_time:42290ms step_avg:95.46ms
step:444/1770 train_time:42388ms step_avg:95.47ms
step:445/1770 train_time:42487ms step_avg:95.48ms
step:446/1770 train_time:42585ms step_avg:95.48ms
step:447/1770 train_time:42682ms step_avg:95.49ms
step:448/1770 train_time:42780ms step_avg:95.49ms
step:449/1770 train_time:42877ms step_avg:95.50ms
step:450/1770 train_time:42976ms step_avg:95.50ms
step:451/1770 train_time:43073ms step_avg:95.51ms
step:452/1770 train_time:43172ms step_avg:95.51ms
step:453/1770 train_time:43270ms step_avg:95.52ms
step:454/1770 train_time:43368ms step_avg:95.52ms
step:455/1770 train_time:43466ms step_avg:95.53ms
step:456/1770 train_time:43564ms step_avg:95.53ms
step:457/1770 train_time:43662ms step_avg:95.54ms
step:458/1770 train_time:43760ms step_avg:95.54ms
step:459/1770 train_time:43858ms step_avg:95.55ms
step:460/1770 train_time:43956ms step_avg:95.56ms
step:461/1770 train_time:44054ms step_avg:95.56ms
step:462/1770 train_time:44151ms step_avg:95.56ms
step:463/1770 train_time:44249ms step_avg:95.57ms
step:464/1770 train_time:44347ms step_avg:95.58ms
step:465/1770 train_time:44445ms step_avg:95.58ms
step:466/1770 train_time:44543ms step_avg:95.59ms
step:467/1770 train_time:44640ms step_avg:95.59ms
step:468/1770 train_time:44741ms step_avg:95.60ms
step:469/1770 train_time:44836ms step_avg:95.60ms
step:470/1770 train_time:44934ms step_avg:95.60ms
step:471/1770 train_time:45032ms step_avg:95.61ms
step:472/1770 train_time:45130ms step_avg:95.61ms
step:473/1770 train_time:45228ms step_avg:95.62ms
step:474/1770 train_time:45326ms step_avg:95.62ms
step:475/1770 train_time:45423ms step_avg:95.63ms
step:476/1770 train_time:45521ms step_avg:95.63ms
step:477/1770 train_time:45619ms step_avg:95.64ms
step:478/1770 train_time:45717ms step_avg:95.64ms
step:479/1770 train_time:45814ms step_avg:95.65ms
step:480/1770 train_time:45913ms step_avg:95.65ms
step:481/1770 train_time:46012ms step_avg:95.66ms
step:482/1770 train_time:46110ms step_avg:95.66ms
step:483/1770 train_time:46209ms step_avg:95.67ms
step:484/1770 train_time:46308ms step_avg:95.68ms
step:485/1770 train_time:46406ms step_avg:95.68ms
step:486/1770 train_time:46503ms step_avg:95.69ms
step:487/1770 train_time:46602ms step_avg:95.69ms
step:488/1770 train_time:46700ms step_avg:95.70ms
step:489/1770 train_time:46797ms step_avg:95.70ms
step:490/1770 train_time:46894ms step_avg:95.70ms
step:491/1770 train_time:46992ms step_avg:95.71ms
step:492/1770 train_time:47090ms step_avg:95.71ms
step:493/1770 train_time:47189ms step_avg:95.72ms
step:494/1770 train_time:47288ms step_avg:95.72ms
step:495/1770 train_time:47386ms step_avg:95.73ms
step:496/1770 train_time:47484ms step_avg:95.73ms
step:497/1770 train_time:47582ms step_avg:95.74ms
step:498/1770 train_time:47681ms step_avg:95.74ms
step:499/1770 train_time:47779ms step_avg:95.75ms
step:500/1770 train_time:47876ms step_avg:95.75ms
step:500/1770 val_loss:3.7504 train_time:47972ms step_avg:95.94ms
step:501/1770 train_time:47991ms step_avg:95.79ms
step:502/1770 train_time:48076ms step_avg:95.77ms
step:503/1770 train_time:48176ms step_avg:95.78ms
step:504/1770 train_time:48274ms step_avg:95.78ms
step:505/1770 train_time:48371ms step_avg:95.78ms
step:506/1770 train_time:48469ms step_avg:95.79ms
step:507/1770 train_time:48566ms step_avg:95.79ms
step:508/1770 train_time:48663ms step_avg:95.79ms
step:509/1770 train_time:48761ms step_avg:95.80ms
step:510/1770 train_time:48858ms step_avg:95.80ms
step:511/1770 train_time:48957ms step_avg:95.81ms
step:512/1770 train_time:49057ms step_avg:95.81ms
step:513/1770 train_time:49157ms step_avg:95.82ms
step:514/1770 train_time:49255ms step_avg:95.83ms
step:515/1770 train_time:49353ms step_avg:95.83ms
step:516/1770 train_time:49451ms step_avg:95.84ms
step:517/1770 train_time:49549ms step_avg:95.84ms
step:518/1770 train_time:49646ms step_avg:95.84ms
step:519/1770 train_time:49747ms step_avg:95.85ms
step:520/1770 train_time:49841ms step_avg:95.85ms
step:521/1770 train_time:49939ms step_avg:95.85ms
step:522/1770 train_time:50040ms step_avg:95.86ms
step:523/1770 train_time:50136ms step_avg:95.86ms
step:524/1770 train_time:50234ms step_avg:95.87ms
step:525/1770 train_time:50332ms step_avg:95.87ms
step:526/1770 train_time:50430ms step_avg:95.87ms
step:527/1770 train_time:50528ms step_avg:95.88ms
step:528/1770 train_time:50626ms step_avg:95.88ms
step:529/1770 train_time:50724ms step_avg:95.89ms
step:530/1770 train_time:50822ms step_avg:95.89ms
step:531/1770 train_time:50921ms step_avg:95.90ms
step:532/1770 train_time:51019ms step_avg:95.90ms
step:533/1770 train_time:51118ms step_avg:95.91ms
step:534/1770 train_time:51217ms step_avg:95.91ms
step:535/1770 train_time:51316ms step_avg:95.92ms
step:536/1770 train_time:51417ms step_avg:95.93ms
step:537/1770 train_time:51516ms step_avg:95.93ms
step:538/1770 train_time:51614ms step_avg:95.94ms
step:539/1770 train_time:51712ms step_avg:95.94ms
step:540/1770 train_time:51811ms step_avg:95.95ms
step:541/1770 train_time:51909ms step_avg:95.95ms
step:542/1770 train_time:52007ms step_avg:95.95ms
step:543/1770 train_time:52104ms step_avg:95.96ms
step:544/1770 train_time:52202ms step_avg:95.96ms
step:545/1770 train_time:52301ms step_avg:95.96ms
step:546/1770 train_time:52400ms step_avg:95.97ms
step:547/1770 train_time:52499ms step_avg:95.98ms
step:548/1770 train_time:52599ms step_avg:95.98ms
step:549/1770 train_time:52699ms step_avg:95.99ms
step:550/1770 train_time:52799ms step_avg:96.00ms
step:551/1770 train_time:52898ms step_avg:96.00ms
step:552/1770 train_time:52997ms step_avg:96.01ms
step:553/1770 train_time:53096ms step_avg:96.01ms
step:554/1770 train_time:53195ms step_avg:96.02ms
step:555/1770 train_time:53293ms step_avg:96.02ms
step:556/1770 train_time:53391ms step_avg:96.03ms
step:557/1770 train_time:53489ms step_avg:96.03ms
step:558/1770 train_time:53587ms step_avg:96.03ms
step:559/1770 train_time:53686ms step_avg:96.04ms
step:560/1770 train_time:53784ms step_avg:96.04ms
step:561/1770 train_time:53883ms step_avg:96.05ms
step:562/1770 train_time:53981ms step_avg:96.05ms
step:563/1770 train_time:54080ms step_avg:96.06ms
step:564/1770 train_time:54179ms step_avg:96.06ms
step:565/1770 train_time:54278ms step_avg:96.07ms
step:566/1770 train_time:54377ms step_avg:96.07ms
step:567/1770 train_time:54476ms step_avg:96.08ms
step:568/1770 train_time:54577ms step_avg:96.09ms
step:569/1770 train_time:54677ms step_avg:96.09ms
step:570/1770 train_time:54777ms step_avg:96.10ms
step:571/1770 train_time:54876ms step_avg:96.10ms
step:572/1770 train_time:54975ms step_avg:96.11ms
step:573/1770 train_time:55074ms step_avg:96.12ms
step:574/1770 train_time:55173ms step_avg:96.12ms
step:575/1770 train_time:55272ms step_avg:96.12ms
step:576/1770 train_time:55370ms step_avg:96.13ms
step:577/1770 train_time:55469ms step_avg:96.13ms
step:578/1770 train_time:55566ms step_avg:96.13ms
step:579/1770 train_time:55663ms step_avg:96.14ms
step:580/1770 train_time:55762ms step_avg:96.14ms
step:581/1770 train_time:55860ms step_avg:96.15ms
step:582/1770 train_time:55959ms step_avg:96.15ms
step:583/1770 train_time:56058ms step_avg:96.15ms
step:584/1770 train_time:56157ms step_avg:96.16ms
step:585/1770 train_time:56255ms step_avg:96.16ms
step:586/1770 train_time:56353ms step_avg:96.17ms
step:587/1770 train_time:56451ms step_avg:96.17ms
step:588/1770 train_time:56550ms step_avg:96.17ms
step:589/1770 train_time:56647ms step_avg:96.18ms
step:590/1770 train_time:56745ms step_avg:96.18ms
step:591/1770 train_time:56844ms step_avg:96.18ms
step:592/1770 train_time:56942ms step_avg:96.19ms
step:593/1770 train_time:57040ms step_avg:96.19ms
step:594/1770 train_time:57139ms step_avg:96.19ms
step:595/1770 train_time:57237ms step_avg:96.20ms
step:596/1770 train_time:57336ms step_avg:96.20ms
step:597/1770 train_time:57436ms step_avg:96.21ms
step:598/1770 train_time:57534ms step_avg:96.21ms
step:599/1770 train_time:57634ms step_avg:96.22ms
step:600/1770 train_time:57733ms step_avg:96.22ms
step:601/1770 train_time:57832ms step_avg:96.23ms
step:602/1770 train_time:57931ms step_avg:96.23ms
step:603/1770 train_time:58028ms step_avg:96.23ms
step:604/1770 train_time:58126ms step_avg:96.23ms
step:605/1770 train_time:58224ms step_avg:96.24ms
step:606/1770 train_time:58322ms step_avg:96.24ms
step:607/1770 train_time:58420ms step_avg:96.24ms
step:608/1770 train_time:58519ms step_avg:96.25ms
step:609/1770 train_time:58618ms step_avg:96.25ms
step:610/1770 train_time:58718ms step_avg:96.26ms
step:611/1770 train_time:58819ms step_avg:96.27ms
step:612/1770 train_time:58919ms step_avg:96.27ms
step:613/1770 train_time:59020ms step_avg:96.28ms
step:614/1770 train_time:59119ms step_avg:96.28ms
step:615/1770 train_time:59218ms step_avg:96.29ms
step:616/1770 train_time:59317ms step_avg:96.29ms
step:617/1770 train_time:59417ms step_avg:96.30ms
step:618/1770 train_time:59516ms step_avg:96.30ms
step:619/1770 train_time:59615ms step_avg:96.31ms
step:620/1770 train_time:59713ms step_avg:96.31ms
step:621/1770 train_time:59812ms step_avg:96.32ms
step:622/1770 train_time:59910ms step_avg:96.32ms
step:623/1770 train_time:60009ms step_avg:96.32ms
step:624/1770 train_time:60107ms step_avg:96.33ms
step:625/1770 train_time:60204ms step_avg:96.33ms
step:625/1770 val_loss:3.6646 train_time:60302ms step_avg:96.48ms
step:626/1770 train_time:60320ms step_avg:96.36ms
step:627/1770 train_time:60409ms step_avg:96.35ms
step:628/1770 train_time:60511ms step_avg:96.36ms
step:629/1770 train_time:60609ms step_avg:96.36ms
step:630/1770 train_time:60707ms step_avg:96.36ms
step:631/1770 train_time:60805ms step_avg:96.36ms
step:632/1770 train_time:60903ms step_avg:96.37ms
step:633/1770 train_time:61000ms step_avg:96.37ms
step:634/1770 train_time:61098ms step_avg:96.37ms
step:635/1770 train_time:61195ms step_avg:96.37ms
step:636/1770 train_time:61293ms step_avg:96.37ms
step:637/1770 train_time:61393ms step_avg:96.38ms
step:638/1770 train_time:61492ms step_avg:96.38ms
step:639/1770 train_time:61590ms step_avg:96.38ms
step:640/1770 train_time:61688ms step_avg:96.39ms
step:641/1770 train_time:61786ms step_avg:96.39ms
step:642/1770 train_time:61886ms step_avg:96.39ms
step:643/1770 train_time:61984ms step_avg:96.40ms
step:644/1770 train_time:62082ms step_avg:96.40ms
step:645/1770 train_time:62180ms step_avg:96.40ms
step:646/1770 train_time:62278ms step_avg:96.41ms
step:647/1770 train_time:62378ms step_avg:96.41ms
step:648/1770 train_time:62477ms step_avg:96.41ms
step:649/1770 train_time:62576ms step_avg:96.42ms
step:650/1770 train_time:62674ms step_avg:96.42ms
step:651/1770 train_time:62771ms step_avg:96.42ms
step:652/1770 train_time:62869ms step_avg:96.42ms
step:653/1770 train_time:62967ms step_avg:96.43ms
step:654/1770 train_time:63064ms step_avg:96.43ms
step:655/1770 train_time:63163ms step_avg:96.43ms
step:656/1770 train_time:63261ms step_avg:96.43ms
step:657/1770 train_time:63361ms step_avg:96.44ms
step:658/1770 train_time:63463ms step_avg:96.45ms
step:659/1770 train_time:63565ms step_avg:96.46ms
step:660/1770 train_time:63666ms step_avg:96.46ms
step:661/1770 train_time:63767ms step_avg:96.47ms
step:662/1770 train_time:63867ms step_avg:96.48ms
step:663/1770 train_time:63968ms step_avg:96.48ms
step:664/1770 train_time:64068ms step_avg:96.49ms
step:665/1770 train_time:64167ms step_avg:96.49ms
step:666/1770 train_time:64267ms step_avg:96.50ms
step:667/1770 train_time:64369ms step_avg:96.51ms
step:668/1770 train_time:64469ms step_avg:96.51ms
step:669/1770 train_time:64570ms step_avg:96.52ms
step:670/1770 train_time:64672ms step_avg:96.52ms
step:671/1770 train_time:64772ms step_avg:96.53ms
step:672/1770 train_time:64872ms step_avg:96.54ms
step:673/1770 train_time:64971ms step_avg:96.54ms
step:674/1770 train_time:65071ms step_avg:96.54ms
step:675/1770 train_time:65169ms step_avg:96.55ms
step:676/1770 train_time:65269ms step_avg:96.55ms
step:677/1770 train_time:65369ms step_avg:96.56ms
step:678/1770 train_time:65469ms step_avg:96.56ms
step:679/1770 train_time:65570ms step_avg:96.57ms
step:680/1770 train_time:65671ms step_avg:96.58ms
step:681/1770 train_time:65772ms step_avg:96.58ms
step:682/1770 train_time:65873ms step_avg:96.59ms
step:683/1770 train_time:65973ms step_avg:96.59ms
step:684/1770 train_time:66072ms step_avg:96.60ms
step:685/1770 train_time:66171ms step_avg:96.60ms
step:686/1770 train_time:66270ms step_avg:96.60ms
step:687/1770 train_time:66370ms step_avg:96.61ms
step:688/1770 train_time:66470ms step_avg:96.61ms
step:689/1770 train_time:66570ms step_avg:96.62ms
step:690/1770 train_time:66672ms step_avg:96.63ms
step:691/1770 train_time:66772ms step_avg:96.63ms
step:692/1770 train_time:66873ms step_avg:96.64ms
step:693/1770 train_time:66972ms step_avg:96.64ms
step:694/1770 train_time:67073ms step_avg:96.65ms
step:695/1770 train_time:67172ms step_avg:96.65ms
step:696/1770 train_time:67271ms step_avg:96.65ms
step:697/1770 train_time:67370ms step_avg:96.66ms
step:698/1770 train_time:67469ms step_avg:96.66ms
step:699/1770 train_time:67569ms step_avg:96.67ms
step:700/1770 train_time:67669ms step_avg:96.67ms
step:701/1770 train_time:67770ms step_avg:96.68ms
step:702/1770 train_time:67870ms step_avg:96.68ms
step:703/1770 train_time:67970ms step_avg:96.69ms
step:704/1770 train_time:68070ms step_avg:96.69ms
step:705/1770 train_time:68170ms step_avg:96.70ms
step:706/1770 train_time:68270ms step_avg:96.70ms
step:707/1770 train_time:68371ms step_avg:96.71ms
step:708/1770 train_time:68470ms step_avg:96.71ms
step:709/1770 train_time:68570ms step_avg:96.71ms
step:710/1770 train_time:68670ms step_avg:96.72ms
step:711/1770 train_time:68769ms step_avg:96.72ms
step:712/1770 train_time:68870ms step_avg:96.73ms
step:713/1770 train_time:68971ms step_avg:96.73ms
step:714/1770 train_time:69071ms step_avg:96.74ms
step:715/1770 train_time:69171ms step_avg:96.74ms
step:716/1770 train_time:69271ms step_avg:96.75ms
step:717/1770 train_time:69371ms step_avg:96.75ms
step:718/1770 train_time:69470ms step_avg:96.76ms
step:719/1770 train_time:69570ms step_avg:96.76ms
step:720/1770 train_time:69669ms step_avg:96.76ms
step:721/1770 train_time:69769ms step_avg:96.77ms
step:722/1770 train_time:69869ms step_avg:96.77ms
step:723/1770 train_time:69970ms step_avg:96.78ms
step:724/1770 train_time:70071ms step_avg:96.78ms
step:725/1770 train_time:70172ms step_avg:96.79ms
step:726/1770 train_time:70272ms step_avg:96.79ms
step:727/1770 train_time:70372ms step_avg:96.80ms
step:728/1770 train_time:70472ms step_avg:96.80ms
step:729/1770 train_time:70571ms step_avg:96.81ms
step:730/1770 train_time:70671ms step_avg:96.81ms
step:731/1770 train_time:70771ms step_avg:96.81ms
step:732/1770 train_time:70871ms step_avg:96.82ms
step:733/1770 train_time:70972ms step_avg:96.82ms
step:734/1770 train_time:71073ms step_avg:96.83ms
step:735/1770 train_time:71173ms step_avg:96.83ms
step:736/1770 train_time:71274ms step_avg:96.84ms
step:737/1770 train_time:71374ms step_avg:96.84ms
step:738/1770 train_time:71473ms step_avg:96.85ms
step:739/1770 train_time:71573ms step_avg:96.85ms
step:740/1770 train_time:71672ms step_avg:96.85ms
step:741/1770 train_time:71771ms step_avg:96.86ms
step:742/1770 train_time:71872ms step_avg:96.86ms
step:743/1770 train_time:71971ms step_avg:96.87ms
step:744/1770 train_time:72071ms step_avg:96.87ms
step:745/1770 train_time:72171ms step_avg:96.87ms
step:746/1770 train_time:72272ms step_avg:96.88ms
step:747/1770 train_time:72371ms step_avg:96.88ms
step:748/1770 train_time:72472ms step_avg:96.89ms
step:749/1770 train_time:72573ms step_avg:96.89ms
step:750/1770 train_time:72673ms step_avg:96.90ms
step:750/1770 val_loss:3.5997 train_time:72771ms step_avg:97.03ms
step:751/1770 train_time:72790ms step_avg:96.92ms
step:752/1770 train_time:72879ms step_avg:96.91ms
step:753/1770 train_time:72980ms step_avg:96.92ms
step:754/1770 train_time:73080ms step_avg:96.92ms
step:755/1770 train_time:73179ms step_avg:96.93ms
step:756/1770 train_time:73280ms step_avg:96.93ms
step:757/1770 train_time:73379ms step_avg:96.93ms
step:758/1770 train_time:73478ms step_avg:96.94ms
step:759/1770 train_time:73577ms step_avg:96.94ms
step:760/1770 train_time:73677ms step_avg:96.94ms
step:761/1770 train_time:73778ms step_avg:96.95ms
step:762/1770 train_time:73881ms step_avg:96.96ms
step:763/1770 train_time:73982ms step_avg:96.96ms
step:764/1770 train_time:74083ms step_avg:96.97ms
step:765/1770 train_time:74182ms step_avg:96.97ms
step:766/1770 train_time:74281ms step_avg:96.97ms
step:767/1770 train_time:74381ms step_avg:96.98ms
step:768/1770 train_time:74480ms step_avg:96.98ms
step:769/1770 train_time:74580ms step_avg:96.98ms
step:770/1770 train_time:74680ms step_avg:96.99ms
step:771/1770 train_time:74779ms step_avg:96.99ms
step:772/1770 train_time:74879ms step_avg:96.99ms
step:773/1770 train_time:74980ms step_avg:97.00ms
step:774/1770 train_time:75080ms step_avg:97.00ms
step:775/1770 train_time:75181ms step_avg:97.01ms
step:776/1770 train_time:75281ms step_avg:97.01ms
step:777/1770 train_time:75381ms step_avg:97.01ms
step:778/1770 train_time:75480ms step_avg:97.02ms
step:779/1770 train_time:75579ms step_avg:97.02ms
step:780/1770 train_time:75680ms step_avg:97.03ms
step:781/1770 train_time:75779ms step_avg:97.03ms
step:782/1770 train_time:75879ms step_avg:97.03ms
step:783/1770 train_time:75980ms step_avg:97.04ms
step:784/1770 train_time:76081ms step_avg:97.04ms
step:785/1770 train_time:76181ms step_avg:97.05ms
step:786/1770 train_time:76282ms step_avg:97.05ms
step:787/1770 train_time:76382ms step_avg:97.05ms
step:788/1770 train_time:76482ms step_avg:97.06ms
step:789/1770 train_time:76582ms step_avg:97.06ms
step:790/1770 train_time:76682ms step_avg:97.07ms
step:791/1770 train_time:76782ms step_avg:97.07ms
step:792/1770 train_time:76881ms step_avg:97.07ms
step:793/1770 train_time:76981ms step_avg:97.08ms
step:794/1770 train_time:77081ms step_avg:97.08ms
step:795/1770 train_time:77181ms step_avg:97.08ms
step:796/1770 train_time:77281ms step_avg:97.09ms
step:797/1770 train_time:77382ms step_avg:97.09ms
step:798/1770 train_time:77481ms step_avg:97.09ms
step:799/1770 train_time:77581ms step_avg:97.10ms
step:800/1770 train_time:77680ms step_avg:97.10ms
step:801/1770 train_time:77781ms step_avg:97.10ms
step:802/1770 train_time:77882ms step_avg:97.11ms
step:803/1770 train_time:77981ms step_avg:97.11ms
step:804/1770 train_time:78081ms step_avg:97.12ms
step:805/1770 train_time:78181ms step_avg:97.12ms
step:806/1770 train_time:78280ms step_avg:97.12ms
step:807/1770 train_time:78380ms step_avg:97.13ms
step:808/1770 train_time:78480ms step_avg:97.13ms
step:809/1770 train_time:78580ms step_avg:97.13ms
step:810/1770 train_time:78680ms step_avg:97.14ms
step:811/1770 train_time:78780ms step_avg:97.14ms
step:812/1770 train_time:78881ms step_avg:97.14ms
step:813/1770 train_time:78981ms step_avg:97.15ms
step:814/1770 train_time:79081ms step_avg:97.15ms
step:815/1770 train_time:79181ms step_avg:97.16ms
step:816/1770 train_time:79282ms step_avg:97.16ms
step:817/1770 train_time:79383ms step_avg:97.16ms
step:818/1770 train_time:79483ms step_avg:97.17ms
step:819/1770 train_time:79584ms step_avg:97.17ms
step:820/1770 train_time:79684ms step_avg:97.18ms
step:821/1770 train_time:79785ms step_avg:97.18ms
step:822/1770 train_time:79885ms step_avg:97.18ms
step:823/1770 train_time:79985ms step_avg:97.19ms
step:824/1770 train_time:80085ms step_avg:97.19ms
step:825/1770 train_time:80185ms step_avg:97.19ms
step:826/1770 train_time:80286ms step_avg:97.20ms
step:827/1770 train_time:80386ms step_avg:97.20ms
step:828/1770 train_time:80486ms step_avg:97.21ms
step:829/1770 train_time:80587ms step_avg:97.21ms
step:830/1770 train_time:80688ms step_avg:97.21ms
step:831/1770 train_time:80789ms step_avg:97.22ms
step:832/1770 train_time:80890ms step_avg:97.22ms
step:833/1770 train_time:80993ms step_avg:97.23ms
step:834/1770 train_time:81095ms step_avg:97.24ms
step:835/1770 train_time:81195ms step_avg:97.24ms
step:836/1770 train_time:81296ms step_avg:97.24ms
step:837/1770 train_time:81397ms step_avg:97.25ms
step:838/1770 train_time:81498ms step_avg:97.25ms
step:839/1770 train_time:81598ms step_avg:97.26ms
step:840/1770 train_time:81700ms step_avg:97.26ms
step:841/1770 train_time:81802ms step_avg:97.27ms
step:842/1770 train_time:81902ms step_avg:97.27ms
step:843/1770 train_time:82002ms step_avg:97.27ms
step:844/1770 train_time:82102ms step_avg:97.28ms
step:845/1770 train_time:82202ms step_avg:97.28ms
step:846/1770 train_time:82303ms step_avg:97.28ms
step:847/1770 train_time:82403ms step_avg:97.29ms
step:848/1770 train_time:82503ms step_avg:97.29ms
step:849/1770 train_time:82603ms step_avg:97.29ms
step:850/1770 train_time:82704ms step_avg:97.30ms
step:851/1770 train_time:82804ms step_avg:97.30ms
step:852/1770 train_time:82904ms step_avg:97.30ms
step:853/1770 train_time:83004ms step_avg:97.31ms
step:854/1770 train_time:83104ms step_avg:97.31ms
step:855/1770 train_time:83204ms step_avg:97.31ms
step:856/1770 train_time:83304ms step_avg:97.32ms
step:857/1770 train_time:83404ms step_avg:97.32ms
step:858/1770 train_time:83504ms step_avg:97.32ms
step:859/1770 train_time:83604ms step_avg:97.33ms
step:860/1770 train_time:83704ms step_avg:97.33ms
step:861/1770 train_time:83804ms step_avg:97.33ms
step:862/1770 train_time:83904ms step_avg:97.34ms
step:863/1770 train_time:84004ms step_avg:97.34ms
step:864/1770 train_time:84103ms step_avg:97.34ms
step:865/1770 train_time:84204ms step_avg:97.35ms
step:866/1770 train_time:84304ms step_avg:97.35ms
step:867/1770 train_time:84404ms step_avg:97.35ms
step:868/1770 train_time:84504ms step_avg:97.36ms
step:869/1770 train_time:84604ms step_avg:97.36ms
step:870/1770 train_time:84705ms step_avg:97.36ms
step:871/1770 train_time:84806ms step_avg:97.37ms
step:872/1770 train_time:84907ms step_avg:97.37ms
step:873/1770 train_time:85008ms step_avg:97.37ms
step:874/1770 train_time:85109ms step_avg:97.38ms
step:875/1770 train_time:85210ms step_avg:97.38ms
step:875/1770 val_loss:3.5505 train_time:85311ms step_avg:97.50ms
step:876/1770 train_time:85329ms step_avg:97.41ms
step:877/1770 train_time:85418ms step_avg:97.40ms
step:878/1770 train_time:85520ms step_avg:97.40ms
step:879/1770 train_time:85620ms step_avg:97.41ms
step:880/1770 train_time:85719ms step_avg:97.41ms
step:881/1770 train_time:85819ms step_avg:97.41ms
step:882/1770 train_time:85919ms step_avg:97.41ms
step:883/1770 train_time:86018ms step_avg:97.42ms
step:884/1770 train_time:86118ms step_avg:97.42ms
step:885/1770 train_time:86217ms step_avg:97.42ms
step:886/1770 train_time:86319ms step_avg:97.43ms
step:887/1770 train_time:86421ms step_avg:97.43ms
step:888/1770 train_time:86523ms step_avg:97.44ms
step:889/1770 train_time:86625ms step_avg:97.44ms
step:890/1770 train_time:86726ms step_avg:97.44ms
step:891/1770 train_time:86827ms step_avg:97.45ms
step:892/1770 train_time:86928ms step_avg:97.45ms
step:893/1770 train_time:87028ms step_avg:97.46ms
step:894/1770 train_time:87128ms step_avg:97.46ms
step:895/1770 train_time:87230ms step_avg:97.46ms
step:896/1770 train_time:87331ms step_avg:97.47ms
step:897/1770 train_time:87431ms step_avg:97.47ms
step:898/1770 train_time:87532ms step_avg:97.47ms
step:899/1770 train_time:87634ms step_avg:97.48ms
step:900/1770 train_time:87735ms step_avg:97.48ms
step:901/1770 train_time:87834ms step_avg:97.48ms
step:902/1770 train_time:87933ms step_avg:97.49ms
step:903/1770 train_time:88033ms step_avg:97.49ms
step:904/1770 train_time:88132ms step_avg:97.49ms
step:905/1770 train_time:88232ms step_avg:97.49ms
step:906/1770 train_time:88333ms step_avg:97.50ms
step:907/1770 train_time:88432ms step_avg:97.50ms
step:908/1770 train_time:88532ms step_avg:97.50ms
step:909/1770 train_time:88634ms step_avg:97.51ms
step:910/1770 train_time:88735ms step_avg:97.51ms
step:911/1770 train_time:88837ms step_avg:97.52ms
step:912/1770 train_time:88934ms step_avg:97.52ms
step:913/1770 train_time:89034ms step_avg:97.52ms
step:914/1770 train_time:89135ms step_avg:97.52ms
step:915/1770 train_time:89235ms step_avg:97.52ms
step:916/1770 train_time:89335ms step_avg:97.53ms
step:917/1770 train_time:89436ms step_avg:97.53ms
step:918/1770 train_time:89536ms step_avg:97.53ms
step:919/1770 train_time:89636ms step_avg:97.54ms
step:920/1770 train_time:89737ms step_avg:97.54ms
step:921/1770 train_time:89839ms step_avg:97.55ms
step:922/1770 train_time:89940ms step_avg:97.55ms
step:923/1770 train_time:90042ms step_avg:97.55ms
step:924/1770 train_time:90145ms step_avg:97.56ms
step:925/1770 train_time:90248ms step_avg:97.56ms
step:926/1770 train_time:90351ms step_avg:97.57ms
step:927/1770 train_time:90453ms step_avg:97.58ms
step:928/1770 train_time:90554ms step_avg:97.58ms
step:929/1770 train_time:90655ms step_avg:97.58ms
step:930/1770 train_time:90758ms step_avg:97.59ms
step:931/1770 train_time:90858ms step_avg:97.59ms
step:932/1770 train_time:90960ms step_avg:97.60ms
step:933/1770 train_time:91061ms step_avg:97.60ms
step:934/1770 train_time:91163ms step_avg:97.61ms
step:935/1770 train_time:91266ms step_avg:97.61ms
step:936/1770 train_time:91370ms step_avg:97.62ms
step:937/1770 train_time:91472ms step_avg:97.62ms
step:938/1770 train_time:91574ms step_avg:97.63ms
step:939/1770 train_time:91675ms step_avg:97.63ms
step:940/1770 train_time:91777ms step_avg:97.64ms
step:941/1770 train_time:91878ms step_avg:97.64ms
step:942/1770 train_time:91980ms step_avg:97.64ms
step:943/1770 train_time:92082ms step_avg:97.65ms
step:944/1770 train_time:92185ms step_avg:97.65ms
step:945/1770 train_time:92287ms step_avg:97.66ms
step:946/1770 train_time:92392ms step_avg:97.67ms
step:947/1770 train_time:92493ms step_avg:97.67ms
step:948/1770 train_time:92594ms step_avg:97.67ms
step:949/1770 train_time:92696ms step_avg:97.68ms
step:950/1770 train_time:92798ms step_avg:97.68ms
step:951/1770 train_time:92899ms step_avg:97.69ms
step:952/1770 train_time:93000ms step_avg:97.69ms
step:953/1770 train_time:93102ms step_avg:97.69ms
step:954/1770 train_time:93203ms step_avg:97.70ms
step:955/1770 train_time:93306ms step_avg:97.70ms
step:956/1770 train_time:93409ms step_avg:97.71ms
step:957/1770 train_time:93511ms step_avg:97.71ms
step:958/1770 train_time:93613ms step_avg:97.72ms
step:959/1770 train_time:93715ms step_avg:97.72ms
step:960/1770 train_time:93816ms step_avg:97.73ms
step:961/1770 train_time:93917ms step_avg:97.73ms
step:962/1770 train_time:94019ms step_avg:97.73ms
step:963/1770 train_time:94120ms step_avg:97.74ms
step:964/1770 train_time:94222ms step_avg:97.74ms
step:965/1770 train_time:94325ms step_avg:97.75ms
step:966/1770 train_time:94428ms step_avg:97.75ms
step:967/1770 train_time:94532ms step_avg:97.76ms
step:968/1770 train_time:94633ms step_avg:97.76ms
step:969/1770 train_time:94735ms step_avg:97.77ms
step:970/1770 train_time:94836ms step_avg:97.77ms
step:971/1770 train_time:94938ms step_avg:97.77ms
step:972/1770 train_time:95039ms step_avg:97.78ms
step:973/1770 train_time:95140ms step_avg:97.78ms
step:974/1770 train_time:95241ms step_avg:97.78ms
step:975/1770 train_time:95343ms step_avg:97.79ms
step:976/1770 train_time:95447ms step_avg:97.79ms
step:977/1770 train_time:95550ms step_avg:97.80ms
step:978/1770 train_time:95652ms step_avg:97.80ms
step:979/1770 train_time:95754ms step_avg:97.81ms
step:980/1770 train_time:95855ms step_avg:97.81ms
step:981/1770 train_time:95956ms step_avg:97.81ms
step:982/1770 train_time:96057ms step_avg:97.82ms
step:983/1770 train_time:96158ms step_avg:97.82ms
step:984/1770 train_time:96261ms step_avg:97.83ms
step:985/1770 train_time:96361ms step_avg:97.83ms
step:986/1770 train_time:96464ms step_avg:97.83ms
step:987/1770 train_time:96566ms step_avg:97.84ms
step:988/1770 train_time:96671ms step_avg:97.85ms
step:989/1770 train_time:96774ms step_avg:97.85ms
step:990/1770 train_time:96876ms step_avg:97.85ms
step:991/1770 train_time:96978ms step_avg:97.86ms
step:992/1770 train_time:97079ms step_avg:97.86ms
step:993/1770 train_time:97181ms step_avg:97.87ms
step:994/1770 train_time:97282ms step_avg:97.87ms
step:995/1770 train_time:97384ms step_avg:97.87ms
step:996/1770 train_time:97486ms step_avg:97.88ms
step:997/1770 train_time:97590ms step_avg:97.88ms
step:998/1770 train_time:97693ms step_avg:97.89ms
step:999/1770 train_time:97794ms step_avg:97.89ms
step:1000/1770 train_time:97895ms step_avg:97.90ms
step:1000/1770 val_loss:3.5116 train_time:97996ms step_avg:98.00ms
step:1001/1770 train_time:98014ms step_avg:97.92ms
step:1002/1770 train_time:98106ms step_avg:97.91ms
step:1003/1770 train_time:98209ms step_avg:97.92ms
step:1004/1770 train_time:98312ms step_avg:97.92ms
step:1005/1770 train_time:98414ms step_avg:97.92ms
step:1006/1770 train_time:98515ms step_avg:97.93ms
step:1007/1770 train_time:98616ms step_avg:97.93ms
step:1008/1770 train_time:98718ms step_avg:97.93ms
step:1009/1770 train_time:98819ms step_avg:97.94ms
step:1010/1770 train_time:98919ms step_avg:97.94ms
step:1011/1770 train_time:99023ms step_avg:97.95ms
step:1012/1770 train_time:99126ms step_avg:97.95ms
step:1013/1770 train_time:99228ms step_avg:97.96ms
step:1014/1770 train_time:99332ms step_avg:97.96ms
step:1015/1770 train_time:99433ms step_avg:97.96ms
step:1016/1770 train_time:99536ms step_avg:97.97ms
step:1017/1770 train_time:99637ms step_avg:97.97ms
step:1018/1770 train_time:99738ms step_avg:97.97ms
step:1019/1770 train_time:99839ms step_avg:97.98ms
step:1020/1770 train_time:99940ms step_avg:97.98ms
step:1021/1770 train_time:100041ms step_avg:97.98ms
step:1022/1770 train_time:100143ms step_avg:97.99ms
step:1023/1770 train_time:100246ms step_avg:97.99ms
step:1024/1770 train_time:100349ms step_avg:98.00ms
step:1025/1770 train_time:100451ms step_avg:98.00ms
step:1026/1770 train_time:100555ms step_avg:98.01ms
step:1027/1770 train_time:100657ms step_avg:98.01ms
step:1028/1770 train_time:100759ms step_avg:98.01ms
step:1029/1770 train_time:100860ms step_avg:98.02ms
step:1030/1770 train_time:100961ms step_avg:98.02ms
step:1031/1770 train_time:101062ms step_avg:98.02ms
step:1032/1770 train_time:101164ms step_avg:98.03ms
step:1033/1770 train_time:101265ms step_avg:98.03ms
step:1034/1770 train_time:101367ms step_avg:98.03ms
step:1035/1770 train_time:101471ms step_avg:98.04ms
step:1036/1770 train_time:101574ms step_avg:98.04ms
step:1037/1770 train_time:101676ms step_avg:98.05ms
step:1038/1770 train_time:101778ms step_avg:98.05ms
step:1039/1770 train_time:101879ms step_avg:98.05ms
step:1040/1770 train_time:101981ms step_avg:98.06ms
step:1041/1770 train_time:102082ms step_avg:98.06ms
step:1042/1770 train_time:102183ms step_avg:98.06ms
step:1043/1770 train_time:102284ms step_avg:98.07ms
step:1044/1770 train_time:102386ms step_avg:98.07ms
step:1045/1770 train_time:102488ms step_avg:98.07ms
step:1046/1770 train_time:102592ms step_avg:98.08ms
step:1047/1770 train_time:102696ms step_avg:98.09ms
step:1048/1770 train_time:102799ms step_avg:98.09ms
step:1049/1770 train_time:102900ms step_avg:98.09ms
step:1050/1770 train_time:103002ms step_avg:98.10ms
step:1051/1770 train_time:103104ms step_avg:98.10ms
step:1052/1770 train_time:103206ms step_avg:98.10ms
step:1053/1770 train_time:103308ms step_avg:98.11ms
step:1054/1770 train_time:103410ms step_avg:98.11ms
step:1055/1770 train_time:103513ms step_avg:98.12ms
step:1056/1770 train_time:103615ms step_avg:98.12ms
step:1057/1770 train_time:103718ms step_avg:98.12ms
step:1058/1770 train_time:103820ms step_avg:98.13ms
step:1059/1770 train_time:103922ms step_avg:98.13ms
step:1060/1770 train_time:104024ms step_avg:98.14ms
step:1061/1770 train_time:104125ms step_avg:98.14ms
step:1062/1770 train_time:104227ms step_avg:98.14ms
step:1063/1770 train_time:104329ms step_avg:98.15ms
step:1064/1770 train_time:104433ms step_avg:98.15ms
step:1065/1770 train_time:104535ms step_avg:98.15ms
step:1066/1770 train_time:104637ms step_avg:98.16ms
step:1067/1770 train_time:104740ms step_avg:98.16ms
step:1068/1770 train_time:104843ms step_avg:98.17ms
step:1069/1770 train_time:104945ms step_avg:98.17ms
step:1070/1770 train_time:105046ms step_avg:98.17ms
step:1071/1770 train_time:105149ms step_avg:98.18ms
step:1072/1770 train_time:105250ms step_avg:98.18ms
step:1073/1770 train_time:105353ms step_avg:98.19ms
step:1074/1770 train_time:105456ms step_avg:98.19ms
step:1075/1770 train_time:105558ms step_avg:98.19ms
step:1076/1770 train_time:105660ms step_avg:98.20ms
step:1077/1770 train_time:105762ms step_avg:98.20ms
step:1078/1770 train_time:105864ms step_avg:98.20ms
step:1079/1770 train_time:105966ms step_avg:98.21ms
step:1080/1770 train_time:106068ms step_avg:98.21ms
step:1081/1770 train_time:106169ms step_avg:98.21ms
step:1082/1770 train_time:106272ms step_avg:98.22ms
step:1083/1770 train_time:106374ms step_avg:98.22ms
step:1084/1770 train_time:106477ms step_avg:98.23ms
step:1085/1770 train_time:106579ms step_avg:98.23ms
step:1086/1770 train_time:106680ms step_avg:98.23ms
step:1087/1770 train_time:106782ms step_avg:98.24ms
step:1088/1770 train_time:106883ms step_avg:98.24ms
step:1089/1770 train_time:106985ms step_avg:98.24ms
step:1090/1770 train_time:107086ms step_avg:98.24ms
step:1091/1770 train_time:107188ms step_avg:98.25ms
step:1092/1770 train_time:107291ms step_avg:98.25ms
step:1093/1770 train_time:107396ms step_avg:98.26ms
step:1094/1770 train_time:107498ms step_avg:98.26ms
step:1095/1770 train_time:107599ms step_avg:98.26ms
step:1096/1770 train_time:107701ms step_avg:98.27ms
step:1097/1770 train_time:107803ms step_avg:98.27ms
step:1098/1770 train_time:107904ms step_avg:98.27ms
step:1099/1770 train_time:108006ms step_avg:98.28ms
step:1100/1770 train_time:108108ms step_avg:98.28ms
step:1101/1770 train_time:108211ms step_avg:98.28ms
step:1102/1770 train_time:108314ms step_avg:98.29ms
step:1103/1770 train_time:108418ms step_avg:98.29ms
step:1104/1770 train_time:108520ms step_avg:98.30ms
step:1105/1770 train_time:108621ms step_avg:98.30ms
step:1106/1770 train_time:108723ms step_avg:98.30ms
step:1107/1770 train_time:108825ms step_avg:98.31ms
step:1108/1770 train_time:108926ms step_avg:98.31ms
step:1109/1770 train_time:109028ms step_avg:98.31ms
step:1110/1770 train_time:109131ms step_avg:98.32ms
step:1111/1770 train_time:109233ms step_avg:98.32ms
step:1112/1770 train_time:109336ms step_avg:98.32ms
step:1113/1770 train_time:109438ms step_avg:98.33ms
step:1114/1770 train_time:109542ms step_avg:98.33ms
step:1115/1770 train_time:109646ms step_avg:98.34ms
step:1116/1770 train_time:109745ms step_avg:98.34ms
step:1117/1770 train_time:109846ms step_avg:98.34ms
step:1118/1770 train_time:109947ms step_avg:98.34ms
step:1119/1770 train_time:110051ms step_avg:98.35ms
step:1120/1770 train_time:110152ms step_avg:98.35ms
step:1121/1770 train_time:110255ms step_avg:98.35ms
step:1122/1770 train_time:110359ms step_avg:98.36ms
step:1123/1770 train_time:110461ms step_avg:98.36ms
step:1124/1770 train_time:110563ms step_avg:98.37ms
step:1125/1770 train_time:110665ms step_avg:98.37ms
step:1125/1770 val_loss:3.4729 train_time:110767ms step_avg:98.46ms
step:1126/1770 train_time:110785ms step_avg:98.39ms
step:1127/1770 train_time:110878ms step_avg:98.38ms
step:1128/1770 train_time:110981ms step_avg:98.39ms
step:1129/1770 train_time:111084ms step_avg:98.39ms
step:1130/1770 train_time:111186ms step_avg:98.39ms
step:1131/1770 train_time:111288ms step_avg:98.40ms
step:1132/1770 train_time:111389ms step_avg:98.40ms
step:1133/1770 train_time:111491ms step_avg:98.40ms
step:1134/1770 train_time:111592ms step_avg:98.41ms
step:1135/1770 train_time:111693ms step_avg:98.41ms
step:1136/1770 train_time:111795ms step_avg:98.41ms
step:1137/1770 train_time:111899ms step_avg:98.42ms
step:1138/1770 train_time:112002ms step_avg:98.42ms
step:1139/1770 train_time:112105ms step_avg:98.42ms
step:1140/1770 train_time:112208ms step_avg:98.43ms
step:1141/1770 train_time:112309ms step_avg:98.43ms
step:1142/1770 train_time:112410ms step_avg:98.43ms
step:1143/1770 train_time:112511ms step_avg:98.43ms
step:1144/1770 train_time:112617ms step_avg:98.44ms
step:1145/1770 train_time:112714ms step_avg:98.44ms
step:1146/1770 train_time:112816ms step_avg:98.44ms
step:1147/1770 train_time:112919ms step_avg:98.45ms
step:1148/1770 train_time:113022ms step_avg:98.45ms
step:1149/1770 train_time:113128ms step_avg:98.46ms
step:1150/1770 train_time:113229ms step_avg:98.46ms
step:1151/1770 train_time:113332ms step_avg:98.46ms
step:1152/1770 train_time:113433ms step_avg:98.47ms
step:1153/1770 train_time:113535ms step_avg:98.47ms
step:1154/1770 train_time:113637ms step_avg:98.47ms
step:1155/1770 train_time:113738ms step_avg:98.47ms
step:1156/1770 train_time:113840ms step_avg:98.48ms
step:1157/1770 train_time:113943ms step_avg:98.48ms
step:1158/1770 train_time:114046ms step_avg:98.49ms
step:1159/1770 train_time:114149ms step_avg:98.49ms
step:1160/1770 train_time:114252ms step_avg:98.49ms
step:1161/1770 train_time:114354ms step_avg:98.50ms
step:1162/1770 train_time:114456ms step_avg:98.50ms
step:1163/1770 train_time:114558ms step_avg:98.50ms
step:1164/1770 train_time:114660ms step_avg:98.51ms
step:1165/1770 train_time:114762ms step_avg:98.51ms
step:1166/1770 train_time:114865ms step_avg:98.51ms
step:1167/1770 train_time:114968ms step_avg:98.52ms
step:1168/1770 train_time:115070ms step_avg:98.52ms
step:1169/1770 train_time:115173ms step_avg:98.52ms
step:1170/1770 train_time:115275ms step_avg:98.53ms
step:1171/1770 train_time:115377ms step_avg:98.53ms
step:1172/1770 train_time:115478ms step_avg:98.53ms
step:1173/1770 train_time:115581ms step_avg:98.53ms
step:1174/1770 train_time:115683ms step_avg:98.54ms
step:1175/1770 train_time:115786ms step_avg:98.54ms
step:1176/1770 train_time:115888ms step_avg:98.54ms
step:1177/1770 train_time:115991ms step_avg:98.55ms
step:1178/1770 train_time:116092ms step_avg:98.55ms
step:1179/1770 train_time:116195ms step_avg:98.55ms
step:1180/1770 train_time:116297ms step_avg:98.56ms
step:1181/1770 train_time:116400ms step_avg:98.56ms
step:1182/1770 train_time:116502ms step_avg:98.56ms
step:1183/1770 train_time:116607ms step_avg:98.57ms
step:1184/1770 train_time:116712ms step_avg:98.57ms
step:1185/1770 train_time:116814ms step_avg:98.58ms
step:1186/1770 train_time:116920ms step_avg:98.58ms
step:1187/1770 train_time:117023ms step_avg:98.59ms
step:1188/1770 train_time:117128ms step_avg:98.59ms
step:1189/1770 train_time:117231ms step_avg:98.60ms
step:1190/1770 train_time:117335ms step_avg:98.60ms
step:1191/1770 train_time:117438ms step_avg:98.60ms
step:1192/1770 train_time:117540ms step_avg:98.61ms
step:1193/1770 train_time:117645ms step_avg:98.61ms
step:1194/1770 train_time:117749ms step_avg:98.62ms
step:1195/1770 train_time:117852ms step_avg:98.62ms
step:1196/1770 train_time:117955ms step_avg:98.62ms
step:1197/1770 train_time:118058ms step_avg:98.63ms
step:1198/1770 train_time:118161ms step_avg:98.63ms
step:1199/1770 train_time:118267ms step_avg:98.64ms
step:1200/1770 train_time:118371ms step_avg:98.64ms
step:1201/1770 train_time:118474ms step_avg:98.65ms
step:1202/1770 train_time:118576ms step_avg:98.65ms
step:1203/1770 train_time:118679ms step_avg:98.65ms
step:1204/1770 train_time:118783ms step_avg:98.66ms
step:1205/1770 train_time:118888ms step_avg:98.66ms
step:1206/1770 train_time:118992ms step_avg:98.67ms
step:1207/1770 train_time:119096ms step_avg:98.67ms
step:1208/1770 train_time:119198ms step_avg:98.67ms
step:1209/1770 train_time:119302ms step_avg:98.68ms
step:1210/1770 train_time:119405ms step_avg:98.68ms
step:1211/1770 train_time:119508ms step_avg:98.69ms
step:1212/1770 train_time:119613ms step_avg:98.69ms
step:1213/1770 train_time:119715ms step_avg:98.69ms
step:1214/1770 train_time:119818ms step_avg:98.70ms
step:1215/1770 train_time:119922ms step_avg:98.70ms
step:1216/1770 train_time:120029ms step_avg:98.71ms
step:1217/1770 train_time:120133ms step_avg:98.71ms
step:1218/1770 train_time:120235ms step_avg:98.71ms
step:1219/1770 train_time:120338ms step_avg:98.72ms
step:1220/1770 train_time:120440ms step_avg:98.72ms
step:1221/1770 train_time:120545ms step_avg:98.73ms
step:1222/1770 train_time:120649ms step_avg:98.73ms
step:1223/1770 train_time:120751ms step_avg:98.73ms
step:1224/1770 train_time:120856ms step_avg:98.74ms
step:1225/1770 train_time:120959ms step_avg:98.74ms
step:1226/1770 train_time:121065ms step_avg:98.75ms
step:1227/1770 train_time:121171ms step_avg:98.75ms
step:1228/1770 train_time:121275ms step_avg:98.76ms
step:1229/1770 train_time:121377ms step_avg:98.76ms
step:1230/1770 train_time:121480ms step_avg:98.76ms
step:1231/1770 train_time:121584ms step_avg:98.77ms
step:1232/1770 train_time:121688ms step_avg:98.77ms
step:1233/1770 train_time:121791ms step_avg:98.78ms
step:1234/1770 train_time:121894ms step_avg:98.78ms
step:1235/1770 train_time:121997ms step_avg:98.78ms
step:1236/1770 train_time:122101ms step_avg:98.79ms
step:1237/1770 train_time:122206ms step_avg:98.79ms
step:1238/1770 train_time:122310ms step_avg:98.80ms
step:1239/1770 train_time:122415ms step_avg:98.80ms
step:1240/1770 train_time:122515ms step_avg:98.80ms
step:1241/1770 train_time:122618ms step_avg:98.81ms
step:1242/1770 train_time:122722ms step_avg:98.81ms
step:1243/1770 train_time:122827ms step_avg:98.82ms
step:1244/1770 train_time:122930ms step_avg:98.82ms
step:1245/1770 train_time:123033ms step_avg:98.82ms
step:1246/1770 train_time:123137ms step_avg:98.83ms
step:1247/1770 train_time:123239ms step_avg:98.83ms
step:1248/1770 train_time:123343ms step_avg:98.83ms
step:1249/1770 train_time:123447ms step_avg:98.84ms
step:1250/1770 train_time:123550ms step_avg:98.84ms
step:1250/1770 val_loss:3.4240 train_time:123653ms step_avg:98.92ms
step:1251/1770 train_time:123672ms step_avg:98.86ms
step:1252/1770 train_time:123767ms step_avg:98.86ms
step:1253/1770 train_time:123872ms step_avg:98.86ms
step:1254/1770 train_time:123976ms step_avg:98.86ms
step:1255/1770 train_time:124081ms step_avg:98.87ms
step:1256/1770 train_time:124183ms step_avg:98.87ms
step:1257/1770 train_time:124285ms step_avg:98.87ms
step:1258/1770 train_time:124387ms step_avg:98.88ms
step:1259/1770 train_time:124491ms step_avg:98.88ms
step:1260/1770 train_time:124597ms step_avg:98.89ms
step:1261/1770 train_time:124701ms step_avg:98.89ms
step:1262/1770 train_time:124807ms step_avg:98.90ms
step:1263/1770 train_time:124911ms step_avg:98.90ms
step:1264/1770 train_time:125016ms step_avg:98.91ms
step:1265/1770 train_time:125119ms step_avg:98.91ms
step:1266/1770 train_time:125222ms step_avg:98.91ms
step:1267/1770 train_time:125325ms step_avg:98.92ms
step:1268/1770 train_time:125428ms step_avg:98.92ms
step:1269/1770 train_time:125531ms step_avg:98.92ms
step:1270/1770 train_time:125635ms step_avg:98.93ms
step:1271/1770 train_time:125739ms step_avg:98.93ms
step:1272/1770 train_time:125843ms step_avg:98.93ms
step:1273/1770 train_time:125948ms step_avg:98.94ms
step:1274/1770 train_time:126052ms step_avg:98.94ms
step:1275/1770 train_time:126155ms step_avg:98.95ms
step:1276/1770 train_time:126259ms step_avg:98.95ms
step:1277/1770 train_time:126361ms step_avg:98.95ms
step:1278/1770 train_time:126464ms step_avg:98.95ms
step:1279/1770 train_time:126567ms step_avg:98.96ms
step:1280/1770 train_time:126671ms step_avg:98.96ms
step:1281/1770 train_time:126775ms step_avg:98.97ms
step:1282/1770 train_time:126879ms step_avg:98.97ms
step:1283/1770 train_time:126983ms step_avg:98.97ms
step:1284/1770 train_time:127087ms step_avg:98.98ms
step:1285/1770 train_time:127191ms step_avg:98.98ms
step:1286/1770 train_time:127295ms step_avg:98.99ms
step:1287/1770 train_time:127399ms step_avg:98.99ms
step:1288/1770 train_time:127501ms step_avg:98.99ms
step:1289/1770 train_time:127604ms step_avg:98.99ms
step:1290/1770 train_time:127707ms step_avg:99.00ms
step:1291/1770 train_time:127811ms step_avg:99.00ms
step:1292/1770 train_time:127916ms step_avg:99.01ms
step:1293/1770 train_time:128019ms step_avg:99.01ms
step:1294/1770 train_time:128122ms step_avg:99.01ms
step:1295/1770 train_time:128225ms step_avg:99.02ms
step:1296/1770 train_time:128329ms step_avg:99.02ms
step:1297/1770 train_time:128434ms step_avg:99.02ms
step:1298/1770 train_time:128538ms step_avg:99.03ms
step:1299/1770 train_time:128641ms step_avg:99.03ms
step:1300/1770 train_time:128744ms step_avg:99.03ms
step:1301/1770 train_time:128848ms step_avg:99.04ms
step:1302/1770 train_time:128952ms step_avg:99.04ms
step:1303/1770 train_time:129056ms step_avg:99.04ms
step:1304/1770 train_time:129159ms step_avg:99.05ms
step:1305/1770 train_time:129262ms step_avg:99.05ms
step:1306/1770 train_time:129364ms step_avg:99.05ms
step:1307/1770 train_time:129467ms step_avg:99.06ms
step:1308/1770 train_time:129571ms step_avg:99.06ms
step:1309/1770 train_time:129675ms step_avg:99.06ms
step:1310/1770 train_time:129782ms step_avg:99.07ms
step:1311/1770 train_time:129881ms step_avg:99.07ms
step:1312/1770 train_time:129984ms step_avg:99.07ms
step:1313/1770 train_time:130087ms step_avg:99.08ms
step:1314/1770 train_time:130191ms step_avg:99.08ms
step:1315/1770 train_time:130295ms step_avg:99.08ms
step:1316/1770 train_time:130398ms step_avg:99.09ms
step:1317/1770 train_time:130501ms step_avg:99.09ms
step:1318/1770 train_time:130608ms step_avg:99.10ms
step:1319/1770 train_time:130711ms step_avg:99.10ms
step:1320/1770 train_time:130815ms step_avg:99.10ms
step:1321/1770 train_time:130919ms step_avg:99.11ms
step:1322/1770 train_time:131022ms step_avg:99.11ms
step:1323/1770 train_time:131125ms step_avg:99.11ms
step:1324/1770 train_time:131227ms step_avg:99.11ms
step:1325/1770 train_time:131332ms step_avg:99.12ms
step:1326/1770 train_time:131435ms step_avg:99.12ms
step:1327/1770 train_time:131542ms step_avg:99.13ms
step:1328/1770 train_time:131644ms step_avg:99.13ms
step:1329/1770 train_time:131748ms step_avg:99.13ms
step:1330/1770 train_time:131853ms step_avg:99.14ms
step:1331/1770 train_time:131957ms step_avg:99.14ms
step:1332/1770 train_time:132060ms step_avg:99.14ms
step:1333/1770 train_time:132162ms step_avg:99.15ms
step:1334/1770 train_time:132265ms step_avg:99.15ms
step:1335/1770 train_time:132368ms step_avg:99.15ms
step:1336/1770 train_time:132473ms step_avg:99.16ms
step:1337/1770 train_time:132577ms step_avg:99.16ms
step:1338/1770 train_time:132681ms step_avg:99.16ms
step:1339/1770 train_time:132785ms step_avg:99.17ms
step:1340/1770 train_time:132890ms step_avg:99.17ms
step:1341/1770 train_time:132995ms step_avg:99.18ms
step:1342/1770 train_time:133100ms step_avg:99.18ms
step:1343/1770 train_time:133203ms step_avg:99.18ms
step:1344/1770 train_time:133306ms step_avg:99.19ms
step:1345/1770 train_time:133409ms step_avg:99.19ms
step:1346/1770 train_time:133514ms step_avg:99.19ms
step:1347/1770 train_time:133618ms step_avg:99.20ms
step:1348/1770 train_time:133723ms step_avg:99.20ms
step:1349/1770 train_time:133826ms step_avg:99.20ms
step:1350/1770 train_time:133931ms step_avg:99.21ms
step:1351/1770 train_time:134035ms step_avg:99.21ms
step:1352/1770 train_time:134139ms step_avg:99.22ms
step:1353/1770 train_time:134242ms step_avg:99.22ms
step:1354/1770 train_time:134345ms step_avg:99.22ms
step:1355/1770 train_time:134448ms step_avg:99.22ms
step:1356/1770 train_time:134552ms step_avg:99.23ms
step:1357/1770 train_time:134656ms step_avg:99.23ms
step:1358/1770 train_time:134759ms step_avg:99.23ms
step:1359/1770 train_time:134862ms step_avg:99.24ms
step:1360/1770 train_time:134966ms step_avg:99.24ms
step:1361/1770 train_time:135071ms step_avg:99.24ms
step:1362/1770 train_time:135175ms step_avg:99.25ms
step:1363/1770 train_time:135279ms step_avg:99.25ms
step:1364/1770 train_time:135382ms step_avg:99.25ms
step:1365/1770 train_time:135488ms step_avg:99.26ms
step:1366/1770 train_time:135587ms step_avg:99.26ms
step:1367/1770 train_time:135693ms step_avg:99.26ms
step:1368/1770 train_time:135795ms step_avg:99.27ms
step:1369/1770 train_time:135900ms step_avg:99.27ms
step:1370/1770 train_time:136004ms step_avg:99.27ms
step:1371/1770 train_time:136106ms step_avg:99.28ms
step:1372/1770 train_time:136210ms step_avg:99.28ms
step:1373/1770 train_time:136315ms step_avg:99.28ms
step:1374/1770 train_time:136420ms step_avg:99.29ms
step:1375/1770 train_time:136522ms step_avg:99.29ms
step:1375/1770 val_loss:3.3812 train_time:136625ms step_avg:99.36ms
step:1376/1770 train_time:136644ms step_avg:99.30ms
step:1377/1770 train_time:136740ms step_avg:99.30ms
step:1378/1770 train_time:136844ms step_avg:99.31ms
step:1379/1770 train_time:136947ms step_avg:99.31ms
step:1380/1770 train_time:137050ms step_avg:99.31ms
step:1381/1770 train_time:137152ms step_avg:99.31ms
step:1382/1770 train_time:137255ms step_avg:99.32ms
step:1383/1770 train_time:137358ms step_avg:99.32ms
step:1384/1770 train_time:137462ms step_avg:99.32ms
step:1385/1770 train_time:137565ms step_avg:99.33ms
step:1386/1770 train_time:137672ms step_avg:99.33ms
step:1387/1770 train_time:137776ms step_avg:99.33ms
step:1388/1770 train_time:137880ms step_avg:99.34ms
step:1389/1770 train_time:137984ms step_avg:99.34ms
step:1390/1770 train_time:138087ms step_avg:99.34ms
step:1391/1770 train_time:138190ms step_avg:99.35ms
step:1392/1770 train_time:138293ms step_avg:99.35ms
step:1393/1770 train_time:138396ms step_avg:99.35ms
step:1394/1770 train_time:138499ms step_avg:99.35ms
step:1395/1770 train_time:138604ms step_avg:99.36ms
step:1396/1770 train_time:138708ms step_avg:99.36ms
step:1397/1770 train_time:138813ms step_avg:99.36ms
step:1398/1770 train_time:138918ms step_avg:99.37ms
step:1399/1770 train_time:139023ms step_avg:99.37ms
step:1400/1770 train_time:139127ms step_avg:99.38ms
step:1401/1770 train_time:139229ms step_avg:99.38ms
step:1402/1770 train_time:139332ms step_avg:99.38ms
step:1403/1770 train_time:139434ms step_avg:99.38ms
step:1404/1770 train_time:139538ms step_avg:99.39ms
step:1405/1770 train_time:139642ms step_avg:99.39ms
step:1406/1770 train_time:139746ms step_avg:99.39ms
step:1407/1770 train_time:139849ms step_avg:99.40ms
step:1408/1770 train_time:139954ms step_avg:99.40ms
step:1409/1770 train_time:140063ms step_avg:99.41ms
step:1410/1770 train_time:140163ms step_avg:99.41ms
step:1411/1770 train_time:140266ms step_avg:99.41ms
step:1412/1770 train_time:140369ms step_avg:99.41ms
step:1413/1770 train_time:140471ms step_avg:99.41ms
step:1414/1770 train_time:140575ms step_avg:99.42ms
step:1415/1770 train_time:140680ms step_avg:99.42ms
step:1416/1770 train_time:140786ms step_avg:99.42ms
step:1417/1770 train_time:140889ms step_avg:99.43ms
step:1418/1770 train_time:140993ms step_avg:99.43ms
step:1419/1770 train_time:141098ms step_avg:99.43ms
step:1420/1770 train_time:141201ms step_avg:99.44ms
step:1421/1770 train_time:141305ms step_avg:99.44ms
step:1422/1770 train_time:141409ms step_avg:99.44ms
step:1423/1770 train_time:141511ms step_avg:99.45ms
step:1424/1770 train_time:141614ms step_avg:99.45ms
step:1425/1770 train_time:141719ms step_avg:99.45ms
step:1426/1770 train_time:141823ms step_avg:99.46ms
step:1427/1770 train_time:141926ms step_avg:99.46ms
step:1428/1770 train_time:142030ms step_avg:99.46ms
step:1429/1770 train_time:142134ms step_avg:99.46ms
step:1430/1770 train_time:142237ms step_avg:99.47ms
step:1431/1770 train_time:142344ms step_avg:99.47ms
step:1432/1770 train_time:142448ms step_avg:99.47ms
step:1433/1770 train_time:142551ms step_avg:99.48ms
step:1434/1770 train_time:142654ms step_avg:99.48ms
step:1435/1770 train_time:142757ms step_avg:99.48ms
step:1436/1770 train_time:142862ms step_avg:99.49ms
step:1437/1770 train_time:142965ms step_avg:99.49ms
step:1438/1770 train_time:143067ms step_avg:99.49ms
step:1439/1770 train_time:143170ms step_avg:99.49ms
step:1440/1770 train_time:143273ms step_avg:99.50ms
step:1441/1770 train_time:143382ms step_avg:99.50ms
step:1442/1770 train_time:143485ms step_avg:99.50ms
step:1443/1770 train_time:143588ms step_avg:99.51ms
step:1444/1770 train_time:143693ms step_avg:99.51ms
step:1445/1770 train_time:143796ms step_avg:99.51ms
step:1446/1770 train_time:143902ms step_avg:99.52ms
step:1447/1770 train_time:144007ms step_avg:99.52ms
step:1448/1770 train_time:144110ms step_avg:99.52ms
step:1449/1770 train_time:144216ms step_avg:99.53ms
step:1450/1770 train_time:144321ms step_avg:99.53ms
step:1451/1770 train_time:144427ms step_avg:99.54ms
step:1452/1770 train_time:144531ms step_avg:99.54ms
step:1453/1770 train_time:144635ms step_avg:99.54ms
step:1454/1770 train_time:144740ms step_avg:99.55ms
step:1455/1770 train_time:144846ms step_avg:99.55ms
step:1456/1770 train_time:144951ms step_avg:99.55ms
step:1457/1770 train_time:145055ms step_avg:99.56ms
step:1458/1770 train_time:145159ms step_avg:99.56ms
step:1459/1770 train_time:145266ms step_avg:99.57ms
step:1460/1770 train_time:145370ms step_avg:99.57ms
step:1461/1770 train_time:145475ms step_avg:99.57ms
step:1462/1770 train_time:145579ms step_avg:99.58ms
step:1463/1770 train_time:145684ms step_avg:99.58ms
step:1464/1770 train_time:145790ms step_avg:99.58ms
step:1465/1770 train_time:145894ms step_avg:99.59ms
step:1466/1770 train_time:146000ms step_avg:99.59ms
step:1467/1770 train_time:146104ms step_avg:99.59ms
step:1468/1770 train_time:146208ms step_avg:99.60ms
step:1469/1770 train_time:146313ms step_avg:99.60ms
step:1470/1770 train_time:146418ms step_avg:99.60ms
step:1471/1770 train_time:146524ms step_avg:99.61ms
step:1472/1770 train_time:146628ms step_avg:99.61ms
step:1473/1770 train_time:146734ms step_avg:99.62ms
step:1474/1770 train_time:146840ms step_avg:99.62ms
step:1475/1770 train_time:146945ms step_avg:99.62ms
step:1476/1770 train_time:147049ms step_avg:99.63ms
step:1477/1770 train_time:147155ms step_avg:99.63ms
step:1478/1770 train_time:147264ms step_avg:99.64ms
step:1479/1770 train_time:147365ms step_avg:99.64ms
step:1480/1770 train_time:147469ms step_avg:99.64ms
step:1481/1770 train_time:147577ms step_avg:99.65ms
step:1482/1770 train_time:147681ms step_avg:99.65ms
step:1483/1770 train_time:147786ms step_avg:99.65ms
step:1484/1770 train_time:147891ms step_avg:99.66ms
step:1485/1770 train_time:147994ms step_avg:99.66ms
step:1486/1770 train_time:148099ms step_avg:99.66ms
step:1487/1770 train_time:148204ms step_avg:99.67ms
step:1488/1770 train_time:148310ms step_avg:99.67ms
step:1489/1770 train_time:148416ms step_avg:99.67ms
step:1490/1770 train_time:148521ms step_avg:99.68ms
step:1491/1770 train_time:148625ms step_avg:99.68ms
step:1492/1770 train_time:148731ms step_avg:99.69ms
step:1493/1770 train_time:148839ms step_avg:99.69ms
step:1494/1770 train_time:148945ms step_avg:99.70ms
step:1495/1770 train_time:149049ms step_avg:99.70ms
step:1496/1770 train_time:149153ms step_avg:99.70ms
step:1497/1770 train_time:149258ms step_avg:99.70ms
step:1498/1770 train_time:149363ms step_avg:99.71ms
step:1499/1770 train_time:149468ms step_avg:99.71ms
step:1500/1770 train_time:149574ms step_avg:99.72ms
step:1500/1770 val_loss:3.3435 train_time:149677ms step_avg:99.78ms
step:1501/1770 train_time:149695ms step_avg:99.73ms
step:1502/1770 train_time:149787ms step_avg:99.73ms
step:1503/1770 train_time:149892ms step_avg:99.73ms
step:1504/1770 train_time:149996ms step_avg:99.73ms
step:1505/1770 train_time:150102ms step_avg:99.74ms
step:1506/1770 train_time:150207ms step_avg:99.74ms
step:1507/1770 train_time:150311ms step_avg:99.74ms
step:1508/1770 train_time:150416ms step_avg:99.75ms
step:1509/1770 train_time:150524ms step_avg:99.75ms
step:1510/1770 train_time:150625ms step_avg:99.75ms
step:1511/1770 train_time:150733ms step_avg:99.76ms
step:1512/1770 train_time:150839ms step_avg:99.76ms
step:1513/1770 train_time:150946ms step_avg:99.77ms
step:1514/1770 train_time:151050ms step_avg:99.77ms
step:1515/1770 train_time:151153ms step_avg:99.77ms
step:1516/1770 train_time:151258ms step_avg:99.77ms
step:1517/1770 train_time:151364ms step_avg:99.78ms
step:1518/1770 train_time:151469ms step_avg:99.78ms
step:1519/1770 train_time:151573ms step_avg:99.78ms
step:1520/1770 train_time:151677ms step_avg:99.79ms
step:1521/1770 train_time:151782ms step_avg:99.79ms
step:1522/1770 train_time:151889ms step_avg:99.80ms
step:1523/1770 train_time:151994ms step_avg:99.80ms
step:1524/1770 train_time:152098ms step_avg:99.80ms
step:1525/1770 train_time:152203ms step_avg:99.81ms
step:1526/1770 train_time:152307ms step_avg:99.81ms
step:1527/1770 train_time:152411ms step_avg:99.81ms
step:1528/1770 train_time:152518ms step_avg:99.82ms
step:1529/1770 train_time:152621ms step_avg:99.82ms
step:1530/1770 train_time:152728ms step_avg:99.82ms
step:1531/1770 train_time:152830ms step_avg:99.82ms
step:1532/1770 train_time:152936ms step_avg:99.83ms
step:1533/1770 train_time:153041ms step_avg:99.83ms
step:1534/1770 train_time:153147ms step_avg:99.83ms
step:1535/1770 train_time:153252ms step_avg:99.84ms
step:1536/1770 train_time:153356ms step_avg:99.84ms
step:1537/1770 train_time:153461ms step_avg:99.84ms
step:1538/1770 train_time:153569ms step_avg:99.85ms
step:1539/1770 train_time:153673ms step_avg:99.85ms
step:1540/1770 train_time:153779ms step_avg:99.86ms
step:1541/1770 train_time:153886ms step_avg:99.86ms
step:1542/1770 train_time:153990ms step_avg:99.86ms
step:1543/1770 train_time:154095ms step_avg:99.87ms
step:1544/1770 train_time:154201ms step_avg:99.87ms
step:1545/1770 train_time:154305ms step_avg:99.87ms
step:1546/1770 train_time:154410ms step_avg:99.88ms
step:1547/1770 train_time:154514ms step_avg:99.88ms
step:1548/1770 train_time:154618ms step_avg:99.88ms
step:1549/1770 train_time:154723ms step_avg:99.89ms
step:1550/1770 train_time:154831ms step_avg:99.89ms
step:1551/1770 train_time:154933ms step_avg:99.89ms
step:1552/1770 train_time:155039ms step_avg:99.90ms
step:1553/1770 train_time:155144ms step_avg:99.90ms
step:1554/1770 train_time:155247ms step_avg:99.90ms
step:1555/1770 train_time:155352ms step_avg:99.90ms
step:1556/1770 train_time:155456ms step_avg:99.91ms
step:1557/1770 train_time:155561ms step_avg:99.91ms
step:1558/1770 train_time:155666ms step_avg:99.91ms
step:1559/1770 train_time:155770ms step_avg:99.92ms
step:1560/1770 train_time:155875ms step_avg:99.92ms
step:1561/1770 train_time:155983ms step_avg:99.92ms
step:1562/1770 train_time:156087ms step_avg:99.93ms
step:1563/1770 train_time:156192ms step_avg:99.93ms
step:1564/1770 train_time:156296ms step_avg:99.93ms
step:1565/1770 train_time:156401ms step_avg:99.94ms
step:1566/1770 train_time:156506ms step_avg:99.94ms
step:1567/1770 train_time:156611ms step_avg:99.94ms
step:1568/1770 train_time:156715ms step_avg:99.95ms
step:1569/1770 train_time:156823ms step_avg:99.95ms
step:1570/1770 train_time:156928ms step_avg:99.95ms
step:1571/1770 train_time:157032ms step_avg:99.96ms
step:1572/1770 train_time:157137ms step_avg:99.96ms
step:1573/1770 train_time:157244ms step_avg:99.96ms
step:1574/1770 train_time:157348ms step_avg:99.97ms
step:1575/1770 train_time:157452ms step_avg:99.97ms
step:1576/1770 train_time:157556ms step_avg:99.97ms
step:1577/1770 train_time:157662ms step_avg:99.98ms
step:1578/1770 train_time:157768ms step_avg:99.98ms
step:1579/1770 train_time:157872ms step_avg:99.98ms
step:1580/1770 train_time:157976ms step_avg:99.99ms
step:1581/1770 train_time:158083ms step_avg:99.99ms
step:1582/1770 train_time:158189ms step_avg:99.99ms
step:1583/1770 train_time:158293ms step_avg:100.00ms
step:1584/1770 train_time:158398ms step_avg:100.00ms
step:1585/1770 train_time:158502ms step_avg:100.00ms
step:1586/1770 train_time:158610ms step_avg:100.01ms
step:1587/1770 train_time:158715ms step_avg:100.01ms
step:1588/1770 train_time:158820ms step_avg:100.01ms
step:1589/1770 train_time:158927ms step_avg:100.02ms
step:1590/1770 train_time:159032ms step_avg:100.02ms
step:1591/1770 train_time:159136ms step_avg:100.02ms
step:1592/1770 train_time:159242ms step_avg:100.03ms
step:1593/1770 train_time:159347ms step_avg:100.03ms
step:1594/1770 train_time:159450ms step_avg:100.03ms
step:1595/1770 train_time:159555ms step_avg:100.03ms
step:1596/1770 train_time:159661ms step_avg:100.04ms
step:1597/1770 train_time:159766ms step_avg:100.04ms
step:1598/1770 train_time:159870ms step_avg:100.04ms
step:1599/1770 train_time:159975ms step_avg:100.05ms
step:1600/1770 train_time:160081ms step_avg:100.05ms
step:1601/1770 train_time:160187ms step_avg:100.05ms
step:1602/1770 train_time:160292ms step_avg:100.06ms
step:1603/1770 train_time:160397ms step_avg:100.06ms
step:1604/1770 train_time:160501ms step_avg:100.06ms
step:1605/1770 train_time:160605ms step_avg:100.07ms
step:1606/1770 train_time:160710ms step_avg:100.07ms
step:1607/1770 train_time:160818ms step_avg:100.07ms
step:1608/1770 train_time:160923ms step_avg:100.08ms
step:1609/1770 train_time:161029ms step_avg:100.08ms
step:1610/1770 train_time:161134ms step_avg:100.08ms
step:1611/1770 train_time:161239ms step_avg:100.09ms
step:1612/1770 train_time:161346ms step_avg:100.09ms
step:1613/1770 train_time:161450ms step_avg:100.09ms
step:1614/1770 train_time:161554ms step_avg:100.10ms
step:1615/1770 train_time:161659ms step_avg:100.10ms
step:1616/1770 train_time:161765ms step_avg:100.10ms
step:1617/1770 train_time:161872ms step_avg:100.11ms
step:1618/1770 train_time:161978ms step_avg:100.11ms
step:1619/1770 train_time:162084ms step_avg:100.11ms
step:1620/1770 train_time:162191ms step_avg:100.12ms
step:1621/1770 train_time:162295ms step_avg:100.12ms
step:1622/1770 train_time:162401ms step_avg:100.12ms
step:1623/1770 train_time:162508ms step_avg:100.13ms
step:1624/1770 train_time:162612ms step_avg:100.13ms
step:1625/1770 train_time:162716ms step_avg:100.13ms
step:1625/1770 val_loss:3.3094 train_time:162820ms step_avg:100.20ms
step:1626/1770 train_time:162839ms step_avg:100.15ms
step:1627/1770 train_time:162933ms step_avg:100.14ms
step:1628/1770 train_time:163038ms step_avg:100.15ms
step:1629/1770 train_time:163142ms step_avg:100.15ms
step:1630/1770 train_time:163246ms step_avg:100.15ms
step:1631/1770 train_time:163350ms step_avg:100.15ms
step:1632/1770 train_time:163454ms step_avg:100.16ms
step:1633/1770 train_time:163559ms step_avg:100.16ms
step:1634/1770 train_time:163662ms step_avg:100.16ms
step:1635/1770 train_time:163766ms step_avg:100.16ms
step:1636/1770 train_time:163873ms step_avg:100.17ms
step:1637/1770 train_time:163980ms step_avg:100.17ms
step:1638/1770 train_time:164084ms step_avg:100.17ms
step:1639/1770 train_time:164189ms step_avg:100.18ms
step:1640/1770 train_time:164293ms step_avg:100.18ms
step:1641/1770 train_time:164398ms step_avg:100.18ms
step:1642/1770 train_time:164502ms step_avg:100.18ms
step:1643/1770 train_time:164606ms step_avg:100.19ms
step:1644/1770 train_time:164711ms step_avg:100.19ms
step:1645/1770 train_time:164817ms step_avg:100.19ms
step:1646/1770 train_time:164923ms step_avg:100.20ms
step:1647/1770 train_time:165029ms step_avg:100.20ms
step:1648/1770 train_time:165133ms step_avg:100.20ms
step:1649/1770 train_time:165238ms step_avg:100.20ms
step:1650/1770 train_time:165342ms step_avg:100.21ms
step:1651/1770 train_time:165446ms step_avg:100.21ms
step:1652/1770 train_time:165550ms step_avg:100.21ms
step:1653/1770 train_time:165656ms step_avg:100.22ms
step:1654/1770 train_time:165764ms step_avg:100.22ms
step:1655/1770 train_time:165871ms step_avg:100.22ms
step:1656/1770 train_time:165979ms step_avg:100.23ms
step:1657/1770 train_time:166083ms step_avg:100.23ms
step:1658/1770 train_time:166188ms step_avg:100.23ms
step:1659/1770 train_time:166293ms step_avg:100.24ms
step:1660/1770 train_time:166399ms step_avg:100.24ms
step:1661/1770 train_time:166504ms step_avg:100.24ms
step:1662/1770 train_time:166608ms step_avg:100.25ms
step:1663/1770 train_time:166712ms step_avg:100.25ms
step:1664/1770 train_time:166818ms step_avg:100.25ms
step:1665/1770 train_time:166922ms step_avg:100.25ms
step:1666/1770 train_time:167027ms step_avg:100.26ms
step:1667/1770 train_time:167132ms step_avg:100.26ms
step:1668/1770 train_time:167238ms step_avg:100.26ms
step:1669/1770 train_time:167341ms step_avg:100.26ms
step:1670/1770 train_time:167445ms step_avg:100.27ms
step:1671/1770 train_time:167550ms step_avg:100.27ms
step:1672/1770 train_time:167656ms step_avg:100.27ms
step:1673/1770 train_time:167762ms step_avg:100.28ms
step:1674/1770 train_time:167866ms step_avg:100.28ms
step:1675/1770 train_time:167970ms step_avg:100.28ms
step:1676/1770 train_time:168076ms step_avg:100.28ms
step:1677/1770 train_time:168185ms step_avg:100.29ms
step:1678/1770 train_time:168288ms step_avg:100.29ms
step:1679/1770 train_time:168394ms step_avg:100.29ms
step:1680/1770 train_time:168498ms step_avg:100.30ms
step:1681/1770 train_time:168603ms step_avg:100.30ms
step:1682/1770 train_time:168709ms step_avg:100.30ms
step:1683/1770 train_time:168814ms step_avg:100.31ms
step:1684/1770 train_time:168919ms step_avg:100.31ms
step:1685/1770 train_time:169023ms step_avg:100.31ms
step:1686/1770 train_time:169128ms step_avg:100.31ms
step:1687/1770 train_time:169235ms step_avg:100.32ms
step:1688/1770 train_time:169340ms step_avg:100.32ms
step:1689/1770 train_time:169445ms step_avg:100.32ms
step:1690/1770 train_time:169549ms step_avg:100.32ms
step:1691/1770 train_time:169655ms step_avg:100.33ms
step:1692/1770 train_time:169760ms step_avg:100.33ms
step:1693/1770 train_time:169865ms step_avg:100.33ms
step:1694/1770 train_time:169969ms step_avg:100.34ms
step:1695/1770 train_time:170075ms step_avg:100.34ms
step:1696/1770 train_time:170181ms step_avg:100.34ms
step:1697/1770 train_time:170287ms step_avg:100.35ms
step:1698/1770 train_time:170392ms step_avg:100.35ms
step:1699/1770 train_time:170497ms step_avg:100.35ms
step:1700/1770 train_time:170601ms step_avg:100.35ms
step:1701/1770 train_time:170705ms step_avg:100.36ms
step:1702/1770 train_time:170812ms step_avg:100.36ms
step:1703/1770 train_time:170917ms step_avg:100.36ms
step:1704/1770 train_time:171022ms step_avg:100.37ms
step:1705/1770 train_time:171127ms step_avg:100.37ms
step:1706/1770 train_time:171231ms step_avg:100.37ms
step:1707/1770 train_time:171337ms step_avg:100.37ms
step:1708/1770 train_time:171444ms step_avg:100.38ms
step:1709/1770 train_time:171549ms step_avg:100.38ms
step:1710/1770 train_time:171658ms step_avg:100.38ms
step:1711/1770 train_time:171768ms step_avg:100.39ms
step:1712/1770 train_time:171874ms step_avg:100.39ms
step:1713/1770 train_time:171979ms step_avg:100.40ms
step:1714/1770 train_time:172084ms step_avg:100.40ms
step:1715/1770 train_time:172188ms step_avg:100.40ms
step:1716/1770 train_time:172294ms step_avg:100.40ms
step:1717/1770 train_time:172401ms step_avg:100.41ms
step:1718/1770 train_time:172506ms step_avg:100.41ms
step:1719/1770 train_time:172612ms step_avg:100.41ms
step:1720/1770 train_time:172719ms step_avg:100.42ms
step:1721/1770 train_time:172824ms step_avg:100.42ms
step:1722/1770 train_time:172932ms step_avg:100.43ms
step:1723/1770 train_time:173039ms step_avg:100.43ms
step:1724/1770 train_time:173146ms step_avg:100.43ms
step:1725/1770 train_time:173253ms step_avg:100.44ms
step:1726/1770 train_time:173361ms step_avg:100.44ms
step:1727/1770 train_time:173467ms step_avg:100.44ms
step:1728/1770 train_time:173575ms step_avg:100.45ms
step:1729/1770 train_time:173680ms step_avg:100.45ms
step:1730/1770 train_time:173786ms step_avg:100.45ms
step:1731/1770 train_time:173893ms step_avg:100.46ms
step:1732/1770 train_time:173999ms step_avg:100.46ms
step:1733/1770 train_time:174105ms step_avg:100.46ms
step:1734/1770 train_time:174210ms step_avg:100.47ms
step:1735/1770 train_time:174318ms step_avg:100.47ms
step:1736/1770 train_time:174422ms step_avg:100.47ms
step:1737/1770 train_time:174528ms step_avg:100.48ms
step:1738/1770 train_time:174634ms step_avg:100.48ms
step:1739/1770 train_time:174739ms step_avg:100.48ms
step:1740/1770 train_time:174844ms step_avg:100.49ms
step:1741/1770 train_time:174951ms step_avg:100.49ms
step:1742/1770 train_time:175060ms step_avg:100.49ms
step:1743/1770 train_time:175166ms step_avg:100.50ms
step:1744/1770 train_time:175272ms step_avg:100.50ms
step:1745/1770 train_time:175378ms step_avg:100.50ms
step:1746/1770 train_time:175486ms step_avg:100.51ms
step:1747/1770 train_time:175589ms step_avg:100.51ms
step:1748/1770 train_time:175696ms step_avg:100.51ms
step:1749/1770 train_time:175802ms step_avg:100.52ms
step:1750/1770 train_time:175906ms step_avg:100.52ms
step:1750/1770 val_loss:3.2824 train_time:176012ms step_avg:100.58ms
step:1751/1770 train_time:176030ms step_avg:100.53ms
step:1752/1770 train_time:176127ms step_avg:100.53ms
step:1753/1770 train_time:176232ms step_avg:100.53ms
step:1754/1770 train_time:176338ms step_avg:100.53ms
step:1755/1770 train_time:176443ms step_avg:100.54ms
step:1756/1770 train_time:176549ms step_avg:100.54ms
step:1757/1770 train_time:176654ms step_avg:100.54ms
step:1758/1770 train_time:176759ms step_avg:100.55ms
step:1759/1770 train_time:176865ms step_avg:100.55ms
step:1760/1770 train_time:176972ms step_avg:100.55ms
step:1761/1770 train_time:177080ms step_avg:100.56ms
step:1762/1770 train_time:177189ms step_avg:100.56ms
step:1763/1770 train_time:177292ms step_avg:100.56ms
step:1764/1770 train_time:177399ms step_avg:100.57ms
step:1765/1770 train_time:177505ms step_avg:100.57ms
step:1766/1770 train_time:177614ms step_avg:100.57ms
step:1767/1770 train_time:177718ms step_avg:100.58ms
step:1768/1770 train_time:177824ms step_avg:100.58ms
step:1769/1770 train_time:177929ms step_avg:100.58ms
step:1770/1770 train_time:178034ms step_avg:100.58ms
step:1770/1770 val_loss:3.2794 train_time:178141ms step_avg:100.64ms
peak memory allocated: 30724 MiB reserved: 46432 MiB
