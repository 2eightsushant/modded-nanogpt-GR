import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
import copy
import glob
from dataclasses import dataclass
from functools import lru_cache, partial # Added partial for hook registration
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
#torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params: list[Tensor] = [*params]
        param_groups = []
        for size in {p.numel() for p in params}:
            b = torch.empty(world_size, size, dtype=torch.bfloat16, device="cuda")
            group = dict(params=[p for p in params if p.numel() == size],
                         update_buffer=b, update_buffer_views=[b[i] for i in range(world_size)])
            param_groups.append(group)
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            update_buffer: Tensor = group["update_buffer"]
            update_buffer_views: list[Tensor] = group["update_buffer_views"]
            # generate weight updates in distributed fashion
            params: list[Tensor] = group["params"]
            handle = None
            params_world = None
            def update_prev(): # optimized Muon implementation contributed by @YouJiacheng
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffer_views):
                    p_world.add_(g_world.view_as(p_world),
                                 alpha=-group["lr"] * max(1, p_world.size(-2) / p_world.size(-1))**0.5)
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if "momentum_buffer" not in state:
                        state["momentum_buffer"] = torch.zeros_like(g)
                    buf: Tensor = state["momentum_buffer"]
                    buf.lerp_(g, 1 - group["momentum"])
                    g = g.lerp_(buf, group["momentum"]) if group["nesterov"] else buf
                    g = zeropower_via_newtonschulz5(g, steps=group["ns_steps"]).flatten()
                else:
                    g = update_buffer_views[self.rank]
                if base_i > 0:
                    update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=0.12).transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, block_mask: BlockMask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, max_seq_len, i) for i in range(num_layers)])
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128),
                                    use_fp8=True, x_s=(model_dim**0.5)/448, w_s=24/448, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        self.skip_weights = nn.Parameter(torch.ones(num_layers//2))

    def create_blockmasks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        docs = (input_seq == 50256).cumsum(0)

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_blockmask: Tensor):
            num_blocks = dense_blockmask.sum(dim=-1, dtype=torch.int32)
            indices = dense_blockmask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
        causal_blockmask_any = block_idx[:, None] >= block_idx
        causal_blockmask_all = block_idx[:, None] > block_idx
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
        document_blockmask_any = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
        document_blockmask_all = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
        blockmask_any = causal_blockmask_any & document_blockmask_any
        blockmask_all = causal_blockmask_all & document_blockmask_all
        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(blockmask_any & ~blockmask_all)
        full_kv_num_blocks, full_kv_indices = dense_to_ordered(blockmask_all)
        def build_bm(window_size_blocks: Tensor) -> BlockMask:
            return BlockMask.from_kv_blocks(
                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(window_size_blocks - full_kv_num_blocks, 1)),
                partial_kv_indices,
                torch.clamp_max(full_kv_num_blocks, window_size_blocks - 1),
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        long_bm, short_bm = self.create_blockmasks(input_seq, sliding_window_num_blocks)
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, long_bm, short_bm, short_bm, short_bm, long_bm]
        assert len(block_masks) == len(self.blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977

        # U-net design by @brendanh0gan
        skip_connections = []
        n = len(self.skip_weights)
        for i in range(len(self.blocks)):
            if i >= n:
                x = x + self.skip_weights[i - n] * skip_connections.pop()
            x = self.blocks[i](x, ve[i], x0, block_masks[i])
            if i < n:
                skip_connections.append(x)

        x = norm(x)
        logits = self.lm_head(x).float()
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / (7.5 * x.size(-1)**0.5))
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq, reduction='sum' if self.training else 'mean')
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn't helpful.
        pos += batch_size
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_seq_len = 48*1024 # FlexAttention sequence length
    val_seq_len = 4*64*1024 # FlexAttention sequence length for validation
    # optimization
    num_iterations = 1770 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    # architecture
    vocab_size = 50257
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert world_size == 8 # this code is designed for 8xH100
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs("logs_bucket", exist_ok=True)
    logfile = f"logs_bucket/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

########################################
#    Construct model and optimizer     #
########################################

model: nn.Module = GPT(vocab_size=args.vocab_size, num_layers=12, num_heads=6, model_dim=768,
                       max_seq_len=max(args.train_seq_len, args.val_seq_len)).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
adam_params = [dict(params=head_params, lr=0.22), dict(params=embed_params, lr=0.6), dict(params=scalar_params, lr=0.04)]
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = torch.optim.Adam(adam_params, betas=(0.8, 0.95), eps=1e-10, fused=True)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then decay
def get_lr(step: int):
    x = step / args.num_iterations # progress in training
    assert 0 <= x < 1
    if x < 1 - args.cooldown_frac:
        return 1.0
    else:
        w = (1 - x) / args.cooldown_frac
        return w * 1.0 + (1 - w) * 0.1

# attention window size schedule: linearly increase
@lru_cache(1)
def get_window_size_blocks_helper(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)
def get_window_size_blocks(step: int):
    x = step / args.num_iterations # progress in training
    assert 0 <= x <= 1
    # Linearly increase the block-wise sliding window size over training 128 -> 1792
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * x, n=128)
    return get_window_size_blocks_helper(window_size)

model: nn.Module = torch.compile(model, dynamic=False)

########################################
#     Warmup kernel and Layerwise Bucketing   #
########################################

# Track the gradient readiness order during warmup
gradient_ready_order = []

def record_grad_order_hook(param):
    def hook(*_):
        if param.grad is not None and all(param is not p for p in gradient_ready_order):
            gradient_ready_order.append(param)
    return hook

# Register temp hooks to track when each parameter's gradient is ready
tmp_handles = []
for p in model.parameters():
    if p.requires_grad:
        handle = p.register_post_accumulate_grad_hook(record_grad_order_hook(p))
        tmp_handles.append(handle)

warmup_steps = 10
initial_state = dict(
    model=copy.deepcopy(model.state_dict()),
    optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]
)

for step in range(warmup_steps):
    # Create a local generator with fixed, step-dependent seed
    g = torch.Generator(device="cuda")
    g.manual_seed(1337 + step)

    # Use this generator to sample inputs
    inputs = targets = torch.randint(
        low=0,
        high=args.vocab_size,
        size=(args.train_seq_len,),
        device="cuda",
        dtype=torch.long,
        generator=g,  # <- Local generator ensures deterministic and isolated RNG
    )
    model(inputs.to(torch.int32), targets, get_window_size_blocks(0)).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)

# Remove hooks after warmup
for h in tmp_handles:
    h.remove()

# Restore model/optimizer state to ensure a clean start
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del initial_state

print0(f"Collected {len(gradient_ready_order)} parameters in gradient-ready order")

########################################
#      Overlap Communication Setup     #
########################################

# Layer-aware bucketing using gradient-ready order
def create_buckets_layerwise(ordered_params, bucket_size_mb=25):
    """Group parameters into buckets of ~bucket_size_mb MB, in gradient ready order"""
    buckets = []
    current_bucket = []
    current_size = 0

    for param in ordered_params:
        if not param.requires_grad:
            continue
        param_size_mb = param.numel() * param.element_size() / (1024 * 1024)

        if current_size + param_size_mb > bucket_size_mb and current_bucket:
            buckets.append(current_bucket)
            current_bucket = [param]
            current_size = param_size_mb
        else:
            current_bucket.append(param)
            current_size += param_size_mb

    if current_bucket:
        buckets.append(current_bucket)

    return buckets

# Create buckets for all parameters
all_params = [p for p in model.parameters() if p.requires_grad]
param_buckets = create_buckets_layerwise(gradient_ready_order)
print0(f"Created {len(param_buckets)} gradient buckets")
for i, bucket in enumerate(param_buckets):
    total_size = sum(p.numel() * p.element_size() for p in bucket) / (1024 * 1024)
    print0(f"Bucket {i}: {len(bucket)} params, {total_size:.1f} MB")

# Bucket state tracking
bucket_ready_count = [0] * len(param_buckets)
bucket_handles = [None] * len(param_buckets)
param_to_bucket = {}

# Map each parameter to its bucket index
for bucket_idx, bucket in enumerate(param_buckets):
    for param in bucket:
        param_to_bucket[param] = bucket_idx

def _gradient_hook(param: Tensor):
    """Called when a parameter's gradient is ready"""
    if param.grad is None:
        return

    bucket_idx = param_to_bucket[param]
    bucket_ready_count[bucket_idx] += 1

    # Check if all parameters in this bucket are ready
    if bucket_ready_count[bucket_idx] == len(param_buckets[bucket_idx]):
        # All-reduce this bucket
        bucket_grads = [p.grad for p in param_buckets[bucket_idx]]

        # For multi-tensor operations, we can reduce them together
        if len(bucket_grads) == 1:
            handle = dist.all_reduce(bucket_grads[0], op=dist.ReduceOp.AVG, async_op=True)
        else:
            # Use multi-tensor all-reduce for efficiency
            handle = dist.all_reduce_coalesced(bucket_grads, op=dist.ReduceOp.AVG, async_op=True)

        bucket_handles[bucket_idx] = handle

# Register hooks for all parameters
print0("Registering bucketed gradient hooks...")
for param in all_params:
    param.register_post_accumulate_grad_hook(_gradient_hook)

def wait_for_gradients():
    """Wait for all gradient reductions to complete and reset bucket state"""
    for handle in bucket_handles:
        if handle is not None:
            handle.wait()

    # Reset state for next iteration
    for i in range(len(bucket_ready_count)):
        bucket_ready_count[i] = 0
        bucket_handles[i] = None

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, world_size * args.train_seq_len, rank, world_size)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_batch_size = world_size * args.val_seq_len
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        val_loader = distributed_data_generator(args.val_files, val_batch_size, rank, world_size)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets = next(val_loader)
                val_loss += model(inputs, targets, get_window_size_blocks(step))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs_bucket/{run_id}", exist_ok=True)
            torch.save(log, f"logs_bucket/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    inputs, targets = next(train_loader)
    model(inputs, targets, get_window_size_blocks(step)).backward()
    #for param in model.parameters():
    #    dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
    wait_for_gradients() # does the same thing as commented two lines above, but faster

    # set optimization hyperparameters
    for opt in optimizers:
        for group in opt.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)
    for group in optimizer2.param_groups:
        frac = min(step / 300, 1) # momentum warmup for muon
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers
    for opt in optimizers:
        opt.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0]
Running PyTorch 2.8.0.dev20250612+cu126 compiled for CUDA 12.6
Fri Jun 13 06:39:54 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   38C    P0            119W /  700W |    5824MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   30C    P0            111W /  700W |    1514MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   29C    P0            113W /  700W |    1514MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   38C    P0            122W /  700W |    1514MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   38C    P0            117W /  700W |    1514MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   30C    P0            111W /  700W |    1514MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   37C    P0            119W /  700W |    1514MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   30C    P0            115W /  700W |    1514MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
Collected 75 parameters in gradient-ready order
Created 21 gradient buckets
Bucket 0: 1 params, 73.6 MB
Bucket 1: 1 params, 73.6 MB
Bucket 2: 4 params, 15.8 MB
Bucket 3: 1 params, 147.4 MB
Bucket 4: 4 params, 20.3 MB
Bucket 5: 4 params, 24.8 MB
Bucket 6: 4 params, 18.0 MB
Bucket 7: 5 params, 18.0 MB
Bucket 8: 4 params, 20.3 MB
Bucket 9: 5 params, 24.8 MB
Bucket 10: 4 params, 20.3 MB
Bucket 11: 4 params, 24.8 MB
Bucket 12: 5 params, 18.0 MB
Bucket 13: 5 params, 20.3 MB
Bucket 14: 5 params, 18.0 MB
Bucket 15: 4 params, 20.3 MB
Bucket 16: 5 params, 24.8 MB
Bucket 17: 4 params, 24.8 MB
Bucket 18: 4 params, 2.3 MB
Bucket 19: 1 params, 73.6 MB
Bucket 20: 1 params, 73.6 MB
Registering bucketed gradient hooks...
step:0/1770 val_loss:10.8258 train_time:0ms step_avg:0.02ms
step:1/1770 train_time:59ms step_avg:59.48ms
step:2/1770 train_time:138ms step_avg:68.90ms
step:3/1770 train_time:226ms step_avg:75.37ms
step:4/1770 train_time:320ms step_avg:79.97ms
step:5/1770 train_time:414ms step_avg:82.79ms
step:6/1770 train_time:509ms step_avg:84.78ms
step:7/1770 train_time:603ms step_avg:86.16ms
step:8/1770 train_time:697ms step_avg:87.16ms
step:9/1770 train_time:791ms step_avg:87.92ms
step:10/1770 train_time:885ms step_avg:88.54ms
step:11/1770 train_time:981ms step_avg:89.17ms
step:12/1770 train_time:1078ms step_avg:89.80ms
step:13/1770 train_time:1174ms step_avg:90.34ms
step:14/1770 train_time:1270ms step_avg:90.74ms
step:15/1770 train_time:1364ms step_avg:90.95ms
step:16/1770 train_time:1459ms step_avg:91.19ms
step:17/1770 train_time:1553ms step_avg:91.37ms
step:18/1770 train_time:1648ms step_avg:91.54ms
step:19/1770 train_time:1743ms step_avg:91.73ms
step:20/1770 train_time:1837ms step_avg:91.87ms
step:21/1770 train_time:1931ms step_avg:91.97ms
step:22/1770 train_time:2026ms step_avg:92.08ms
step:23/1770 train_time:2121ms step_avg:92.23ms
step:24/1770 train_time:2217ms step_avg:92.36ms
step:25/1770 train_time:2311ms step_avg:92.45ms
step:26/1770 train_time:2406ms step_avg:92.52ms
step:27/1770 train_time:2501ms step_avg:92.62ms
step:28/1770 train_time:2595ms step_avg:92.69ms
step:29/1770 train_time:2689ms step_avg:92.73ms
step:30/1770 train_time:2783ms step_avg:92.78ms
step:31/1770 train_time:2877ms step_avg:92.82ms
step:32/1770 train_time:2972ms step_avg:92.87ms
step:33/1770 train_time:3066ms step_avg:92.90ms
step:34/1770 train_time:3161ms step_avg:92.98ms
step:35/1770 train_time:3258ms step_avg:93.08ms
step:36/1770 train_time:3353ms step_avg:93.13ms
step:37/1770 train_time:3447ms step_avg:93.18ms
step:38/1770 train_time:3542ms step_avg:93.22ms
step:39/1770 train_time:3637ms step_avg:93.26ms
step:40/1770 train_time:3732ms step_avg:93.29ms
step:41/1770 train_time:3826ms step_avg:93.31ms
step:42/1770 train_time:3921ms step_avg:93.36ms
step:43/1770 train_time:4015ms step_avg:93.38ms
step:44/1770 train_time:4109ms step_avg:93.39ms
step:45/1770 train_time:4204ms step_avg:93.43ms
step:46/1770 train_time:4300ms step_avg:93.48ms
step:47/1770 train_time:4395ms step_avg:93.52ms
step:48/1770 train_time:4490ms step_avg:93.54ms
step:49/1770 train_time:4584ms step_avg:93.56ms
step:50/1770 train_time:4680ms step_avg:93.59ms
step:51/1770 train_time:4774ms step_avg:93.61ms
step:52/1770 train_time:4868ms step_avg:93.61ms
step:53/1770 train_time:4963ms step_avg:93.64ms
step:54/1770 train_time:5058ms step_avg:93.66ms
step:55/1770 train_time:5152ms step_avg:93.67ms
step:56/1770 train_time:5246ms step_avg:93.68ms
step:57/1770 train_time:5341ms step_avg:93.71ms
step:58/1770 train_time:5436ms step_avg:93.72ms
step:59/1770 train_time:5530ms step_avg:93.73ms
step:60/1770 train_time:5625ms step_avg:93.75ms
step:61/1770 train_time:5720ms step_avg:93.77ms
step:62/1770 train_time:5815ms step_avg:93.78ms
step:63/1770 train_time:5909ms step_avg:93.79ms
step:64/1770 train_time:6004ms step_avg:93.81ms
step:65/1770 train_time:6100ms step_avg:93.85ms
step:66/1770 train_time:6194ms step_avg:93.85ms
step:67/1770 train_time:6288ms step_avg:93.85ms
step:68/1770 train_time:6383ms step_avg:93.87ms
step:69/1770 train_time:6479ms step_avg:93.89ms
step:70/1770 train_time:6573ms step_avg:93.90ms
step:71/1770 train_time:6668ms step_avg:93.91ms
step:72/1770 train_time:6762ms step_avg:93.92ms
step:73/1770 train_time:6857ms step_avg:93.93ms
step:74/1770 train_time:6952ms step_avg:93.94ms
step:75/1770 train_time:7046ms step_avg:93.95ms
step:76/1770 train_time:7142ms step_avg:93.97ms
step:77/1770 train_time:7237ms step_avg:93.99ms
step:78/1770 train_time:7331ms step_avg:93.99ms
step:79/1770 train_time:7426ms step_avg:94.00ms
step:80/1770 train_time:7521ms step_avg:94.02ms
step:81/1770 train_time:7616ms step_avg:94.03ms
step:82/1770 train_time:7710ms step_avg:94.03ms
step:83/1770 train_time:7805ms step_avg:94.04ms
step:84/1770 train_time:7900ms step_avg:94.05ms
step:85/1770 train_time:7995ms step_avg:94.06ms
step:86/1770 train_time:8089ms step_avg:94.06ms
step:87/1770 train_time:8184ms step_avg:94.07ms
step:88/1770 train_time:8280ms step_avg:94.09ms
step:89/1770 train_time:8374ms step_avg:94.09ms
step:90/1770 train_time:8469ms step_avg:94.10ms
step:91/1770 train_time:8564ms step_avg:94.11ms
step:92/1770 train_time:8659ms step_avg:94.12ms
step:93/1770 train_time:8753ms step_avg:94.12ms
step:94/1770 train_time:8847ms step_avg:94.12ms
step:95/1770 train_time:8942ms step_avg:94.13ms
step:96/1770 train_time:9037ms step_avg:94.14ms
step:97/1770 train_time:9131ms step_avg:94.14ms
step:98/1770 train_time:9225ms step_avg:94.14ms
step:99/1770 train_time:9320ms step_avg:94.14ms
step:100/1770 train_time:9415ms step_avg:94.15ms
step:101/1770 train_time:9509ms step_avg:94.15ms
step:102/1770 train_time:9603ms step_avg:94.15ms
step:103/1770 train_time:9698ms step_avg:94.15ms
step:104/1770 train_time:9792ms step_avg:94.15ms
step:105/1770 train_time:9886ms step_avg:94.16ms
step:106/1770 train_time:9983ms step_avg:94.18ms
step:107/1770 train_time:10078ms step_avg:94.19ms
step:108/1770 train_time:10173ms step_avg:94.19ms
step:109/1770 train_time:10267ms step_avg:94.19ms
step:110/1770 train_time:10362ms step_avg:94.20ms
step:111/1770 train_time:10457ms step_avg:94.21ms
step:112/1770 train_time:10552ms step_avg:94.21ms
step:113/1770 train_time:10647ms step_avg:94.22ms
step:114/1770 train_time:10742ms step_avg:94.23ms
step:115/1770 train_time:10836ms step_avg:94.23ms
step:116/1770 train_time:10931ms step_avg:94.23ms
step:117/1770 train_time:11025ms step_avg:94.23ms
step:118/1770 train_time:11120ms step_avg:94.24ms
step:119/1770 train_time:11215ms step_avg:94.24ms
step:120/1770 train_time:11310ms step_avg:94.25ms
step:121/1770 train_time:11404ms step_avg:94.25ms
step:122/1770 train_time:11499ms step_avg:94.25ms
step:123/1770 train_time:11594ms step_avg:94.26ms
step:124/1770 train_time:11688ms step_avg:94.26ms
step:125/1770 train_time:11783ms step_avg:94.26ms
step:125/1770 val_loss:4.6500 train_time:11878ms step_avg:95.02ms
step:126/1770 train_time:11895ms step_avg:94.41ms
step:127/1770 train_time:11981ms step_avg:94.34ms
step:128/1770 train_time:12081ms step_avg:94.38ms
step:129/1770 train_time:12177ms step_avg:94.39ms
step:130/1770 train_time:12271ms step_avg:94.39ms
step:131/1770 train_time:12365ms step_avg:94.39ms
step:132/1770 train_time:12459ms step_avg:94.39ms
step:133/1770 train_time:12553ms step_avg:94.38ms
step:134/1770 train_time:12648ms step_avg:94.38ms
step:135/1770 train_time:12742ms step_avg:94.39ms
step:136/1770 train_time:12837ms step_avg:94.39ms
step:137/1770 train_time:12933ms step_avg:94.40ms
step:138/1770 train_time:13029ms step_avg:94.41ms
step:139/1770 train_time:13127ms step_avg:94.44ms
step:140/1770 train_time:13223ms step_avg:94.45ms
step:141/1770 train_time:13318ms step_avg:94.46ms
step:142/1770 train_time:13413ms step_avg:94.46ms
step:143/1770 train_time:13507ms step_avg:94.46ms
step:144/1770 train_time:13602ms step_avg:94.46ms
step:145/1770 train_time:13697ms step_avg:94.46ms
step:146/1770 train_time:13791ms step_avg:94.46ms
step:147/1770 train_time:13886ms step_avg:94.47ms
step:148/1770 train_time:13982ms step_avg:94.47ms
step:149/1770 train_time:14079ms step_avg:94.49ms
step:150/1770 train_time:14174ms step_avg:94.50ms
step:151/1770 train_time:14270ms step_avg:94.50ms
step:152/1770 train_time:14367ms step_avg:94.52ms
step:153/1770 train_time:14462ms step_avg:94.53ms
step:154/1770 train_time:14557ms step_avg:94.52ms
step:155/1770 train_time:14651ms step_avg:94.52ms
step:156/1770 train_time:14746ms step_avg:94.53ms
step:157/1770 train_time:14841ms step_avg:94.53ms
step:158/1770 train_time:14936ms step_avg:94.53ms
step:159/1770 train_time:15031ms step_avg:94.54ms
step:160/1770 train_time:15127ms step_avg:94.54ms
step:161/1770 train_time:15223ms step_avg:94.55ms
step:162/1770 train_time:15319ms step_avg:94.56ms
step:163/1770 train_time:15414ms step_avg:94.57ms
step:164/1770 train_time:15510ms step_avg:94.57ms
step:165/1770 train_time:15605ms step_avg:94.58ms
step:166/1770 train_time:15700ms step_avg:94.58ms
step:167/1770 train_time:15795ms step_avg:94.58ms
step:168/1770 train_time:15890ms step_avg:94.58ms
step:169/1770 train_time:15986ms step_avg:94.59ms
step:170/1770 train_time:16080ms step_avg:94.59ms
step:171/1770 train_time:16176ms step_avg:94.59ms
step:172/1770 train_time:16271ms step_avg:94.60ms
step:173/1770 train_time:16366ms step_avg:94.60ms
step:174/1770 train_time:16462ms step_avg:94.61ms
step:175/1770 train_time:16557ms step_avg:94.61ms
step:176/1770 train_time:16651ms step_avg:94.61ms
step:177/1770 train_time:16747ms step_avg:94.61ms
step:178/1770 train_time:16842ms step_avg:94.62ms
step:179/1770 train_time:16938ms step_avg:94.62ms
step:180/1770 train_time:17033ms step_avg:94.63ms
step:181/1770 train_time:17129ms step_avg:94.64ms
step:182/1770 train_time:17225ms step_avg:94.64ms
step:183/1770 train_time:17321ms step_avg:94.65ms
step:184/1770 train_time:17416ms step_avg:94.65ms
step:185/1770 train_time:17511ms step_avg:94.65ms
step:186/1770 train_time:17607ms step_avg:94.66ms
step:187/1770 train_time:17702ms step_avg:94.66ms
step:188/1770 train_time:17797ms step_avg:94.66ms
step:189/1770 train_time:17893ms step_avg:94.67ms
step:190/1770 train_time:17989ms step_avg:94.68ms
step:191/1770 train_time:18084ms step_avg:94.68ms
step:192/1770 train_time:18179ms step_avg:94.68ms
step:193/1770 train_time:18274ms step_avg:94.68ms
step:194/1770 train_time:18369ms step_avg:94.69ms
step:195/1770 train_time:18465ms step_avg:94.69ms
step:196/1770 train_time:18560ms step_avg:94.70ms
step:197/1770 train_time:18656ms step_avg:94.70ms
step:198/1770 train_time:18751ms step_avg:94.70ms
step:199/1770 train_time:18846ms step_avg:94.70ms
step:200/1770 train_time:18942ms step_avg:94.71ms
step:201/1770 train_time:19037ms step_avg:94.71ms
step:202/1770 train_time:19132ms step_avg:94.71ms
step:203/1770 train_time:19229ms step_avg:94.72ms
step:204/1770 train_time:19324ms step_avg:94.73ms
step:205/1770 train_time:19419ms step_avg:94.73ms
step:206/1770 train_time:19514ms step_avg:94.73ms
step:207/1770 train_time:19610ms step_avg:94.73ms
step:208/1770 train_time:19706ms step_avg:94.74ms
step:209/1770 train_time:19801ms step_avg:94.74ms
step:210/1770 train_time:19897ms step_avg:94.75ms
step:211/1770 train_time:19992ms step_avg:94.75ms
step:212/1770 train_time:20088ms step_avg:94.76ms
step:213/1770 train_time:20184ms step_avg:94.76ms
step:214/1770 train_time:20280ms step_avg:94.77ms
step:215/1770 train_time:20375ms step_avg:94.77ms
step:216/1770 train_time:20470ms step_avg:94.77ms
step:217/1770 train_time:20566ms step_avg:94.77ms
step:218/1770 train_time:20661ms step_avg:94.78ms
step:219/1770 train_time:20756ms step_avg:94.78ms
step:220/1770 train_time:20851ms step_avg:94.78ms
step:221/1770 train_time:20947ms step_avg:94.78ms
step:222/1770 train_time:21043ms step_avg:94.79ms
step:223/1770 train_time:21138ms step_avg:94.79ms
step:224/1770 train_time:21233ms step_avg:94.79ms
step:225/1770 train_time:21330ms step_avg:94.80ms
step:226/1770 train_time:21425ms step_avg:94.80ms
step:227/1770 train_time:21520ms step_avg:94.80ms
step:228/1770 train_time:21615ms step_avg:94.80ms
step:229/1770 train_time:21710ms step_avg:94.80ms
step:230/1770 train_time:21806ms step_avg:94.81ms
step:231/1770 train_time:21901ms step_avg:94.81ms
step:232/1770 train_time:21996ms step_avg:94.81ms
step:233/1770 train_time:22091ms step_avg:94.81ms
step:234/1770 train_time:22186ms step_avg:94.81ms
step:235/1770 train_time:22282ms step_avg:94.82ms
step:236/1770 train_time:22377ms step_avg:94.82ms
step:237/1770 train_time:22472ms step_avg:94.82ms
step:238/1770 train_time:22568ms step_avg:94.82ms
step:239/1770 train_time:22663ms step_avg:94.83ms
step:240/1770 train_time:22759ms step_avg:94.83ms
step:241/1770 train_time:22854ms step_avg:94.83ms
step:242/1770 train_time:22950ms step_avg:94.83ms
step:243/1770 train_time:23045ms step_avg:94.84ms
step:244/1770 train_time:23141ms step_avg:94.84ms
step:245/1770 train_time:23236ms step_avg:94.84ms
step:246/1770 train_time:23330ms step_avg:94.84ms
step:247/1770 train_time:23426ms step_avg:94.84ms
step:248/1770 train_time:23522ms step_avg:94.85ms
step:249/1770 train_time:23616ms step_avg:94.85ms
step:250/1770 train_time:23711ms step_avg:94.85ms
step:250/1770 val_loss:4.1088 train_time:23807ms step_avg:95.23ms
step:251/1770 train_time:23824ms step_avg:94.92ms
step:252/1770 train_time:23912ms step_avg:94.89ms
step:253/1770 train_time:24012ms step_avg:94.91ms
step:254/1770 train_time:24107ms step_avg:94.91ms
step:255/1770 train_time:24202ms step_avg:94.91ms
step:256/1770 train_time:24296ms step_avg:94.91ms
step:257/1770 train_time:24391ms step_avg:94.91ms
step:258/1770 train_time:24485ms step_avg:94.90ms
step:259/1770 train_time:24580ms step_avg:94.90ms
step:260/1770 train_time:24675ms step_avg:94.90ms
step:261/1770 train_time:24770ms step_avg:94.90ms
step:262/1770 train_time:24865ms step_avg:94.91ms
step:263/1770 train_time:24963ms step_avg:94.92ms
step:264/1770 train_time:25060ms step_avg:94.93ms
step:265/1770 train_time:25157ms step_avg:94.93ms
step:266/1770 train_time:25252ms step_avg:94.93ms
step:267/1770 train_time:25348ms step_avg:94.93ms
step:268/1770 train_time:25443ms step_avg:94.94ms
step:269/1770 train_time:25539ms step_avg:94.94ms
step:270/1770 train_time:25634ms step_avg:94.94ms
step:271/1770 train_time:25729ms step_avg:94.94ms
step:272/1770 train_time:25825ms step_avg:94.94ms
step:273/1770 train_time:25921ms step_avg:94.95ms
step:274/1770 train_time:26018ms step_avg:94.96ms
step:275/1770 train_time:26114ms step_avg:94.96ms
step:276/1770 train_time:26210ms step_avg:94.96ms
step:277/1770 train_time:26306ms step_avg:94.97ms
step:278/1770 train_time:26402ms step_avg:94.97ms
step:279/1770 train_time:26497ms step_avg:94.97ms
step:280/1770 train_time:26593ms step_avg:94.98ms
step:281/1770 train_time:26689ms step_avg:94.98ms
step:282/1770 train_time:26784ms step_avg:94.98ms
step:283/1770 train_time:26880ms step_avg:94.98ms
step:284/1770 train_time:26977ms step_avg:94.99ms
step:285/1770 train_time:27073ms step_avg:94.99ms
step:286/1770 train_time:27169ms step_avg:94.99ms
step:287/1770 train_time:27265ms step_avg:95.00ms
step:288/1770 train_time:27361ms step_avg:95.00ms
step:289/1770 train_time:27457ms step_avg:95.01ms
step:290/1770 train_time:27552ms step_avg:95.01ms
step:291/1770 train_time:27648ms step_avg:95.01ms
step:292/1770 train_time:27744ms step_avg:95.01ms
step:293/1770 train_time:27839ms step_avg:95.01ms
step:294/1770 train_time:27935ms step_avg:95.02ms
step:295/1770 train_time:28031ms step_avg:95.02ms
step:296/1770 train_time:28126ms step_avg:95.02ms
step:297/1770 train_time:28221ms step_avg:95.02ms
step:298/1770 train_time:28318ms step_avg:95.03ms
step:299/1770 train_time:28415ms step_avg:95.03ms
step:300/1770 train_time:28510ms step_avg:95.03ms
step:301/1770 train_time:28606ms step_avg:95.04ms
step:302/1770 train_time:28702ms step_avg:95.04ms
step:303/1770 train_time:28797ms step_avg:95.04ms
step:304/1770 train_time:28894ms step_avg:95.04ms
step:305/1770 train_time:28990ms step_avg:95.05ms
step:306/1770 train_time:29085ms step_avg:95.05ms
step:307/1770 train_time:29181ms step_avg:95.05ms
step:308/1770 train_time:29277ms step_avg:95.06ms
step:309/1770 train_time:29373ms step_avg:95.06ms
step:310/1770 train_time:29468ms step_avg:95.06ms
step:311/1770 train_time:29563ms step_avg:95.06ms
step:312/1770 train_time:29659ms step_avg:95.06ms
step:313/1770 train_time:29754ms step_avg:95.06ms
step:314/1770 train_time:29850ms step_avg:95.06ms
step:315/1770 train_time:29946ms step_avg:95.07ms
step:316/1770 train_time:30042ms step_avg:95.07ms
step:317/1770 train_time:30139ms step_avg:95.08ms
step:318/1770 train_time:30235ms step_avg:95.08ms
step:319/1770 train_time:30331ms step_avg:95.08ms
step:320/1770 train_time:30425ms step_avg:95.08ms
step:321/1770 train_time:30521ms step_avg:95.08ms
step:322/1770 train_time:30616ms step_avg:95.08ms
step:323/1770 train_time:30712ms step_avg:95.08ms
step:324/1770 train_time:30807ms step_avg:95.08ms
step:325/1770 train_time:30902ms step_avg:95.08ms
step:326/1770 train_time:30999ms step_avg:95.09ms
step:327/1770 train_time:31096ms step_avg:95.09ms
step:328/1770 train_time:31192ms step_avg:95.10ms
step:329/1770 train_time:31288ms step_avg:95.10ms
step:330/1770 train_time:31384ms step_avg:95.10ms
step:331/1770 train_time:31481ms step_avg:95.11ms
step:332/1770 train_time:31577ms step_avg:95.11ms
step:333/1770 train_time:31672ms step_avg:95.11ms
step:334/1770 train_time:31767ms step_avg:95.11ms
step:335/1770 train_time:31863ms step_avg:95.11ms
step:336/1770 train_time:31958ms step_avg:95.11ms
step:337/1770 train_time:32054ms step_avg:95.12ms
step:338/1770 train_time:32150ms step_avg:95.12ms
step:339/1770 train_time:32245ms step_avg:95.12ms
step:340/1770 train_time:32341ms step_avg:95.12ms
step:341/1770 train_time:32437ms step_avg:95.12ms
step:342/1770 train_time:32533ms step_avg:95.13ms
step:343/1770 train_time:32629ms step_avg:95.13ms
step:344/1770 train_time:32725ms step_avg:95.13ms
step:345/1770 train_time:32821ms step_avg:95.13ms
step:346/1770 train_time:32917ms step_avg:95.14ms
step:347/1770 train_time:33013ms step_avg:95.14ms
step:348/1770 train_time:33108ms step_avg:95.14ms
step:349/1770 train_time:33204ms step_avg:95.14ms
step:350/1770 train_time:33300ms step_avg:95.14ms
step:351/1770 train_time:33397ms step_avg:95.15ms
step:352/1770 train_time:33492ms step_avg:95.15ms
step:353/1770 train_time:33588ms step_avg:95.15ms
step:354/1770 train_time:33684ms step_avg:95.15ms
step:355/1770 train_time:33780ms step_avg:95.16ms
step:356/1770 train_time:33876ms step_avg:95.16ms
step:357/1770 train_time:33972ms step_avg:95.16ms
step:358/1770 train_time:34067ms step_avg:95.16ms
step:359/1770 train_time:34163ms step_avg:95.16ms
step:360/1770 train_time:34260ms step_avg:95.17ms
step:361/1770 train_time:34356ms step_avg:95.17ms
step:362/1770 train_time:34452ms step_avg:95.17ms
step:363/1770 train_time:34548ms step_avg:95.17ms
step:364/1770 train_time:34643ms step_avg:95.17ms
step:365/1770 train_time:34739ms step_avg:95.18ms
step:366/1770 train_time:34836ms step_avg:95.18ms
step:367/1770 train_time:34931ms step_avg:95.18ms
step:368/1770 train_time:35026ms step_avg:95.18ms
step:369/1770 train_time:35123ms step_avg:95.18ms
step:370/1770 train_time:35219ms step_avg:95.19ms
step:371/1770 train_time:35314ms step_avg:95.19ms
step:372/1770 train_time:35410ms step_avg:95.19ms
step:373/1770 train_time:35506ms step_avg:95.19ms
step:374/1770 train_time:35602ms step_avg:95.19ms
step:375/1770 train_time:35698ms step_avg:95.20ms
step:375/1770 val_loss:3.9031 train_time:35794ms step_avg:95.45ms
step:376/1770 train_time:35812ms step_avg:95.24ms
step:377/1770 train_time:35898ms step_avg:95.22ms
step:378/1770 train_time:35996ms step_avg:95.23ms
step:379/1770 train_time:36093ms step_avg:95.23ms
step:380/1770 train_time:36189ms step_avg:95.23ms
step:381/1770 train_time:36284ms step_avg:95.23ms
step:382/1770 train_time:36379ms step_avg:95.23ms
step:383/1770 train_time:36474ms step_avg:95.23ms
step:384/1770 train_time:36569ms step_avg:95.23ms
step:385/1770 train_time:36664ms step_avg:95.23ms
step:386/1770 train_time:36759ms step_avg:95.23ms
step:387/1770 train_time:36855ms step_avg:95.23ms
step:388/1770 train_time:36953ms step_avg:95.24ms
step:389/1770 train_time:37051ms step_avg:95.25ms
step:390/1770 train_time:37147ms step_avg:95.25ms
step:391/1770 train_time:37242ms step_avg:95.25ms
step:392/1770 train_time:37337ms step_avg:95.25ms
step:393/1770 train_time:37433ms step_avg:95.25ms
step:394/1770 train_time:37528ms step_avg:95.25ms
step:395/1770 train_time:37623ms step_avg:95.25ms
step:396/1770 train_time:37721ms step_avg:95.25ms
step:397/1770 train_time:37819ms step_avg:95.26ms
step:398/1770 train_time:37918ms step_avg:95.27ms
step:399/1770 train_time:38016ms step_avg:95.28ms
step:400/1770 train_time:38114ms step_avg:95.28ms
step:401/1770 train_time:38212ms step_avg:95.29ms
step:402/1770 train_time:38310ms step_avg:95.30ms
step:403/1770 train_time:38408ms step_avg:95.31ms
step:404/1770 train_time:38505ms step_avg:95.31ms
step:405/1770 train_time:38603ms step_avg:95.32ms
step:406/1770 train_time:38700ms step_avg:95.32ms
step:407/1770 train_time:38798ms step_avg:95.33ms
step:408/1770 train_time:38896ms step_avg:95.33ms
step:409/1770 train_time:38995ms step_avg:95.34ms
step:410/1770 train_time:39094ms step_avg:95.35ms
step:411/1770 train_time:39192ms step_avg:95.36ms
step:412/1770 train_time:39290ms step_avg:95.36ms
step:413/1770 train_time:39389ms step_avg:95.37ms
step:414/1770 train_time:39487ms step_avg:95.38ms
step:415/1770 train_time:39584ms step_avg:95.38ms
step:416/1770 train_time:39683ms step_avg:95.39ms
step:417/1770 train_time:39781ms step_avg:95.40ms
step:418/1770 train_time:39878ms step_avg:95.40ms
step:419/1770 train_time:39975ms step_avg:95.41ms
step:420/1770 train_time:40073ms step_avg:95.41ms
step:421/1770 train_time:40172ms step_avg:95.42ms
step:422/1770 train_time:40270ms step_avg:95.43ms
step:423/1770 train_time:40369ms step_avg:95.43ms
step:424/1770 train_time:40467ms step_avg:95.44ms
step:425/1770 train_time:40565ms step_avg:95.45ms
step:426/1770 train_time:40663ms step_avg:95.45ms
step:427/1770 train_time:40761ms step_avg:95.46ms
step:428/1770 train_time:40859ms step_avg:95.46ms
step:429/1770 train_time:40957ms step_avg:95.47ms
step:430/1770 train_time:41054ms step_avg:95.48ms
step:431/1770 train_time:41152ms step_avg:95.48ms
step:432/1770 train_time:41250ms step_avg:95.49ms
step:433/1770 train_time:41349ms step_avg:95.49ms
step:434/1770 train_time:41447ms step_avg:95.50ms
step:435/1770 train_time:41545ms step_avg:95.51ms
step:436/1770 train_time:41643ms step_avg:95.51ms
step:437/1770 train_time:41741ms step_avg:95.52ms
step:438/1770 train_time:41839ms step_avg:95.52ms
step:439/1770 train_time:41937ms step_avg:95.53ms
step:440/1770 train_time:42035ms step_avg:95.53ms
step:441/1770 train_time:42133ms step_avg:95.54ms
step:442/1770 train_time:42231ms step_avg:95.55ms
step:443/1770 train_time:42329ms step_avg:95.55ms
step:444/1770 train_time:42428ms step_avg:95.56ms
step:445/1770 train_time:42525ms step_avg:95.56ms
step:446/1770 train_time:42624ms step_avg:95.57ms
step:447/1770 train_time:42722ms step_avg:95.58ms
step:448/1770 train_time:42821ms step_avg:95.58ms
step:449/1770 train_time:42918ms step_avg:95.59ms
step:450/1770 train_time:43016ms step_avg:95.59ms
step:451/1770 train_time:43114ms step_avg:95.60ms
step:452/1770 train_time:43212ms step_avg:95.60ms
step:453/1770 train_time:43311ms step_avg:95.61ms
step:454/1770 train_time:43408ms step_avg:95.61ms
step:455/1770 train_time:43506ms step_avg:95.62ms
step:456/1770 train_time:43604ms step_avg:95.62ms
step:457/1770 train_time:43702ms step_avg:95.63ms
step:458/1770 train_time:43800ms step_avg:95.63ms
step:459/1770 train_time:43898ms step_avg:95.64ms
step:460/1770 train_time:43995ms step_avg:95.64ms
step:461/1770 train_time:44092ms step_avg:95.65ms
step:462/1770 train_time:44190ms step_avg:95.65ms
step:463/1770 train_time:44289ms step_avg:95.66ms
step:464/1770 train_time:44387ms step_avg:95.66ms
step:465/1770 train_time:44485ms step_avg:95.67ms
step:466/1770 train_time:44582ms step_avg:95.67ms
step:467/1770 train_time:44680ms step_avg:95.67ms
step:468/1770 train_time:44777ms step_avg:95.68ms
step:469/1770 train_time:44876ms step_avg:95.68ms
step:470/1770 train_time:44975ms step_avg:95.69ms
step:471/1770 train_time:45073ms step_avg:95.70ms
step:472/1770 train_time:45171ms step_avg:95.70ms
step:473/1770 train_time:45269ms step_avg:95.71ms
step:474/1770 train_time:45367ms step_avg:95.71ms
step:475/1770 train_time:45465ms step_avg:95.72ms
step:476/1770 train_time:45563ms step_avg:95.72ms
step:477/1770 train_time:45661ms step_avg:95.72ms
step:478/1770 train_time:45758ms step_avg:95.73ms
step:479/1770 train_time:45856ms step_avg:95.73ms
step:480/1770 train_time:45954ms step_avg:95.74ms
step:481/1770 train_time:46052ms step_avg:95.74ms
step:482/1770 train_time:46151ms step_avg:95.75ms
step:483/1770 train_time:46248ms step_avg:95.75ms
step:484/1770 train_time:46346ms step_avg:95.76ms
step:485/1770 train_time:46444ms step_avg:95.76ms
step:486/1770 train_time:46541ms step_avg:95.76ms
step:487/1770 train_time:46639ms step_avg:95.77ms
step:488/1770 train_time:46737ms step_avg:95.77ms
step:489/1770 train_time:46835ms step_avg:95.78ms
step:490/1770 train_time:46933ms step_avg:95.78ms
step:491/1770 train_time:47032ms step_avg:95.79ms
step:492/1770 train_time:47130ms step_avg:95.79ms
step:493/1770 train_time:47228ms step_avg:95.80ms
step:494/1770 train_time:47326ms step_avg:95.80ms
step:495/1770 train_time:47424ms step_avg:95.81ms
step:496/1770 train_time:47521ms step_avg:95.81ms
step:497/1770 train_time:47618ms step_avg:95.81ms
step:498/1770 train_time:47716ms step_avg:95.82ms
step:499/1770 train_time:47815ms step_avg:95.82ms
step:500/1770 train_time:47913ms step_avg:95.83ms
step:500/1770 val_loss:3.7525 train_time:48011ms step_avg:96.02ms
step:501/1770 train_time:48029ms step_avg:95.87ms
step:502/1770 train_time:48117ms step_avg:95.85ms
step:503/1770 train_time:48218ms step_avg:95.86ms
step:504/1770 train_time:48315ms step_avg:95.86ms
step:505/1770 train_time:48413ms step_avg:95.87ms
step:506/1770 train_time:48511ms step_avg:95.87ms
step:507/1770 train_time:48607ms step_avg:95.87ms
step:508/1770 train_time:48704ms step_avg:95.87ms
step:509/1770 train_time:48803ms step_avg:95.88ms
step:510/1770 train_time:48900ms step_avg:95.88ms
step:511/1770 train_time:48998ms step_avg:95.89ms
step:512/1770 train_time:49098ms step_avg:95.89ms
step:513/1770 train_time:49198ms step_avg:95.90ms
step:514/1770 train_time:49297ms step_avg:95.91ms
step:515/1770 train_time:49395ms step_avg:95.91ms
step:516/1770 train_time:49494ms step_avg:95.92ms
step:517/1770 train_time:49592ms step_avg:95.92ms
step:518/1770 train_time:49689ms step_avg:95.93ms
step:519/1770 train_time:49787ms step_avg:95.93ms
step:520/1770 train_time:49884ms step_avg:95.93ms
step:521/1770 train_time:49982ms step_avg:95.94ms
step:522/1770 train_time:50080ms step_avg:95.94ms
step:523/1770 train_time:50179ms step_avg:95.95ms
step:524/1770 train_time:50278ms step_avg:95.95ms
step:525/1770 train_time:50376ms step_avg:95.95ms
step:526/1770 train_time:50475ms step_avg:95.96ms
step:527/1770 train_time:50574ms step_avg:95.97ms
step:528/1770 train_time:50673ms step_avg:95.97ms
step:529/1770 train_time:50770ms step_avg:95.97ms
step:530/1770 train_time:50867ms step_avg:95.98ms
step:531/1770 train_time:50965ms step_avg:95.98ms
step:532/1770 train_time:51063ms step_avg:95.98ms
step:533/1770 train_time:51162ms step_avg:95.99ms
step:534/1770 train_time:51261ms step_avg:95.99ms
step:535/1770 train_time:51359ms step_avg:96.00ms
step:536/1770 train_time:51459ms step_avg:96.01ms
step:537/1770 train_time:51558ms step_avg:96.01ms
step:538/1770 train_time:51657ms step_avg:96.02ms
step:539/1770 train_time:51756ms step_avg:96.02ms
step:540/1770 train_time:51855ms step_avg:96.03ms
step:541/1770 train_time:51952ms step_avg:96.03ms
step:542/1770 train_time:52050ms step_avg:96.03ms
step:543/1770 train_time:52149ms step_avg:96.04ms
step:544/1770 train_time:52248ms step_avg:96.04ms
step:545/1770 train_time:52346ms step_avg:96.05ms
step:546/1770 train_time:52444ms step_avg:96.05ms
step:547/1770 train_time:52542ms step_avg:96.05ms
step:548/1770 train_time:52641ms step_avg:96.06ms
step:549/1770 train_time:52741ms step_avg:96.07ms
step:550/1770 train_time:52842ms step_avg:96.08ms
step:551/1770 train_time:52941ms step_avg:96.08ms
step:552/1770 train_time:53040ms step_avg:96.09ms
step:553/1770 train_time:53139ms step_avg:96.09ms
step:554/1770 train_time:53237ms step_avg:96.10ms
step:555/1770 train_time:53336ms step_avg:96.10ms
step:556/1770 train_time:53434ms step_avg:96.10ms
step:557/1770 train_time:53532ms step_avg:96.11ms
step:558/1770 train_time:53630ms step_avg:96.11ms
step:559/1770 train_time:53729ms step_avg:96.12ms
step:560/1770 train_time:53827ms step_avg:96.12ms
step:561/1770 train_time:53925ms step_avg:96.12ms
step:562/1770 train_time:54024ms step_avg:96.13ms
step:563/1770 train_time:54123ms step_avg:96.13ms
step:564/1770 train_time:54222ms step_avg:96.14ms
step:565/1770 train_time:54321ms step_avg:96.14ms
step:566/1770 train_time:54420ms step_avg:96.15ms
step:567/1770 train_time:54519ms step_avg:96.15ms
step:568/1770 train_time:54618ms step_avg:96.16ms
step:569/1770 train_time:54717ms step_avg:96.16ms
step:570/1770 train_time:54816ms step_avg:96.17ms
step:571/1770 train_time:54915ms step_avg:96.17ms
step:572/1770 train_time:55013ms step_avg:96.18ms
step:573/1770 train_time:55110ms step_avg:96.18ms
step:574/1770 train_time:55208ms step_avg:96.18ms
step:575/1770 train_time:55306ms step_avg:96.18ms
step:576/1770 train_time:55405ms step_avg:96.19ms
step:577/1770 train_time:55505ms step_avg:96.20ms
step:578/1770 train_time:55605ms step_avg:96.20ms
step:579/1770 train_time:55704ms step_avg:96.21ms
step:580/1770 train_time:55803ms step_avg:96.21ms
step:581/1770 train_time:55902ms step_avg:96.22ms
step:582/1770 train_time:56001ms step_avg:96.22ms
step:583/1770 train_time:56101ms step_avg:96.23ms
step:584/1770 train_time:56200ms step_avg:96.23ms
step:585/1770 train_time:56298ms step_avg:96.24ms
step:586/1770 train_time:56397ms step_avg:96.24ms
step:587/1770 train_time:56496ms step_avg:96.25ms
step:588/1770 train_time:56595ms step_avg:96.25ms
step:589/1770 train_time:56693ms step_avg:96.25ms
step:590/1770 train_time:56792ms step_avg:96.26ms
step:591/1770 train_time:56890ms step_avg:96.26ms
step:592/1770 train_time:56988ms step_avg:96.26ms
step:593/1770 train_time:57086ms step_avg:96.27ms
step:594/1770 train_time:57185ms step_avg:96.27ms
step:595/1770 train_time:57283ms step_avg:96.27ms
step:596/1770 train_time:57382ms step_avg:96.28ms
step:597/1770 train_time:57480ms step_avg:96.28ms
step:598/1770 train_time:57579ms step_avg:96.29ms
step:599/1770 train_time:57678ms step_avg:96.29ms
step:600/1770 train_time:57777ms step_avg:96.30ms
step:601/1770 train_time:57876ms step_avg:96.30ms
step:602/1770 train_time:57975ms step_avg:96.30ms
step:603/1770 train_time:58073ms step_avg:96.31ms
step:604/1770 train_time:58171ms step_avg:96.31ms
step:605/1770 train_time:58269ms step_avg:96.31ms
step:606/1770 train_time:58367ms step_avg:96.32ms
step:607/1770 train_time:58465ms step_avg:96.32ms
step:608/1770 train_time:58564ms step_avg:96.32ms
step:609/1770 train_time:58662ms step_avg:96.33ms
step:610/1770 train_time:58762ms step_avg:96.33ms
step:611/1770 train_time:58862ms step_avg:96.34ms
step:612/1770 train_time:58961ms step_avg:96.34ms
step:613/1770 train_time:59061ms step_avg:96.35ms
step:614/1770 train_time:59160ms step_avg:96.35ms
step:615/1770 train_time:59259ms step_avg:96.36ms
step:616/1770 train_time:59358ms step_avg:96.36ms
step:617/1770 train_time:59457ms step_avg:96.36ms
step:618/1770 train_time:59555ms step_avg:96.37ms
step:619/1770 train_time:59653ms step_avg:96.37ms
step:620/1770 train_time:59751ms step_avg:96.37ms
step:621/1770 train_time:59849ms step_avg:96.38ms
step:622/1770 train_time:59947ms step_avg:96.38ms
step:623/1770 train_time:60046ms step_avg:96.38ms
step:624/1770 train_time:60144ms step_avg:96.39ms
step:625/1770 train_time:60244ms step_avg:96.39ms
step:625/1770 val_loss:3.6651 train_time:60343ms step_avg:96.55ms
step:626/1770 train_time:60360ms step_avg:96.42ms
step:627/1770 train_time:60452ms step_avg:96.41ms
step:628/1770 train_time:60555ms step_avg:96.42ms
step:629/1770 train_time:60653ms step_avg:96.43ms
step:630/1770 train_time:60751ms step_avg:96.43ms
step:631/1770 train_time:60849ms step_avg:96.43ms
step:632/1770 train_time:60946ms step_avg:96.43ms
step:633/1770 train_time:61045ms step_avg:96.44ms
step:634/1770 train_time:61142ms step_avg:96.44ms
step:635/1770 train_time:61240ms step_avg:96.44ms
step:636/1770 train_time:61338ms step_avg:96.44ms
step:637/1770 train_time:61438ms step_avg:96.45ms
step:638/1770 train_time:61539ms step_avg:96.46ms
step:639/1770 train_time:61638ms step_avg:96.46ms
step:640/1770 train_time:61736ms step_avg:96.46ms
step:641/1770 train_time:61834ms step_avg:96.47ms
step:642/1770 train_time:61933ms step_avg:96.47ms
step:643/1770 train_time:62031ms step_avg:96.47ms
step:644/1770 train_time:62129ms step_avg:96.47ms
step:645/1770 train_time:62227ms step_avg:96.48ms
step:646/1770 train_time:62325ms step_avg:96.48ms
step:647/1770 train_time:62423ms step_avg:96.48ms
step:648/1770 train_time:62522ms step_avg:96.49ms
step:649/1770 train_time:62622ms step_avg:96.49ms
step:650/1770 train_time:62722ms step_avg:96.50ms
step:651/1770 train_time:62821ms step_avg:96.50ms
step:652/1770 train_time:62920ms step_avg:96.50ms
step:653/1770 train_time:63019ms step_avg:96.51ms
step:654/1770 train_time:63118ms step_avg:96.51ms
step:655/1770 train_time:63217ms step_avg:96.51ms
step:656/1770 train_time:63316ms step_avg:96.52ms
step:657/1770 train_time:63414ms step_avg:96.52ms
step:658/1770 train_time:63515ms step_avg:96.53ms
step:659/1770 train_time:63616ms step_avg:96.53ms
step:660/1770 train_time:63717ms step_avg:96.54ms
step:661/1770 train_time:63818ms step_avg:96.55ms
step:662/1770 train_time:63918ms step_avg:96.55ms
step:663/1770 train_time:64020ms step_avg:96.56ms
step:664/1770 train_time:64120ms step_avg:96.57ms
step:665/1770 train_time:64221ms step_avg:96.57ms
step:666/1770 train_time:64322ms step_avg:96.58ms
step:667/1770 train_time:64423ms step_avg:96.59ms
step:668/1770 train_time:64524ms step_avg:96.59ms
step:669/1770 train_time:64625ms step_avg:96.60ms
step:670/1770 train_time:64725ms step_avg:96.60ms
step:671/1770 train_time:64825ms step_avg:96.61ms
step:672/1770 train_time:64925ms step_avg:96.62ms
step:673/1770 train_time:65026ms step_avg:96.62ms
step:674/1770 train_time:65125ms step_avg:96.63ms
step:675/1770 train_time:65226ms step_avg:96.63ms
step:676/1770 train_time:65326ms step_avg:96.64ms
step:677/1770 train_time:65426ms step_avg:96.64ms
step:678/1770 train_time:65526ms step_avg:96.65ms
step:679/1770 train_time:65626ms step_avg:96.65ms
step:680/1770 train_time:65725ms step_avg:96.65ms
step:681/1770 train_time:65825ms step_avg:96.66ms
step:682/1770 train_time:65925ms step_avg:96.66ms
step:683/1770 train_time:66025ms step_avg:96.67ms
step:684/1770 train_time:66126ms step_avg:96.68ms
step:685/1770 train_time:66226ms step_avg:96.68ms
step:686/1770 train_time:66326ms step_avg:96.69ms
step:687/1770 train_time:66426ms step_avg:96.69ms
step:688/1770 train_time:66526ms step_avg:96.69ms
step:689/1770 train_time:66626ms step_avg:96.70ms
step:690/1770 train_time:66726ms step_avg:96.70ms
step:691/1770 train_time:66825ms step_avg:96.71ms
step:692/1770 train_time:66926ms step_avg:96.71ms
step:693/1770 train_time:67027ms step_avg:96.72ms
step:694/1770 train_time:67127ms step_avg:96.72ms
step:695/1770 train_time:67227ms step_avg:96.73ms
step:696/1770 train_time:67327ms step_avg:96.73ms
step:697/1770 train_time:67427ms step_avg:96.74ms
step:698/1770 train_time:67527ms step_avg:96.74ms
step:699/1770 train_time:67626ms step_avg:96.75ms
step:700/1770 train_time:67726ms step_avg:96.75ms
step:701/1770 train_time:67826ms step_avg:96.76ms
step:702/1770 train_time:67926ms step_avg:96.76ms
step:703/1770 train_time:68026ms step_avg:96.77ms
step:704/1770 train_time:68126ms step_avg:96.77ms
step:705/1770 train_time:68227ms step_avg:96.78ms
step:706/1770 train_time:68326ms step_avg:96.78ms
step:707/1770 train_time:68426ms step_avg:96.78ms
step:708/1770 train_time:68526ms step_avg:96.79ms
step:709/1770 train_time:68626ms step_avg:96.79ms
step:710/1770 train_time:68726ms step_avg:96.80ms
step:711/1770 train_time:68825ms step_avg:96.80ms
step:712/1770 train_time:68925ms step_avg:96.81ms
step:713/1770 train_time:69025ms step_avg:96.81ms
step:714/1770 train_time:69125ms step_avg:96.81ms
step:715/1770 train_time:69226ms step_avg:96.82ms
step:716/1770 train_time:69327ms step_avg:96.83ms
step:717/1770 train_time:69427ms step_avg:96.83ms
step:718/1770 train_time:69528ms step_avg:96.84ms
step:719/1770 train_time:69627ms step_avg:96.84ms
step:720/1770 train_time:69727ms step_avg:96.84ms
step:721/1770 train_time:69827ms step_avg:96.85ms
step:722/1770 train_time:69926ms step_avg:96.85ms
step:723/1770 train_time:70026ms step_avg:96.86ms
step:724/1770 train_time:70126ms step_avg:96.86ms
step:725/1770 train_time:70226ms step_avg:96.86ms
step:726/1770 train_time:70327ms step_avg:96.87ms
step:727/1770 train_time:70427ms step_avg:96.87ms
step:728/1770 train_time:70527ms step_avg:96.88ms
step:729/1770 train_time:70627ms step_avg:96.88ms
step:730/1770 train_time:70727ms step_avg:96.89ms
step:731/1770 train_time:70826ms step_avg:96.89ms
step:732/1770 train_time:70926ms step_avg:96.89ms
step:733/1770 train_time:71026ms step_avg:96.90ms
step:734/1770 train_time:71127ms step_avg:96.90ms
step:735/1770 train_time:71226ms step_avg:96.91ms
step:736/1770 train_time:71326ms step_avg:96.91ms
step:737/1770 train_time:71426ms step_avg:96.91ms
step:738/1770 train_time:71526ms step_avg:96.92ms
step:739/1770 train_time:71626ms step_avg:96.92ms
step:740/1770 train_time:71726ms step_avg:96.93ms
step:741/1770 train_time:71826ms step_avg:96.93ms
step:742/1770 train_time:71926ms step_avg:96.94ms
step:743/1770 train_time:72027ms step_avg:96.94ms
step:744/1770 train_time:72126ms step_avg:96.94ms
step:745/1770 train_time:72227ms step_avg:96.95ms
step:746/1770 train_time:72327ms step_avg:96.95ms
step:747/1770 train_time:72426ms step_avg:96.96ms
step:748/1770 train_time:72526ms step_avg:96.96ms
step:749/1770 train_time:72626ms step_avg:96.96ms
step:750/1770 train_time:72726ms step_avg:96.97ms
step:750/1770 val_loss:3.6013 train_time:72825ms step_avg:97.10ms
step:751/1770 train_time:72843ms step_avg:96.99ms
step:752/1770 train_time:72934ms step_avg:96.99ms
step:753/1770 train_time:73036ms step_avg:96.99ms
step:754/1770 train_time:73136ms step_avg:97.00ms
step:755/1770 train_time:73236ms step_avg:97.00ms
step:756/1770 train_time:73336ms step_avg:97.00ms
step:757/1770 train_time:73435ms step_avg:97.01ms
step:758/1770 train_time:73534ms step_avg:97.01ms
step:759/1770 train_time:73633ms step_avg:97.01ms
step:760/1770 train_time:73732ms step_avg:97.02ms
step:761/1770 train_time:73834ms step_avg:97.02ms
step:762/1770 train_time:73936ms step_avg:97.03ms
step:763/1770 train_time:74038ms step_avg:97.04ms
step:764/1770 train_time:74139ms step_avg:97.04ms
step:765/1770 train_time:74240ms step_avg:97.05ms
step:766/1770 train_time:74339ms step_avg:97.05ms
step:767/1770 train_time:74440ms step_avg:97.05ms
step:768/1770 train_time:74540ms step_avg:97.06ms
step:769/1770 train_time:74641ms step_avg:97.06ms
step:770/1770 train_time:74742ms step_avg:97.07ms
step:771/1770 train_time:74845ms step_avg:97.07ms
step:772/1770 train_time:74946ms step_avg:97.08ms
step:773/1770 train_time:75047ms step_avg:97.09ms
step:774/1770 train_time:75148ms step_avg:97.09ms
step:775/1770 train_time:75248ms step_avg:97.09ms
step:776/1770 train_time:75348ms step_avg:97.10ms
step:777/1770 train_time:75448ms step_avg:97.10ms
step:778/1770 train_time:75547ms step_avg:97.10ms
step:779/1770 train_time:75647ms step_avg:97.11ms
step:780/1770 train_time:75747ms step_avg:97.11ms
step:781/1770 train_time:75848ms step_avg:97.12ms
step:782/1770 train_time:75947ms step_avg:97.12ms
step:783/1770 train_time:76047ms step_avg:97.12ms
step:784/1770 train_time:76147ms step_avg:97.13ms
step:785/1770 train_time:76247ms step_avg:97.13ms
step:786/1770 train_time:76348ms step_avg:97.13ms
step:787/1770 train_time:76448ms step_avg:97.14ms
step:788/1770 train_time:76547ms step_avg:97.14ms
step:789/1770 train_time:76648ms step_avg:97.15ms
step:790/1770 train_time:76748ms step_avg:97.15ms
step:791/1770 train_time:76848ms step_avg:97.15ms
step:792/1770 train_time:76948ms step_avg:97.16ms
step:793/1770 train_time:77048ms step_avg:97.16ms
step:794/1770 train_time:77148ms step_avg:97.16ms
step:795/1770 train_time:77249ms step_avg:97.17ms
step:796/1770 train_time:77348ms step_avg:97.17ms
step:797/1770 train_time:77449ms step_avg:97.18ms
step:798/1770 train_time:77549ms step_avg:97.18ms
step:799/1770 train_time:77649ms step_avg:97.18ms
step:800/1770 train_time:77748ms step_avg:97.19ms
step:801/1770 train_time:77849ms step_avg:97.19ms
step:802/1770 train_time:77949ms step_avg:97.19ms
step:803/1770 train_time:78049ms step_avg:97.20ms
step:804/1770 train_time:78150ms step_avg:97.20ms
step:805/1770 train_time:78250ms step_avg:97.20ms
step:806/1770 train_time:78350ms step_avg:97.21ms
step:807/1770 train_time:78451ms step_avg:97.21ms
step:808/1770 train_time:78551ms step_avg:97.22ms
step:809/1770 train_time:78651ms step_avg:97.22ms
step:810/1770 train_time:78751ms step_avg:97.22ms
step:811/1770 train_time:78851ms step_avg:97.23ms
step:812/1770 train_time:78952ms step_avg:97.23ms
step:813/1770 train_time:79052ms step_avg:97.24ms
step:814/1770 train_time:79152ms step_avg:97.24ms
step:815/1770 train_time:79252ms step_avg:97.24ms
step:816/1770 train_time:79352ms step_avg:97.25ms
step:817/1770 train_time:79452ms step_avg:97.25ms
step:818/1770 train_time:79552ms step_avg:97.25ms
step:819/1770 train_time:79652ms step_avg:97.25ms
step:820/1770 train_time:79752ms step_avg:97.26ms
step:821/1770 train_time:79851ms step_avg:97.26ms
step:822/1770 train_time:79951ms step_avg:97.26ms
step:823/1770 train_time:80052ms step_avg:97.27ms
step:824/1770 train_time:80151ms step_avg:97.27ms
step:825/1770 train_time:80251ms step_avg:97.27ms
step:826/1770 train_time:80352ms step_avg:97.28ms
step:827/1770 train_time:80452ms step_avg:97.28ms
step:828/1770 train_time:80552ms step_avg:97.28ms
step:829/1770 train_time:80652ms step_avg:97.29ms
step:830/1770 train_time:80752ms step_avg:97.29ms
step:831/1770 train_time:80853ms step_avg:97.30ms
step:832/1770 train_time:80953ms step_avg:97.30ms
step:833/1770 train_time:81053ms step_avg:97.30ms
step:834/1770 train_time:81154ms step_avg:97.31ms
step:835/1770 train_time:81254ms step_avg:97.31ms
step:836/1770 train_time:81355ms step_avg:97.31ms
step:837/1770 train_time:81455ms step_avg:97.32ms
step:838/1770 train_time:81555ms step_avg:97.32ms
step:839/1770 train_time:81656ms step_avg:97.33ms
step:840/1770 train_time:81756ms step_avg:97.33ms
step:841/1770 train_time:81857ms step_avg:97.33ms
step:842/1770 train_time:81958ms step_avg:97.34ms
step:843/1770 train_time:82060ms step_avg:97.34ms
step:844/1770 train_time:82162ms step_avg:97.35ms
step:845/1770 train_time:82263ms step_avg:97.35ms
step:846/1770 train_time:82364ms step_avg:97.36ms
step:847/1770 train_time:82465ms step_avg:97.36ms
step:848/1770 train_time:82566ms step_avg:97.37ms
step:849/1770 train_time:82666ms step_avg:97.37ms
step:850/1770 train_time:82766ms step_avg:97.37ms
step:851/1770 train_time:82866ms step_avg:97.37ms
step:852/1770 train_time:82967ms step_avg:97.38ms
step:853/1770 train_time:83067ms step_avg:97.38ms
step:854/1770 train_time:83168ms step_avg:97.39ms
step:855/1770 train_time:83269ms step_avg:97.39ms
step:856/1770 train_time:83370ms step_avg:97.39ms
step:857/1770 train_time:83470ms step_avg:97.40ms
step:858/1770 train_time:83571ms step_avg:97.40ms
step:859/1770 train_time:83671ms step_avg:97.40ms
step:860/1770 train_time:83771ms step_avg:97.41ms
step:861/1770 train_time:83871ms step_avg:97.41ms
step:862/1770 train_time:83972ms step_avg:97.42ms
step:863/1770 train_time:84072ms step_avg:97.42ms
step:864/1770 train_time:84173ms step_avg:97.42ms
step:865/1770 train_time:84273ms step_avg:97.43ms
step:866/1770 train_time:84374ms step_avg:97.43ms
step:867/1770 train_time:84475ms step_avg:97.43ms
step:868/1770 train_time:84575ms step_avg:97.44ms
step:869/1770 train_time:84675ms step_avg:97.44ms
step:870/1770 train_time:84776ms step_avg:97.44ms
step:871/1770 train_time:84877ms step_avg:97.45ms
step:872/1770 train_time:84978ms step_avg:97.45ms
step:873/1770 train_time:85079ms step_avg:97.46ms
step:874/1770 train_time:85180ms step_avg:97.46ms
step:875/1770 train_time:85282ms step_avg:97.47ms
step:875/1770 val_loss:3.5533 train_time:85384ms step_avg:97.58ms
step:876/1770 train_time:85403ms step_avg:97.49ms
step:877/1770 train_time:85493ms step_avg:97.48ms
step:878/1770 train_time:85596ms step_avg:97.49ms
step:879/1770 train_time:85697ms step_avg:97.49ms
step:880/1770 train_time:85797ms step_avg:97.50ms
step:881/1770 train_time:85897ms step_avg:97.50ms
step:882/1770 train_time:85997ms step_avg:97.50ms
step:883/1770 train_time:86096ms step_avg:97.50ms
step:884/1770 train_time:86196ms step_avg:97.51ms
step:885/1770 train_time:86296ms step_avg:97.51ms
step:886/1770 train_time:86398ms step_avg:97.51ms
step:887/1770 train_time:86500ms step_avg:97.52ms
step:888/1770 train_time:86602ms step_avg:97.52ms
step:889/1770 train_time:86704ms step_avg:97.53ms
step:890/1770 train_time:86804ms step_avg:97.53ms
step:891/1770 train_time:86905ms step_avg:97.54ms
step:892/1770 train_time:87005ms step_avg:97.54ms
step:893/1770 train_time:87105ms step_avg:97.54ms
step:894/1770 train_time:87206ms step_avg:97.55ms
step:895/1770 train_time:87306ms step_avg:97.55ms
step:896/1770 train_time:87407ms step_avg:97.55ms
step:897/1770 train_time:87508ms step_avg:97.56ms
step:898/1770 train_time:87609ms step_avg:97.56ms
step:899/1770 train_time:87709ms step_avg:97.56ms
step:900/1770 train_time:87809ms step_avg:97.57ms
step:901/1770 train_time:87909ms step_avg:97.57ms
step:902/1770 train_time:88010ms step_avg:97.57ms
step:903/1770 train_time:88110ms step_avg:97.58ms
step:904/1770 train_time:88210ms step_avg:97.58ms
step:905/1770 train_time:88311ms step_avg:97.58ms
step:906/1770 train_time:88411ms step_avg:97.58ms
step:907/1770 train_time:88512ms step_avg:97.59ms
step:908/1770 train_time:88612ms step_avg:97.59ms
step:909/1770 train_time:88712ms step_avg:97.59ms
step:910/1770 train_time:88812ms step_avg:97.60ms
step:911/1770 train_time:88912ms step_avg:97.60ms
step:912/1770 train_time:89012ms step_avg:97.60ms
step:913/1770 train_time:89113ms step_avg:97.60ms
step:914/1770 train_time:89214ms step_avg:97.61ms
step:915/1770 train_time:89315ms step_avg:97.61ms
step:916/1770 train_time:89416ms step_avg:97.62ms
step:917/1770 train_time:89516ms step_avg:97.62ms
step:918/1770 train_time:89617ms step_avg:97.62ms
step:919/1770 train_time:89719ms step_avg:97.63ms
step:920/1770 train_time:89822ms step_avg:97.63ms
step:921/1770 train_time:89924ms step_avg:97.64ms
step:922/1770 train_time:90026ms step_avg:97.64ms
step:923/1770 train_time:90129ms step_avg:97.65ms
step:924/1770 train_time:90230ms step_avg:97.65ms
step:925/1770 train_time:90333ms step_avg:97.66ms
step:926/1770 train_time:90434ms step_avg:97.66ms
step:927/1770 train_time:90536ms step_avg:97.67ms
step:928/1770 train_time:90639ms step_avg:97.67ms
step:929/1770 train_time:90740ms step_avg:97.68ms
step:930/1770 train_time:90843ms step_avg:97.68ms
step:931/1770 train_time:90945ms step_avg:97.69ms
step:932/1770 train_time:91048ms step_avg:97.69ms
step:933/1770 train_time:91150ms step_avg:97.70ms
step:934/1770 train_time:91252ms step_avg:97.70ms
step:935/1770 train_time:91353ms step_avg:97.70ms
step:936/1770 train_time:91454ms step_avg:97.71ms
step:937/1770 train_time:91555ms step_avg:97.71ms
step:938/1770 train_time:91659ms step_avg:97.72ms
step:939/1770 train_time:91760ms step_avg:97.72ms
step:940/1770 train_time:91863ms step_avg:97.73ms
step:941/1770 train_time:91966ms step_avg:97.73ms
step:942/1770 train_time:92069ms step_avg:97.74ms
step:943/1770 train_time:92171ms step_avg:97.74ms
step:944/1770 train_time:92272ms step_avg:97.75ms
step:945/1770 train_time:92373ms step_avg:97.75ms
step:946/1770 train_time:92474ms step_avg:97.75ms
step:947/1770 train_time:92575ms step_avg:97.76ms
step:948/1770 train_time:92677ms step_avg:97.76ms
step:949/1770 train_time:92781ms step_avg:97.77ms
step:950/1770 train_time:92884ms step_avg:97.77ms
step:951/1770 train_time:92988ms step_avg:97.78ms
step:952/1770 train_time:93089ms step_avg:97.78ms
step:953/1770 train_time:93190ms step_avg:97.79ms
step:954/1770 train_time:93292ms step_avg:97.79ms
step:955/1770 train_time:93393ms step_avg:97.79ms
step:956/1770 train_time:93494ms step_avg:97.80ms
step:957/1770 train_time:93596ms step_avg:97.80ms
step:958/1770 train_time:93697ms step_avg:97.81ms
step:959/1770 train_time:93799ms step_avg:97.81ms
step:960/1770 train_time:93902ms step_avg:97.81ms
step:961/1770 train_time:94005ms step_avg:97.82ms
step:962/1770 train_time:94107ms step_avg:97.82ms
step:963/1770 train_time:94210ms step_avg:97.83ms
step:964/1770 train_time:94312ms step_avg:97.83ms
step:965/1770 train_time:94413ms step_avg:97.84ms
step:966/1770 train_time:94514ms step_avg:97.84ms
step:967/1770 train_time:94615ms step_avg:97.84ms
step:968/1770 train_time:94717ms step_avg:97.85ms
step:969/1770 train_time:94819ms step_avg:97.85ms
step:970/1770 train_time:94922ms step_avg:97.86ms
step:971/1770 train_time:95024ms step_avg:97.86ms
step:972/1770 train_time:95127ms step_avg:97.87ms
step:973/1770 train_time:95229ms step_avg:97.87ms
step:974/1770 train_time:95331ms step_avg:97.88ms
step:975/1770 train_time:95433ms step_avg:97.88ms
step:976/1770 train_time:95534ms step_avg:97.88ms
step:977/1770 train_time:95636ms step_avg:97.89ms
step:978/1770 train_time:95738ms step_avg:97.89ms
step:979/1770 train_time:95840ms step_avg:97.90ms
step:980/1770 train_time:95942ms step_avg:97.90ms
step:981/1770 train_time:96044ms step_avg:97.90ms
step:982/1770 train_time:96147ms step_avg:97.91ms
step:983/1770 train_time:96250ms step_avg:97.91ms
step:984/1770 train_time:96352ms step_avg:97.92ms
step:985/1770 train_time:96453ms step_avg:97.92ms
step:986/1770 train_time:96554ms step_avg:97.92ms
step:987/1770 train_time:96655ms step_avg:97.93ms
step:988/1770 train_time:96756ms step_avg:97.93ms
step:989/1770 train_time:96858ms step_avg:97.94ms
step:990/1770 train_time:96960ms step_avg:97.94ms
step:991/1770 train_time:97064ms step_avg:97.95ms
step:992/1770 train_time:97167ms step_avg:97.95ms
step:993/1770 train_time:97270ms step_avg:97.96ms
step:994/1770 train_time:97372ms step_avg:97.96ms
step:995/1770 train_time:97474ms step_avg:97.96ms
step:996/1770 train_time:97575ms step_avg:97.97ms
step:997/1770 train_time:97676ms step_avg:97.97ms
step:998/1770 train_time:97778ms step_avg:97.97ms
step:999/1770 train_time:97880ms step_avg:97.98ms
step:1000/1770 train_time:97983ms step_avg:97.98ms
step:1000/1770 val_loss:3.5140 train_time:98084ms step_avg:98.08ms
step:1001/1770 train_time:98102ms step_avg:98.00ms
step:1002/1770 train_time:98192ms step_avg:98.00ms
step:1003/1770 train_time:98296ms step_avg:98.00ms
step:1004/1770 train_time:98400ms step_avg:98.01ms
step:1005/1770 train_time:98502ms step_avg:98.01ms
step:1006/1770 train_time:98604ms step_avg:98.02ms
step:1007/1770 train_time:98705ms step_avg:98.02ms
step:1008/1770 train_time:98806ms step_avg:98.02ms
step:1009/1770 train_time:98907ms step_avg:98.02ms
step:1010/1770 train_time:99008ms step_avg:98.03ms
step:1011/1770 train_time:99111ms step_avg:98.03ms
step:1012/1770 train_time:99213ms step_avg:98.04ms
step:1013/1770 train_time:99316ms step_avg:98.04ms
step:1014/1770 train_time:99418ms step_avg:98.05ms
step:1015/1770 train_time:99520ms step_avg:98.05ms
step:1016/1770 train_time:99623ms step_avg:98.05ms
step:1017/1770 train_time:99725ms step_avg:98.06ms
step:1018/1770 train_time:99825ms step_avg:98.06ms
step:1019/1770 train_time:99927ms step_avg:98.06ms
step:1020/1770 train_time:100028ms step_avg:98.07ms
step:1021/1770 train_time:100130ms step_avg:98.07ms
step:1022/1770 train_time:100231ms step_avg:98.07ms
step:1023/1770 train_time:100333ms step_avg:98.08ms
step:1024/1770 train_time:100436ms step_avg:98.08ms
step:1025/1770 train_time:100539ms step_avg:98.09ms
step:1026/1770 train_time:100641ms step_avg:98.09ms
step:1027/1770 train_time:100744ms step_avg:98.10ms
step:1028/1770 train_time:100846ms step_avg:98.10ms
step:1029/1770 train_time:100947ms step_avg:98.10ms
step:1030/1770 train_time:101048ms step_avg:98.11ms
step:1031/1770 train_time:101150ms step_avg:98.11ms
step:1032/1770 train_time:101251ms step_avg:98.11ms
step:1033/1770 train_time:101353ms step_avg:98.12ms
step:1034/1770 train_time:101456ms step_avg:98.12ms
step:1035/1770 train_time:101558ms step_avg:98.12ms
step:1036/1770 train_time:101660ms step_avg:98.13ms
step:1037/1770 train_time:101762ms step_avg:98.13ms
step:1038/1770 train_time:101865ms step_avg:98.14ms
step:1039/1770 train_time:101967ms step_avg:98.14ms
step:1040/1770 train_time:102069ms step_avg:98.14ms
step:1041/1770 train_time:102170ms step_avg:98.15ms
step:1042/1770 train_time:102271ms step_avg:98.15ms
step:1043/1770 train_time:102372ms step_avg:98.15ms
step:1044/1770 train_time:102473ms step_avg:98.15ms
step:1045/1770 train_time:102575ms step_avg:98.16ms
step:1046/1770 train_time:102677ms step_avg:98.16ms
step:1047/1770 train_time:102780ms step_avg:98.17ms
step:1048/1770 train_time:102884ms step_avg:98.17ms
step:1049/1770 train_time:102985ms step_avg:98.17ms
step:1050/1770 train_time:103087ms step_avg:98.18ms
step:1051/1770 train_time:103189ms step_avg:98.18ms
step:1052/1770 train_time:103290ms step_avg:98.18ms
step:1053/1770 train_time:103392ms step_avg:98.19ms
step:1054/1770 train_time:103493ms step_avg:98.19ms
step:1055/1770 train_time:103595ms step_avg:98.19ms
step:1056/1770 train_time:103698ms step_avg:98.20ms
step:1057/1770 train_time:103800ms step_avg:98.20ms
step:1058/1770 train_time:103903ms step_avg:98.21ms
step:1059/1770 train_time:104006ms step_avg:98.21ms
step:1060/1770 train_time:104108ms step_avg:98.22ms
step:1061/1770 train_time:104210ms step_avg:98.22ms
step:1062/1770 train_time:104311ms step_avg:98.22ms
step:1063/1770 train_time:104414ms step_avg:98.23ms
step:1064/1770 train_time:104516ms step_avg:98.23ms
step:1065/1770 train_time:104618ms step_avg:98.23ms
step:1066/1770 train_time:104721ms step_avg:98.24ms
step:1067/1770 train_time:104823ms step_avg:98.24ms
step:1068/1770 train_time:104927ms step_avg:98.25ms
step:1069/1770 train_time:105029ms step_avg:98.25ms
step:1070/1770 train_time:105132ms step_avg:98.25ms
step:1071/1770 train_time:105234ms step_avg:98.26ms
step:1072/1770 train_time:105336ms step_avg:98.26ms
step:1073/1770 train_time:105438ms step_avg:98.26ms
step:1074/1770 train_time:105541ms step_avg:98.27ms
step:1075/1770 train_time:105643ms step_avg:98.27ms
step:1076/1770 train_time:105745ms step_avg:98.28ms
step:1077/1770 train_time:105846ms step_avg:98.28ms
step:1078/1770 train_time:105948ms step_avg:98.28ms
step:1079/1770 train_time:106050ms step_avg:98.29ms
step:1080/1770 train_time:106151ms step_avg:98.29ms
step:1081/1770 train_time:106253ms step_avg:98.29ms
step:1082/1770 train_time:106356ms step_avg:98.30ms
step:1083/1770 train_time:106458ms step_avg:98.30ms
step:1084/1770 train_time:106560ms step_avg:98.30ms
step:1085/1770 train_time:106663ms step_avg:98.31ms
step:1086/1770 train_time:106764ms step_avg:98.31ms
step:1087/1770 train_time:106866ms step_avg:98.31ms
step:1088/1770 train_time:106968ms step_avg:98.32ms
step:1089/1770 train_time:107070ms step_avg:98.32ms
step:1090/1770 train_time:107173ms step_avg:98.32ms
step:1091/1770 train_time:107274ms step_avg:98.33ms
step:1092/1770 train_time:107376ms step_avg:98.33ms
step:1093/1770 train_time:107478ms step_avg:98.33ms
step:1094/1770 train_time:107582ms step_avg:98.34ms
step:1095/1770 train_time:107685ms step_avg:98.34ms
step:1096/1770 train_time:107787ms step_avg:98.35ms
step:1097/1770 train_time:107888ms step_avg:98.35ms
step:1098/1770 train_time:107990ms step_avg:98.35ms
step:1099/1770 train_time:108092ms step_avg:98.35ms
step:1100/1770 train_time:108193ms step_avg:98.36ms
step:1101/1770 train_time:108295ms step_avg:98.36ms
step:1102/1770 train_time:108397ms step_avg:98.36ms
step:1103/1770 train_time:108500ms step_avg:98.37ms
step:1104/1770 train_time:108603ms step_avg:98.37ms
step:1105/1770 train_time:108706ms step_avg:98.38ms
step:1106/1770 train_time:108808ms step_avg:98.38ms
step:1107/1770 train_time:108910ms step_avg:98.38ms
step:1108/1770 train_time:109012ms step_avg:98.39ms
step:1109/1770 train_time:109114ms step_avg:98.39ms
step:1110/1770 train_time:109215ms step_avg:98.39ms
step:1111/1770 train_time:109318ms step_avg:98.40ms
step:1112/1770 train_time:109421ms step_avg:98.40ms
step:1113/1770 train_time:109523ms step_avg:98.40ms
step:1114/1770 train_time:109626ms step_avg:98.41ms
step:1115/1770 train_time:109728ms step_avg:98.41ms
step:1116/1770 train_time:109831ms step_avg:98.41ms
step:1117/1770 train_time:109933ms step_avg:98.42ms
step:1118/1770 train_time:110034ms step_avg:98.42ms
step:1119/1770 train_time:110138ms step_avg:98.42ms
step:1120/1770 train_time:110239ms step_avg:98.43ms
step:1121/1770 train_time:110341ms step_avg:98.43ms
step:1122/1770 train_time:110444ms step_avg:98.43ms
step:1123/1770 train_time:110545ms step_avg:98.44ms
step:1124/1770 train_time:110648ms step_avg:98.44ms
step:1125/1770 train_time:110749ms step_avg:98.44ms
step:1125/1770 val_loss:3.4728 train_time:110850ms step_avg:98.53ms
step:1126/1770 train_time:110867ms step_avg:98.46ms
step:1127/1770 train_time:110959ms step_avg:98.45ms
step:1128/1770 train_time:111062ms step_avg:98.46ms
step:1129/1770 train_time:111163ms step_avg:98.46ms
step:1130/1770 train_time:111264ms step_avg:98.46ms
step:1131/1770 train_time:111365ms step_avg:98.47ms
step:1132/1770 train_time:111466ms step_avg:98.47ms
step:1133/1770 train_time:111567ms step_avg:98.47ms
step:1134/1770 train_time:111670ms step_avg:98.47ms
step:1135/1770 train_time:111772ms step_avg:98.48ms
step:1136/1770 train_time:111876ms step_avg:98.48ms
step:1137/1770 train_time:111981ms step_avg:98.49ms
step:1138/1770 train_time:112083ms step_avg:98.49ms
step:1139/1770 train_time:112185ms step_avg:98.49ms
step:1140/1770 train_time:112288ms step_avg:98.50ms
step:1141/1770 train_time:112389ms step_avg:98.50ms
step:1142/1770 train_time:112490ms step_avg:98.50ms
step:1143/1770 train_time:112591ms step_avg:98.51ms
step:1144/1770 train_time:112694ms step_avg:98.51ms
step:1145/1770 train_time:112796ms step_avg:98.51ms
step:1146/1770 train_time:112900ms step_avg:98.52ms
step:1147/1770 train_time:113003ms step_avg:98.52ms
step:1148/1770 train_time:113104ms step_avg:98.52ms
step:1149/1770 train_time:113205ms step_avg:98.53ms
step:1150/1770 train_time:113307ms step_avg:98.53ms
step:1151/1770 train_time:113408ms step_avg:98.53ms
step:1152/1770 train_time:113510ms step_avg:98.53ms
step:1153/1770 train_time:113612ms step_avg:98.54ms
step:1154/1770 train_time:113716ms step_avg:98.54ms
step:1155/1770 train_time:113818ms step_avg:98.54ms
step:1156/1770 train_time:113921ms step_avg:98.55ms
step:1157/1770 train_time:114024ms step_avg:98.55ms
step:1158/1770 train_time:114126ms step_avg:98.55ms
step:1159/1770 train_time:114227ms step_avg:98.56ms
step:1160/1770 train_time:114329ms step_avg:98.56ms
step:1161/1770 train_time:114430ms step_avg:98.56ms
step:1162/1770 train_time:114533ms step_avg:98.57ms
step:1163/1770 train_time:114636ms step_avg:98.57ms
step:1164/1770 train_time:114738ms step_avg:98.57ms
step:1165/1770 train_time:114841ms step_avg:98.58ms
step:1166/1770 train_time:114943ms step_avg:98.58ms
step:1167/1770 train_time:115045ms step_avg:98.58ms
step:1168/1770 train_time:115147ms step_avg:98.58ms
step:1169/1770 train_time:115249ms step_avg:98.59ms
step:1170/1770 train_time:115350ms step_avg:98.59ms
step:1171/1770 train_time:115453ms step_avg:98.59ms
step:1172/1770 train_time:115556ms step_avg:98.60ms
step:1173/1770 train_time:115658ms step_avg:98.60ms
step:1174/1770 train_time:115760ms step_avg:98.60ms
step:1175/1770 train_time:115862ms step_avg:98.61ms
step:1176/1770 train_time:115964ms step_avg:98.61ms
step:1177/1770 train_time:116066ms step_avg:98.61ms
step:1178/1770 train_time:116168ms step_avg:98.61ms
step:1179/1770 train_time:116270ms step_avg:98.62ms
step:1180/1770 train_time:116373ms step_avg:98.62ms
step:1181/1770 train_time:116475ms step_avg:98.62ms
step:1182/1770 train_time:116577ms step_avg:98.63ms
step:1183/1770 train_time:116681ms step_avg:98.63ms
step:1184/1770 train_time:116785ms step_avg:98.64ms
step:1185/1770 train_time:116887ms step_avg:98.64ms
step:1186/1770 train_time:116991ms step_avg:98.64ms
step:1187/1770 train_time:117096ms step_avg:98.65ms
step:1188/1770 train_time:117199ms step_avg:98.65ms
step:1189/1770 train_time:117303ms step_avg:98.66ms
step:1190/1770 train_time:117406ms step_avg:98.66ms
step:1191/1770 train_time:117508ms step_avg:98.66ms
step:1192/1770 train_time:117613ms step_avg:98.67ms
step:1193/1770 train_time:117717ms step_avg:98.67ms
step:1194/1770 train_time:117821ms step_avg:98.68ms
step:1195/1770 train_time:117923ms step_avg:98.68ms
step:1196/1770 train_time:118027ms step_avg:98.68ms
step:1197/1770 train_time:118130ms step_avg:98.69ms
step:1198/1770 train_time:118234ms step_avg:98.69ms
step:1199/1770 train_time:118338ms step_avg:98.70ms
step:1200/1770 train_time:118441ms step_avg:98.70ms
step:1201/1770 train_time:118545ms step_avg:98.71ms
step:1202/1770 train_time:118648ms step_avg:98.71ms
step:1203/1770 train_time:118752ms step_avg:98.71ms
step:1204/1770 train_time:118858ms step_avg:98.72ms
step:1205/1770 train_time:118961ms step_avg:98.72ms
step:1206/1770 train_time:119064ms step_avg:98.73ms
step:1207/1770 train_time:119166ms step_avg:98.73ms
step:1208/1770 train_time:119269ms step_avg:98.73ms
step:1209/1770 train_time:119372ms step_avg:98.74ms
step:1210/1770 train_time:119477ms step_avg:98.74ms
step:1211/1770 train_time:119580ms step_avg:98.75ms
step:1212/1770 train_time:119684ms step_avg:98.75ms
step:1213/1770 train_time:119788ms step_avg:98.75ms
step:1214/1770 train_time:119891ms step_avg:98.76ms
step:1215/1770 train_time:119995ms step_avg:98.76ms
step:1216/1770 train_time:120100ms step_avg:98.77ms
step:1217/1770 train_time:120203ms step_avg:98.77ms
step:1218/1770 train_time:120305ms step_avg:98.77ms
step:1219/1770 train_time:120408ms step_avg:98.78ms
step:1220/1770 train_time:120512ms step_avg:98.78ms
step:1221/1770 train_time:120616ms step_avg:98.78ms
step:1222/1770 train_time:120722ms step_avg:98.79ms
step:1223/1770 train_time:120824ms step_avg:98.79ms
step:1224/1770 train_time:120927ms step_avg:98.80ms
step:1225/1770 train_time:121031ms step_avg:98.80ms
step:1226/1770 train_time:121134ms step_avg:98.80ms
step:1227/1770 train_time:121239ms step_avg:98.81ms
step:1228/1770 train_time:121344ms step_avg:98.81ms
step:1229/1770 train_time:121447ms step_avg:98.82ms
step:1230/1770 train_time:121550ms step_avg:98.82ms
step:1231/1770 train_time:121655ms step_avg:98.83ms
step:1232/1770 train_time:121758ms step_avg:98.83ms
step:1233/1770 train_time:121861ms step_avg:98.83ms
step:1234/1770 train_time:121964ms step_avg:98.84ms
step:1235/1770 train_time:122067ms step_avg:98.84ms
step:1236/1770 train_time:122170ms step_avg:98.84ms
step:1237/1770 train_time:122274ms step_avg:98.85ms
step:1238/1770 train_time:122378ms step_avg:98.85ms
step:1239/1770 train_time:122482ms step_avg:98.86ms
step:1240/1770 train_time:122585ms step_avg:98.86ms
step:1241/1770 train_time:122689ms step_avg:98.86ms
step:1242/1770 train_time:122792ms step_avg:98.87ms
step:1243/1770 train_time:122897ms step_avg:98.87ms
step:1244/1770 train_time:123000ms step_avg:98.87ms
step:1245/1770 train_time:123103ms step_avg:98.88ms
step:1246/1770 train_time:123205ms step_avg:98.88ms
step:1247/1770 train_time:123308ms step_avg:98.88ms
step:1248/1770 train_time:123412ms step_avg:98.89ms
step:1249/1770 train_time:123517ms step_avg:98.89ms
step:1250/1770 train_time:123621ms step_avg:98.90ms
step:1250/1770 val_loss:3.4262 train_time:123725ms step_avg:98.98ms
step:1251/1770 train_time:123742ms step_avg:98.91ms
step:1252/1770 train_time:123835ms step_avg:98.91ms
step:1253/1770 train_time:123940ms step_avg:98.91ms
step:1254/1770 train_time:124044ms step_avg:98.92ms
step:1255/1770 train_time:124148ms step_avg:98.92ms
step:1256/1770 train_time:124250ms step_avg:98.93ms
step:1257/1770 train_time:124352ms step_avg:98.93ms
step:1258/1770 train_time:124454ms step_avg:98.93ms
step:1259/1770 train_time:124558ms step_avg:98.93ms
step:1260/1770 train_time:124660ms step_avg:98.94ms
step:1261/1770 train_time:124766ms step_avg:98.94ms
step:1262/1770 train_time:124870ms step_avg:98.95ms
step:1263/1770 train_time:124974ms step_avg:98.95ms
step:1264/1770 train_time:125079ms step_avg:98.95ms
step:1265/1770 train_time:125183ms step_avg:98.96ms
step:1266/1770 train_time:125285ms step_avg:98.96ms
step:1267/1770 train_time:125389ms step_avg:98.96ms
step:1268/1770 train_time:125492ms step_avg:98.97ms
step:1269/1770 train_time:125594ms step_avg:98.97ms
step:1270/1770 train_time:125699ms step_avg:98.98ms
step:1271/1770 train_time:125803ms step_avg:98.98ms
step:1272/1770 train_time:125906ms step_avg:98.98ms
step:1273/1770 train_time:126011ms step_avg:98.99ms
step:1274/1770 train_time:126116ms step_avg:98.99ms
step:1275/1770 train_time:126220ms step_avg:99.00ms
step:1276/1770 train_time:126324ms step_avg:99.00ms
step:1277/1770 train_time:126427ms step_avg:99.00ms
step:1278/1770 train_time:126530ms step_avg:99.01ms
step:1279/1770 train_time:126634ms step_avg:99.01ms
step:1280/1770 train_time:126737ms step_avg:99.01ms
step:1281/1770 train_time:126841ms step_avg:99.02ms
step:1282/1770 train_time:126946ms step_avg:99.02ms
step:1283/1770 train_time:127049ms step_avg:99.02ms
step:1284/1770 train_time:127153ms step_avg:99.03ms
step:1285/1770 train_time:127256ms step_avg:99.03ms
step:1286/1770 train_time:127362ms step_avg:99.04ms
step:1287/1770 train_time:127466ms step_avg:99.04ms
step:1288/1770 train_time:127569ms step_avg:99.04ms
step:1289/1770 train_time:127672ms step_avg:99.05ms
step:1290/1770 train_time:127775ms step_avg:99.05ms
step:1291/1770 train_time:127880ms step_avg:99.05ms
step:1292/1770 train_time:127983ms step_avg:99.06ms
step:1293/1770 train_time:128086ms step_avg:99.06ms
step:1294/1770 train_time:128189ms step_avg:99.06ms
step:1295/1770 train_time:128293ms step_avg:99.07ms
step:1296/1770 train_time:128397ms step_avg:99.07ms
step:1297/1770 train_time:128501ms step_avg:99.08ms
step:1298/1770 train_time:128604ms step_avg:99.08ms
step:1299/1770 train_time:128707ms step_avg:99.08ms
step:1300/1770 train_time:128811ms step_avg:99.09ms
step:1301/1770 train_time:128915ms step_avg:99.09ms
step:1302/1770 train_time:129018ms step_avg:99.09ms
step:1303/1770 train_time:129122ms step_avg:99.10ms
step:1304/1770 train_time:129226ms step_avg:99.10ms
step:1305/1770 train_time:129329ms step_avg:99.10ms
step:1306/1770 train_time:129432ms step_avg:99.11ms
step:1307/1770 train_time:129535ms step_avg:99.11ms
step:1308/1770 train_time:129639ms step_avg:99.11ms
step:1309/1770 train_time:129744ms step_avg:99.12ms
step:1310/1770 train_time:129847ms step_avg:99.12ms
step:1311/1770 train_time:129949ms step_avg:99.12ms
step:1312/1770 train_time:130052ms step_avg:99.13ms
step:1313/1770 train_time:130155ms step_avg:99.13ms
step:1314/1770 train_time:130260ms step_avg:99.13ms
step:1315/1770 train_time:130364ms step_avg:99.14ms
step:1316/1770 train_time:130467ms step_avg:99.14ms
step:1317/1770 train_time:130571ms step_avg:99.14ms
step:1318/1770 train_time:130676ms step_avg:99.15ms
step:1319/1770 train_time:130781ms step_avg:99.15ms
step:1320/1770 train_time:130884ms step_avg:99.15ms
step:1321/1770 train_time:130987ms step_avg:99.16ms
step:1322/1770 train_time:131091ms step_avg:99.16ms
step:1323/1770 train_time:131195ms step_avg:99.16ms
step:1324/1770 train_time:131300ms step_avg:99.17ms
step:1325/1770 train_time:131404ms step_avg:99.17ms
step:1326/1770 train_time:131508ms step_avg:99.18ms
step:1327/1770 train_time:131612ms step_avg:99.18ms
step:1328/1770 train_time:131716ms step_avg:99.18ms
step:1329/1770 train_time:131821ms step_avg:99.19ms
step:1330/1770 train_time:131924ms step_avg:99.19ms
step:1331/1770 train_time:132027ms step_avg:99.19ms
step:1332/1770 train_time:132130ms step_avg:99.20ms
step:1333/1770 train_time:132233ms step_avg:99.20ms
step:1334/1770 train_time:132336ms step_avg:99.20ms
step:1335/1770 train_time:132442ms step_avg:99.21ms
step:1336/1770 train_time:132545ms step_avg:99.21ms
step:1337/1770 train_time:132649ms step_avg:99.21ms
step:1338/1770 train_time:132752ms step_avg:99.22ms
step:1339/1770 train_time:132856ms step_avg:99.22ms
step:1340/1770 train_time:132961ms step_avg:99.22ms
step:1341/1770 train_time:133064ms step_avg:99.23ms
step:1342/1770 train_time:133169ms step_avg:99.23ms
step:1343/1770 train_time:133272ms step_avg:99.23ms
step:1344/1770 train_time:133376ms step_avg:99.24ms
step:1345/1770 train_time:133479ms step_avg:99.24ms
step:1346/1770 train_time:133582ms step_avg:99.24ms
step:1347/1770 train_time:133686ms step_avg:99.25ms
step:1348/1770 train_time:133791ms step_avg:99.25ms
step:1349/1770 train_time:133894ms step_avg:99.25ms
step:1350/1770 train_time:134000ms step_avg:99.26ms
step:1351/1770 train_time:134103ms step_avg:99.26ms
step:1352/1770 train_time:134206ms step_avg:99.27ms
step:1353/1770 train_time:134311ms step_avg:99.27ms
step:1354/1770 train_time:134414ms step_avg:99.27ms
step:1355/1770 train_time:134518ms step_avg:99.28ms
step:1356/1770 train_time:134623ms step_avg:99.28ms
step:1357/1770 train_time:134728ms step_avg:99.28ms
step:1358/1770 train_time:134830ms step_avg:99.29ms
step:1359/1770 train_time:134935ms step_avg:99.29ms
step:1360/1770 train_time:135040ms step_avg:99.29ms
step:1361/1770 train_time:135143ms step_avg:99.30ms
step:1362/1770 train_time:135246ms step_avg:99.30ms
step:1363/1770 train_time:135350ms step_avg:99.30ms
step:1364/1770 train_time:135454ms step_avg:99.31ms
step:1365/1770 train_time:135558ms step_avg:99.31ms
step:1366/1770 train_time:135662ms step_avg:99.31ms
step:1367/1770 train_time:135767ms step_avg:99.32ms
step:1368/1770 train_time:135869ms step_avg:99.32ms
step:1369/1770 train_time:135973ms step_avg:99.32ms
step:1370/1770 train_time:136078ms step_avg:99.33ms
step:1371/1770 train_time:136181ms step_avg:99.33ms
step:1372/1770 train_time:136285ms step_avg:99.33ms
step:1373/1770 train_time:136389ms step_avg:99.34ms
step:1374/1770 train_time:136493ms step_avg:99.34ms
step:1375/1770 train_time:136597ms step_avg:99.34ms
step:1375/1770 val_loss:3.3817 train_time:136702ms step_avg:99.42ms
step:1376/1770 train_time:136719ms step_avg:99.36ms
step:1377/1770 train_time:136813ms step_avg:99.36ms
step:1378/1770 train_time:136916ms step_avg:99.36ms
step:1379/1770 train_time:137019ms step_avg:99.36ms
step:1380/1770 train_time:137122ms step_avg:99.36ms
step:1381/1770 train_time:137226ms step_avg:99.37ms
step:1382/1770 train_time:137329ms step_avg:99.37ms
step:1383/1770 train_time:137432ms step_avg:99.37ms
step:1384/1770 train_time:137535ms step_avg:99.38ms
step:1385/1770 train_time:137638ms step_avg:99.38ms
step:1386/1770 train_time:137743ms step_avg:99.38ms
step:1387/1770 train_time:137848ms step_avg:99.39ms
step:1388/1770 train_time:137953ms step_avg:99.39ms
step:1389/1770 train_time:138056ms step_avg:99.39ms
step:1390/1770 train_time:138159ms step_avg:99.40ms
step:1391/1770 train_time:138263ms step_avg:99.40ms
step:1392/1770 train_time:138366ms step_avg:99.40ms
step:1393/1770 train_time:138471ms step_avg:99.40ms
step:1394/1770 train_time:138573ms step_avg:99.41ms
step:1395/1770 train_time:138677ms step_avg:99.41ms
step:1396/1770 train_time:138781ms step_avg:99.41ms
step:1397/1770 train_time:138885ms step_avg:99.42ms
step:1398/1770 train_time:138989ms step_avg:99.42ms
step:1399/1770 train_time:139092ms step_avg:99.42ms
step:1400/1770 train_time:139196ms step_avg:99.43ms
step:1401/1770 train_time:139299ms step_avg:99.43ms
step:1402/1770 train_time:139403ms step_avg:99.43ms
step:1403/1770 train_time:139507ms step_avg:99.43ms
step:1404/1770 train_time:139611ms step_avg:99.44ms
step:1405/1770 train_time:139714ms step_avg:99.44ms
step:1406/1770 train_time:139817ms step_avg:99.44ms
step:1407/1770 train_time:139920ms step_avg:99.45ms
step:1408/1770 train_time:140025ms step_avg:99.45ms
step:1409/1770 train_time:140128ms step_avg:99.45ms
step:1410/1770 train_time:140232ms step_avg:99.46ms
step:1411/1770 train_time:140334ms step_avg:99.46ms
step:1412/1770 train_time:140437ms step_avg:99.46ms
step:1413/1770 train_time:140541ms step_avg:99.46ms
step:1414/1770 train_time:140645ms step_avg:99.47ms
step:1415/1770 train_time:140749ms step_avg:99.47ms
step:1416/1770 train_time:140853ms step_avg:99.47ms
step:1417/1770 train_time:140955ms step_avg:99.47ms
step:1418/1770 train_time:141059ms step_avg:99.48ms
step:1419/1770 train_time:141164ms step_avg:99.48ms
step:1420/1770 train_time:141267ms step_avg:99.48ms
step:1421/1770 train_time:141370ms step_avg:99.49ms
step:1422/1770 train_time:141473ms step_avg:99.49ms
step:1423/1770 train_time:141577ms step_avg:99.49ms
step:1424/1770 train_time:141682ms step_avg:99.50ms
step:1425/1770 train_time:141786ms step_avg:99.50ms
step:1426/1770 train_time:141890ms step_avg:99.50ms
step:1427/1770 train_time:141993ms step_avg:99.50ms
step:1428/1770 train_time:142098ms step_avg:99.51ms
step:1429/1770 train_time:142201ms step_avg:99.51ms
step:1430/1770 train_time:142305ms step_avg:99.51ms
step:1431/1770 train_time:142410ms step_avg:99.52ms
step:1432/1770 train_time:142513ms step_avg:99.52ms
step:1433/1770 train_time:142616ms step_avg:99.52ms
step:1434/1770 train_time:142718ms step_avg:99.52ms
step:1435/1770 train_time:142824ms step_avg:99.53ms
step:1436/1770 train_time:142929ms step_avg:99.53ms
step:1437/1770 train_time:143033ms step_avg:99.54ms
step:1438/1770 train_time:143136ms step_avg:99.54ms
step:1439/1770 train_time:143238ms step_avg:99.54ms
step:1440/1770 train_time:143343ms step_avg:99.54ms
step:1441/1770 train_time:143447ms step_avg:99.55ms
step:1442/1770 train_time:143550ms step_avg:99.55ms
step:1443/1770 train_time:143654ms step_avg:99.55ms
step:1444/1770 train_time:143757ms step_avg:99.55ms
step:1445/1770 train_time:143863ms step_avg:99.56ms
step:1446/1770 train_time:143968ms step_avg:99.56ms
step:1447/1770 train_time:144073ms step_avg:99.57ms
step:1448/1770 train_time:144177ms step_avg:99.57ms
step:1449/1770 train_time:144283ms step_avg:99.57ms
step:1450/1770 train_time:144387ms step_avg:99.58ms
step:1451/1770 train_time:144491ms step_avg:99.58ms
step:1452/1770 train_time:144596ms step_avg:99.58ms
step:1453/1770 train_time:144700ms step_avg:99.59ms
step:1454/1770 train_time:144804ms step_avg:99.59ms
step:1455/1770 train_time:144912ms step_avg:99.60ms
step:1456/1770 train_time:145018ms step_avg:99.60ms
step:1457/1770 train_time:145124ms step_avg:99.60ms
step:1458/1770 train_time:145229ms step_avg:99.61ms
step:1459/1770 train_time:145334ms step_avg:99.61ms
step:1460/1770 train_time:145438ms step_avg:99.62ms
step:1461/1770 train_time:145543ms step_avg:99.62ms
step:1462/1770 train_time:145648ms step_avg:99.62ms
step:1463/1770 train_time:145753ms step_avg:99.63ms
step:1464/1770 train_time:145859ms step_avg:99.63ms
step:1465/1770 train_time:145963ms step_avg:99.63ms
step:1466/1770 train_time:146070ms step_avg:99.64ms
step:1467/1770 train_time:146177ms step_avg:99.64ms
step:1468/1770 train_time:146281ms step_avg:99.65ms
step:1469/1770 train_time:146385ms step_avg:99.65ms
step:1470/1770 train_time:146489ms step_avg:99.65ms
step:1471/1770 train_time:146593ms step_avg:99.66ms
step:1472/1770 train_time:146698ms step_avg:99.66ms
step:1473/1770 train_time:146803ms step_avg:99.66ms
step:1474/1770 train_time:146908ms step_avg:99.67ms
step:1475/1770 train_time:147013ms step_avg:99.67ms
step:1476/1770 train_time:147117ms step_avg:99.67ms
step:1477/1770 train_time:147224ms step_avg:99.68ms
step:1478/1770 train_time:147329ms step_avg:99.68ms
step:1479/1770 train_time:147432ms step_avg:99.68ms
step:1480/1770 train_time:147536ms step_avg:99.69ms
step:1481/1770 train_time:147645ms step_avg:99.69ms
step:1482/1770 train_time:147748ms step_avg:99.70ms
step:1483/1770 train_time:147853ms step_avg:99.70ms
step:1484/1770 train_time:147957ms step_avg:99.70ms
step:1485/1770 train_time:148062ms step_avg:99.71ms
step:1486/1770 train_time:148166ms step_avg:99.71ms
step:1487/1770 train_time:148270ms step_avg:99.71ms
step:1488/1770 train_time:148374ms step_avg:99.71ms
step:1489/1770 train_time:148479ms step_avg:99.72ms
step:1490/1770 train_time:148585ms step_avg:99.72ms
step:1491/1770 train_time:148690ms step_avg:99.73ms
step:1492/1770 train_time:148795ms step_avg:99.73ms
step:1493/1770 train_time:148902ms step_avg:99.73ms
step:1494/1770 train_time:149011ms step_avg:99.74ms
step:1495/1770 train_time:149114ms step_avg:99.74ms
step:1496/1770 train_time:149218ms step_avg:99.74ms
step:1497/1770 train_time:149323ms step_avg:99.75ms
step:1498/1770 train_time:149427ms step_avg:99.75ms
step:1499/1770 train_time:149532ms step_avg:99.75ms
step:1500/1770 train_time:149635ms step_avg:99.76ms
step:1500/1770 val_loss:3.3442 train_time:149739ms step_avg:99.83ms
step:1501/1770 train_time:149757ms step_avg:99.77ms
step:1502/1770 train_time:149853ms step_avg:99.77ms
step:1503/1770 train_time:149956ms step_avg:99.77ms
step:1504/1770 train_time:150060ms step_avg:99.77ms
step:1505/1770 train_time:150167ms step_avg:99.78ms
step:1506/1770 train_time:150271ms step_avg:99.78ms
step:1507/1770 train_time:150376ms step_avg:99.78ms
step:1508/1770 train_time:150480ms step_avg:99.79ms
step:1509/1770 train_time:150584ms step_avg:99.79ms
step:1510/1770 train_time:150688ms step_avg:99.79ms
step:1511/1770 train_time:150795ms step_avg:99.80ms
step:1512/1770 train_time:150901ms step_avg:99.80ms
step:1513/1770 train_time:151006ms step_avg:99.81ms
step:1514/1770 train_time:151110ms step_avg:99.81ms
step:1515/1770 train_time:151215ms step_avg:99.81ms
step:1516/1770 train_time:151321ms step_avg:99.82ms
step:1517/1770 train_time:151425ms step_avg:99.82ms
step:1518/1770 train_time:151531ms step_avg:99.82ms
step:1519/1770 train_time:151634ms step_avg:99.82ms
step:1520/1770 train_time:151739ms step_avg:99.83ms
step:1521/1770 train_time:151845ms step_avg:99.83ms
step:1522/1770 train_time:151950ms step_avg:99.84ms
step:1523/1770 train_time:152055ms step_avg:99.84ms
step:1524/1770 train_time:152160ms step_avg:99.84ms
step:1525/1770 train_time:152264ms step_avg:99.85ms
step:1526/1770 train_time:152369ms step_avg:99.85ms
step:1527/1770 train_time:152473ms step_avg:99.85ms
step:1528/1770 train_time:152580ms step_avg:99.86ms
step:1529/1770 train_time:152685ms step_avg:99.86ms
step:1530/1770 train_time:152789ms step_avg:99.86ms
step:1531/1770 train_time:152893ms step_avg:99.86ms
step:1532/1770 train_time:152998ms step_avg:99.87ms
step:1533/1770 train_time:153104ms step_avg:99.87ms
step:1534/1770 train_time:153208ms step_avg:99.88ms
step:1535/1770 train_time:153313ms step_avg:99.88ms
step:1536/1770 train_time:153417ms step_avg:99.88ms
step:1537/1770 train_time:153522ms step_avg:99.88ms
step:1538/1770 train_time:153628ms step_avg:99.89ms
step:1539/1770 train_time:153732ms step_avg:99.89ms
step:1540/1770 train_time:153839ms step_avg:99.90ms
step:1541/1770 train_time:153944ms step_avg:99.90ms
step:1542/1770 train_time:154049ms step_avg:99.90ms
step:1543/1770 train_time:154153ms step_avg:99.90ms
step:1544/1770 train_time:154261ms step_avg:99.91ms
step:1545/1770 train_time:154365ms step_avg:99.91ms
step:1546/1770 train_time:154470ms step_avg:99.92ms
step:1547/1770 train_time:154573ms step_avg:99.92ms
step:1548/1770 train_time:154679ms step_avg:99.92ms
step:1549/1770 train_time:154784ms step_avg:99.93ms
step:1550/1770 train_time:154890ms step_avg:99.93ms
step:1551/1770 train_time:154994ms step_avg:99.93ms
step:1552/1770 train_time:155100ms step_avg:99.94ms
step:1553/1770 train_time:155204ms step_avg:99.94ms
step:1554/1770 train_time:155308ms step_avg:99.94ms
step:1555/1770 train_time:155413ms step_avg:99.94ms
step:1556/1770 train_time:155517ms step_avg:99.95ms
step:1557/1770 train_time:155622ms step_avg:99.95ms
step:1558/1770 train_time:155727ms step_avg:99.95ms
step:1559/1770 train_time:155831ms step_avg:99.96ms
step:1560/1770 train_time:155935ms step_avg:99.96ms
step:1561/1770 train_time:156042ms step_avg:99.96ms
step:1562/1770 train_time:156147ms step_avg:99.97ms
step:1563/1770 train_time:156251ms step_avg:99.97ms
step:1564/1770 train_time:156355ms step_avg:99.97ms
step:1565/1770 train_time:156459ms step_avg:99.97ms
step:1566/1770 train_time:156565ms step_avg:99.98ms
step:1567/1770 train_time:156669ms step_avg:99.98ms
step:1568/1770 train_time:156773ms step_avg:99.98ms
step:1569/1770 train_time:156882ms step_avg:99.99ms
step:1570/1770 train_time:156986ms step_avg:99.99ms
step:1571/1770 train_time:157091ms step_avg:99.99ms
step:1572/1770 train_time:157196ms step_avg:100.00ms
step:1573/1770 train_time:157302ms step_avg:100.00ms
step:1574/1770 train_time:157406ms step_avg:100.00ms
step:1575/1770 train_time:157510ms step_avg:100.01ms
step:1576/1770 train_time:157615ms step_avg:100.01ms
step:1577/1770 train_time:157723ms step_avg:100.01ms
step:1578/1770 train_time:157829ms step_avg:100.02ms
step:1579/1770 train_time:157934ms step_avg:100.02ms
step:1580/1770 train_time:158039ms step_avg:100.02ms
step:1581/1770 train_time:158147ms step_avg:100.03ms
step:1582/1770 train_time:158252ms step_avg:100.03ms
step:1583/1770 train_time:158357ms step_avg:100.04ms
step:1584/1770 train_time:158463ms step_avg:100.04ms
step:1585/1770 train_time:158568ms step_avg:100.04ms
step:1586/1770 train_time:158676ms step_avg:100.05ms
step:1587/1770 train_time:158782ms step_avg:100.05ms
step:1588/1770 train_time:158887ms step_avg:100.05ms
step:1589/1770 train_time:158993ms step_avg:100.06ms
step:1590/1770 train_time:159097ms step_avg:100.06ms
step:1591/1770 train_time:159203ms step_avg:100.06ms
step:1592/1770 train_time:159309ms step_avg:100.07ms
step:1593/1770 train_time:159413ms step_avg:100.07ms
step:1594/1770 train_time:159518ms step_avg:100.07ms
step:1595/1770 train_time:159624ms step_avg:100.08ms
step:1596/1770 train_time:159730ms step_avg:100.08ms
step:1597/1770 train_time:159834ms step_avg:100.08ms
step:1598/1770 train_time:159939ms step_avg:100.09ms
step:1599/1770 train_time:160045ms step_avg:100.09ms
step:1600/1770 train_time:160152ms step_avg:100.09ms
step:1601/1770 train_time:160257ms step_avg:100.10ms
step:1602/1770 train_time:160363ms step_avg:100.10ms
step:1603/1770 train_time:160468ms step_avg:100.10ms
step:1604/1770 train_time:160571ms step_avg:100.11ms
step:1605/1770 train_time:160676ms step_avg:100.11ms
step:1606/1770 train_time:160781ms step_avg:100.11ms
step:1607/1770 train_time:160888ms step_avg:100.12ms
step:1608/1770 train_time:160993ms step_avg:100.12ms
step:1609/1770 train_time:161097ms step_avg:100.12ms
step:1610/1770 train_time:161203ms step_avg:100.13ms
step:1611/1770 train_time:161309ms step_avg:100.13ms
step:1612/1770 train_time:161414ms step_avg:100.13ms
step:1613/1770 train_time:161518ms step_avg:100.14ms
step:1614/1770 train_time:161623ms step_avg:100.14ms
step:1615/1770 train_time:161729ms step_avg:100.14ms
step:1616/1770 train_time:161833ms step_avg:100.14ms
step:1617/1770 train_time:161938ms step_avg:100.15ms
step:1618/1770 train_time:162045ms step_avg:100.15ms
step:1619/1770 train_time:162150ms step_avg:100.15ms
step:1620/1770 train_time:162256ms step_avg:100.16ms
step:1621/1770 train_time:162362ms step_avg:100.16ms
step:1622/1770 train_time:162467ms step_avg:100.16ms
step:1623/1770 train_time:162574ms step_avg:100.17ms
step:1624/1770 train_time:162679ms step_avg:100.17ms
step:1625/1770 train_time:162783ms step_avg:100.17ms
step:1625/1770 val_loss:3.3098 train_time:162887ms step_avg:100.24ms
step:1626/1770 train_time:162904ms step_avg:100.19ms
step:1627/1770 train_time:162997ms step_avg:100.18ms
step:1628/1770 train_time:163101ms step_avg:100.19ms
step:1629/1770 train_time:163205ms step_avg:100.19ms
step:1630/1770 train_time:163309ms step_avg:100.19ms
step:1631/1770 train_time:163414ms step_avg:100.19ms
step:1632/1770 train_time:163518ms step_avg:100.20ms
step:1633/1770 train_time:163622ms step_avg:100.20ms
step:1634/1770 train_time:163727ms step_avg:100.20ms
step:1635/1770 train_time:163833ms step_avg:100.20ms
step:1636/1770 train_time:163939ms step_avg:100.21ms
step:1637/1770 train_time:164045ms step_avg:100.21ms
step:1638/1770 train_time:164151ms step_avg:100.21ms
step:1639/1770 train_time:164255ms step_avg:100.22ms
step:1640/1770 train_time:164361ms step_avg:100.22ms
step:1641/1770 train_time:164465ms step_avg:100.22ms
step:1642/1770 train_time:164568ms step_avg:100.22ms
step:1643/1770 train_time:164672ms step_avg:100.23ms
step:1644/1770 train_time:164779ms step_avg:100.23ms
step:1645/1770 train_time:164884ms step_avg:100.23ms
step:1646/1770 train_time:164993ms step_avg:100.24ms
step:1647/1770 train_time:165099ms step_avg:100.24ms
step:1648/1770 train_time:165204ms step_avg:100.25ms
step:1649/1770 train_time:165309ms step_avg:100.25ms
step:1650/1770 train_time:165413ms step_avg:100.25ms
step:1651/1770 train_time:165518ms step_avg:100.25ms
step:1652/1770 train_time:165623ms step_avg:100.26ms
step:1653/1770 train_time:165727ms step_avg:100.26ms
step:1654/1770 train_time:165834ms step_avg:100.26ms
step:1655/1770 train_time:165941ms step_avg:100.27ms
step:1656/1770 train_time:166046ms step_avg:100.27ms
step:1657/1770 train_time:166152ms step_avg:100.27ms
step:1658/1770 train_time:166256ms step_avg:100.28ms
step:1659/1770 train_time:166363ms step_avg:100.28ms
step:1660/1770 train_time:166467ms step_avg:100.28ms
step:1661/1770 train_time:166573ms step_avg:100.28ms
step:1662/1770 train_time:166678ms step_avg:100.29ms
step:1663/1770 train_time:166783ms step_avg:100.29ms
step:1664/1770 train_time:166888ms step_avg:100.29ms
step:1665/1770 train_time:166992ms step_avg:100.30ms
step:1666/1770 train_time:167097ms step_avg:100.30ms
step:1667/1770 train_time:167203ms step_avg:100.30ms
step:1668/1770 train_time:167307ms step_avg:100.30ms
step:1669/1770 train_time:167410ms step_avg:100.31ms
step:1670/1770 train_time:167515ms step_avg:100.31ms
step:1671/1770 train_time:167619ms step_avg:100.31ms
step:1672/1770 train_time:167725ms step_avg:100.31ms
step:1673/1770 train_time:167831ms step_avg:100.32ms
step:1674/1770 train_time:167935ms step_avg:100.32ms
step:1675/1770 train_time:168040ms step_avg:100.32ms
step:1676/1770 train_time:168146ms step_avg:100.33ms
step:1677/1770 train_time:168254ms step_avg:100.33ms
step:1678/1770 train_time:168358ms step_avg:100.33ms
step:1679/1770 train_time:168463ms step_avg:100.34ms
step:1680/1770 train_time:168568ms step_avg:100.34ms
step:1681/1770 train_time:168673ms step_avg:100.34ms
step:1682/1770 train_time:168780ms step_avg:100.34ms
step:1683/1770 train_time:168884ms step_avg:100.35ms
step:1684/1770 train_time:168989ms step_avg:100.35ms
step:1685/1770 train_time:169093ms step_avg:100.35ms
step:1686/1770 train_time:169199ms step_avg:100.36ms
step:1687/1770 train_time:169307ms step_avg:100.36ms
step:1688/1770 train_time:169412ms step_avg:100.36ms
step:1689/1770 train_time:169517ms step_avg:100.37ms
step:1690/1770 train_time:169624ms step_avg:100.37ms
step:1691/1770 train_time:169729ms step_avg:100.37ms
step:1692/1770 train_time:169833ms step_avg:100.37ms
step:1693/1770 train_time:169940ms step_avg:100.38ms
step:1694/1770 train_time:170045ms step_avg:100.38ms
step:1695/1770 train_time:170150ms step_avg:100.38ms
step:1696/1770 train_time:170257ms step_avg:100.39ms
step:1697/1770 train_time:170362ms step_avg:100.39ms
step:1698/1770 train_time:170468ms step_avg:100.39ms
step:1699/1770 train_time:170571ms step_avg:100.39ms
step:1700/1770 train_time:170675ms step_avg:100.40ms
step:1701/1770 train_time:170781ms step_avg:100.40ms
step:1702/1770 train_time:170885ms step_avg:100.40ms
step:1703/1770 train_time:170988ms step_avg:100.40ms
step:1704/1770 train_time:171093ms step_avg:100.41ms
step:1705/1770 train_time:171199ms step_avg:100.41ms
step:1706/1770 train_time:171303ms step_avg:100.41ms
step:1707/1770 train_time:171409ms step_avg:100.42ms
step:1708/1770 train_time:171514ms step_avg:100.42ms
step:1709/1770 train_time:171621ms step_avg:100.42ms
step:1710/1770 train_time:171728ms step_avg:100.43ms
step:1711/1770 train_time:171835ms step_avg:100.43ms
step:1712/1770 train_time:171940ms step_avg:100.43ms
step:1713/1770 train_time:172045ms step_avg:100.43ms
step:1714/1770 train_time:172150ms step_avg:100.44ms
step:1715/1770 train_time:172255ms step_avg:100.44ms
step:1716/1770 train_time:172361ms step_avg:100.44ms
step:1717/1770 train_time:172466ms step_avg:100.45ms
step:1718/1770 train_time:172572ms step_avg:100.45ms
step:1719/1770 train_time:172678ms step_avg:100.45ms
step:1720/1770 train_time:172785ms step_avg:100.46ms
step:1721/1770 train_time:172890ms step_avg:100.46ms
step:1722/1770 train_time:172997ms step_avg:100.46ms
step:1723/1770 train_time:173104ms step_avg:100.47ms
step:1724/1770 train_time:173211ms step_avg:100.47ms
step:1725/1770 train_time:173318ms step_avg:100.47ms
step:1726/1770 train_time:173425ms step_avg:100.48ms
step:1727/1770 train_time:173531ms step_avg:100.48ms
step:1728/1770 train_time:173638ms step_avg:100.48ms
step:1729/1770 train_time:173744ms step_avg:100.49ms
step:1730/1770 train_time:173851ms step_avg:100.49ms
step:1731/1770 train_time:173958ms step_avg:100.50ms
step:1732/1770 train_time:174064ms step_avg:100.50ms
step:1733/1770 train_time:174170ms step_avg:100.50ms
step:1734/1770 train_time:174275ms step_avg:100.50ms
step:1735/1770 train_time:174383ms step_avg:100.51ms
step:1736/1770 train_time:174488ms step_avg:100.51ms
step:1737/1770 train_time:174594ms step_avg:100.51ms
step:1738/1770 train_time:174699ms step_avg:100.52ms
step:1739/1770 train_time:174806ms step_avg:100.52ms
step:1740/1770 train_time:174910ms step_avg:100.52ms
step:1741/1770 train_time:175018ms step_avg:100.53ms
step:1742/1770 train_time:175126ms step_avg:100.53ms
step:1743/1770 train_time:175232ms step_avg:100.53ms
step:1744/1770 train_time:175339ms step_avg:100.54ms
step:1745/1770 train_time:175445ms step_avg:100.54ms
step:1746/1770 train_time:175553ms step_avg:100.55ms
step:1747/1770 train_time:175658ms step_avg:100.55ms
step:1748/1770 train_time:175767ms step_avg:100.55ms
step:1749/1770 train_time:175874ms step_avg:100.56ms
step:1750/1770 train_time:175979ms step_avg:100.56ms
step:1750/1770 val_loss:3.2830 train_time:176084ms step_avg:100.62ms
step:1751/1770 train_time:176102ms step_avg:100.57ms
step:1752/1770 train_time:176199ms step_avg:100.57ms
step:1753/1770 train_time:176304ms step_avg:100.57ms
step:1754/1770 train_time:176410ms step_avg:100.58ms
step:1755/1770 train_time:176515ms step_avg:100.58ms
step:1756/1770 train_time:176621ms step_avg:100.58ms
step:1757/1770 train_time:176727ms step_avg:100.58ms
step:1758/1770 train_time:176833ms step_avg:100.59ms
step:1759/1770 train_time:176939ms step_avg:100.59ms
step:1760/1770 train_time:177044ms step_avg:100.59ms
step:1761/1770 train_time:177152ms step_avg:100.60ms
step:1762/1770 train_time:177261ms step_avg:100.60ms
step:1763/1770 train_time:177366ms step_avg:100.60ms
step:1764/1770 train_time:177472ms step_avg:100.61ms
step:1765/1770 train_time:177577ms step_avg:100.61ms
step:1766/1770 train_time:177687ms step_avg:100.62ms
step:1767/1770 train_time:177790ms step_avg:100.62ms
step:1768/1770 train_time:177896ms step_avg:100.62ms
step:1769/1770 train_time:178000ms step_avg:100.62ms
step:1770/1770 train_time:178106ms step_avg:100.62ms
step:1770/1770 val_loss:3.2799 train_time:178212ms step_avg:100.68ms
peak memory allocated: 30724 MiB reserved: 46492 MiB
