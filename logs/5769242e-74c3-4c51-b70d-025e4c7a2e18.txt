import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
import copy
import glob
from dataclasses import dataclass
from functools import lru_cache, partial # Added partial for hook registration
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
#torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params: list[Tensor] = [*params]
        param_groups = []
        for size in {p.numel() for p in params}:
            b = torch.empty(world_size, size, dtype=torch.bfloat16, device="cuda")
            group = dict(params=[p for p in params if p.numel() == size],
                         update_buffer=b, update_buffer_views=[b[i] for i in range(world_size)])
            param_groups.append(group)
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            update_buffer: Tensor = group["update_buffer"]
            update_buffer_views: list[Tensor] = group["update_buffer_views"]
            # generate weight updates in distributed fashion
            params: list[Tensor] = group["params"]
            handle = None
            params_world = None
            def update_prev(): # optimized Muon implementation contributed by @YouJiacheng
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffer_views):
                    p_world.add_(g_world.view_as(p_world),
                                 alpha=-group["lr"] * max(1, p_world.size(-2) / p_world.size(-1))**0.5)
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if "momentum_buffer" not in state:
                        state["momentum_buffer"] = torch.zeros_like(g)
                    buf: Tensor = state["momentum_buffer"]
                    buf.lerp_(g, 1 - group["momentum"])
                    g = g.lerp_(buf, group["momentum"]) if group["nesterov"] else buf
                    g = zeropower_via_newtonschulz5(g, steps=group["ns_steps"]).flatten()
                else:
                    g = update_buffer_views[self.rank]
                if base_i > 0:
                    update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=0.12).transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, block_mask: BlockMask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, max_seq_len, i) for i in range(num_layers)])
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128),
                                    use_fp8=True, x_s=(model_dim**0.5)/448, w_s=24/448, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        self.skip_weights = nn.Parameter(torch.ones(num_layers//2))

    def create_blockmasks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        docs = (input_seq == 50256).cumsum(0)

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_blockmask: Tensor):
            num_blocks = dense_blockmask.sum(dim=-1, dtype=torch.int32)
            indices = dense_blockmask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
        causal_blockmask_any = block_idx[:, None] >= block_idx
        causal_blockmask_all = block_idx[:, None] > block_idx
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
        document_blockmask_any = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
        document_blockmask_all = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
        blockmask_any = causal_blockmask_any & document_blockmask_any
        blockmask_all = causal_blockmask_all & document_blockmask_all
        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(blockmask_any & ~blockmask_all)
        full_kv_num_blocks, full_kv_indices = dense_to_ordered(blockmask_all)
        def build_bm(window_size_blocks: Tensor) -> BlockMask:
            return BlockMask.from_kv_blocks(
                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(window_size_blocks - full_kv_num_blocks, 1)),
                partial_kv_indices,
                torch.clamp_max(full_kv_num_blocks, window_size_blocks - 1),
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        long_bm, short_bm = self.create_blockmasks(input_seq, sliding_window_num_blocks)
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, long_bm, short_bm, short_bm, short_bm, long_bm]
        assert len(block_masks) == len(self.blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977

        # U-net design by @brendanh0gan
        skip_connections = []
        n = len(self.skip_weights)
        for i in range(len(self.blocks)):
            if i >= n:
                x = x + self.skip_weights[i - n] * skip_connections.pop()
            x = self.blocks[i](x, ve[i], x0, block_masks[i])
            if i < n:
                skip_connections.append(x)

        x = norm(x)
        logits = self.lm_head(x).float()
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / (7.5 * x.size(-1)**0.5))
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq, reduction='sum' if self.training else 'mean')
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn't helpful.
        pos += batch_size
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_seq_len = 48*1024 # FlexAttention sequence length
    val_seq_len = 4*64*1024 # FlexAttention sequence length for validation
    # optimization
    num_iterations = 1770 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    # architecture
    vocab_size = 50257
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert world_size == 8 # this code is designed for 8xH100
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs("logs_bucket", exist_ok=True)
    logfile = f"logs_bucket/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

########################################
#    Construct model and optimizer     #
########################################

model: nn.Module = GPT(vocab_size=args.vocab_size, num_layers=12, num_heads=6, model_dim=768,
                       max_seq_len=max(args.train_seq_len, args.val_seq_len)).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
adam_params = [dict(params=head_params, lr=0.22), dict(params=embed_params, lr=0.6), dict(params=scalar_params, lr=0.04)]
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = torch.optim.Adam(adam_params, betas=(0.8, 0.95), eps=1e-10, fused=True)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then decay
def get_lr(step: int):
    x = step / args.num_iterations # progress in training
    assert 0 <= x < 1
    if x < 1 - args.cooldown_frac:
        return 1.0
    else:
        w = (1 - x) / args.cooldown_frac
        return w * 1.0 + (1 - w) * 0.1

# attention window size schedule: linearly increase
@lru_cache(1)
def get_window_size_blocks_helper(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)
def get_window_size_blocks(step: int):
    x = step / args.num_iterations # progress in training
    assert 0 <= x <= 1
    # Linearly increase the block-wise sliding window size over training 128 -> 1792
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * x, n=128)
    return get_window_size_blocks_helper(window_size)

model: nn.Module = torch.compile(model, dynamic=False)

########################################
#     Warmup kernel and Layerwise Bucketing   #
########################################

# Track the gradient readiness order during warmup
gradient_ready_order = []

def record_grad_order_hook(param):
    def hook(*_):
        if param.grad is not None and all(param is not p for p in gradient_ready_order):
            gradient_ready_order.append(param)
    return hook

# Register temp hooks to track when each parameter's gradient is ready
tmp_handles = []
for p in model.parameters():
    if p.requires_grad:
        handle = p.register_post_accumulate_grad_hook(record_grad_order_hook(p))
        tmp_handles.append(handle)

warmup_steps = 10
initial_state = dict(
    model=copy.deepcopy(model.state_dict()),
    optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]
)

for step in range(warmup_steps):
    # Create a local generator with fixed, step-dependent seed
    g = torch.Generator(device="cuda")
    g.manual_seed(1337 + step)

    # Use this generator to sample inputs
    inputs = targets = torch.randint(
        low=0,
        high=args.vocab_size,
        size=(args.train_seq_len,),
        device="cuda",
        dtype=torch.long,
        generator=g,  # <- Local generator ensures deterministic and isolated RNG
    )
    model(inputs.to(torch.int32), targets, get_window_size_blocks(0)).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)

# Remove hooks after warmup
for h in tmp_handles:
    h.remove()

# Restore model/optimizer state to ensure a clean start
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del initial_state

print0(f"Collected {len(gradient_ready_order)} parameters in gradient-ready order")

########################################
#      Overlap Communication Setup     #
########################################

# Layer-aware bucketing using gradient-ready order
def create_buckets_layerwise(ordered_params, bucket_size_mb=25):
    """Group parameters into buckets of ~bucket_size_mb MB, in gradient ready order"""
    buckets = []
    current_bucket = []
    current_size = 0

    for param in ordered_params:
        if not param.requires_grad:
            continue
        param_size_mb = param.numel() * param.element_size() / (1024 * 1024)

        if current_size + param_size_mb > bucket_size_mb and current_bucket:
            buckets.append(current_bucket)
            current_bucket = [param]
            current_size = param_size_mb
        else:
            current_bucket.append(param)
            current_size += param_size_mb

    if current_bucket:
        buckets.append(current_bucket)

    return buckets

# Create buckets for all parameters
all_params = [p for p in model.parameters() if p.requires_grad]
param_buckets = create_buckets_layerwise(gradient_ready_order)
print0(f"Created {len(param_buckets)} gradient buckets")
for i, bucket in enumerate(param_buckets):
    total_size = sum(p.numel() * p.element_size() for p in bucket) / (1024 * 1024)
    print0(f"Bucket {i}: {len(bucket)} params, {total_size:.1f} MB")

# Bucket state tracking
bucket_ready_count = [0] * len(param_buckets)
bucket_handles = [None] * len(param_buckets)
param_to_bucket = {}

# Map each parameter to its bucket index
for bucket_idx, bucket in enumerate(param_buckets):
    for param in bucket:
        param_to_bucket[param] = bucket_idx

def _gradient_hook(param: Tensor):
    """Called when a parameter's gradient is ready"""
    if param.grad is None:
        return

    bucket_idx = param_to_bucket[param]
    bucket_ready_count[bucket_idx] += 1

    # Check if all parameters in this bucket are ready
    if bucket_ready_count[bucket_idx] == len(param_buckets[bucket_idx]):
        # All-reduce this bucket
        bucket_grads = [p.grad for p in param_buckets[bucket_idx]]

        # For multi-tensor operations, we can reduce them together
        if len(bucket_grads) == 1:
            handle = dist.all_reduce(bucket_grads[0], op=dist.ReduceOp.AVG, async_op=True)
        else:
            # Use multi-tensor all-reduce for efficiency
            handle = dist.all_reduce_coalesced(bucket_grads, op=dist.ReduceOp.AVG, async_op=True)

        bucket_handles[bucket_idx] = handle

# Register hooks for all parameters
print0("Registering bucketed gradient hooks...")
for param in all_params:
    param.register_post_accumulate_grad_hook(_gradient_hook)

def wait_for_gradients():
    """Wait for all gradient reductions to complete and reset bucket state"""
    for handle in bucket_handles:
        if handle is not None:
            handle.wait()

    # Reset state for next iteration
    for i in range(len(bucket_ready_count)):
        bucket_ready_count[i] = 0
        bucket_handles[i] = None

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, world_size * args.train_seq_len, rank, world_size)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_batch_size = world_size * args.val_seq_len
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        val_loader = distributed_data_generator(args.val_files, val_batch_size, rank, world_size)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets = next(val_loader)
                val_loss += model(inputs, targets, get_window_size_blocks(step))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs_bucket/{run_id}", exist_ok=True)
            torch.save(log, f"logs_bucket/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    inputs, targets = next(train_loader)
    model(inputs, targets, get_window_size_blocks(step)).backward()
    #for param in model.parameters():
    #    dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
    wait_for_gradients() # does the same thing as commented two lines above, but faster

    # set optimization hyperparameters
    for opt in optimizers:
        for group in opt.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)
    for group in optimizer2.param_groups:
        frac = min(step / 300, 1) # momentum warmup for muon
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers
    for opt in optimizers:
        opt.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0]
Running PyTorch 2.8.0.dev20250612+cu126 compiled for CUDA 12.6
Fri Jun 13 06:22:57 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   38C    P0            119W /  700W |    5824MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   30C    P0            110W /  700W |    1514MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   29C    P0            113W /  700W |    1514MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   38C    P0            122W /  700W |    1514MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   39C    P0            118W /  700W |    1514MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   30C    P0            112W /  700W |    1514MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   38C    P0            119W /  700W |    1514MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   30C    P0            114W /  700W |    1514MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
Collected 75 parameters in gradient-ready order
Created 21 gradient buckets
Bucket 0: 1 params, 73.6 MB
Bucket 1: 1 params, 73.6 MB
Bucket 2: 4 params, 15.8 MB
Bucket 3: 1 params, 147.4 MB
Bucket 4: 4 params, 20.3 MB
Bucket 5: 4 params, 24.8 MB
Bucket 6: 4 params, 18.0 MB
Bucket 7: 5 params, 18.0 MB
Bucket 8: 4 params, 20.3 MB
Bucket 9: 5 params, 24.8 MB
Bucket 10: 4 params, 20.3 MB
Bucket 11: 4 params, 24.8 MB
Bucket 12: 5 params, 18.0 MB
Bucket 13: 5 params, 20.3 MB
Bucket 14: 5 params, 18.0 MB
Bucket 15: 4 params, 20.3 MB
Bucket 16: 5 params, 24.8 MB
Bucket 17: 4 params, 24.8 MB
Bucket 18: 4 params, 2.3 MB
Bucket 19: 1 params, 73.6 MB
Bucket 20: 1 params, 73.6 MB
Registering bucketed gradient hooks...
step:0/1770 val_loss:10.8258 train_time:0ms step_avg:0.02ms
step:1/1770 train_time:65ms step_avg:64.72ms
step:2/1770 train_time:141ms step_avg:70.50ms
step:3/1770 train_time:229ms step_avg:76.42ms
step:4/1770 train_time:322ms step_avg:80.55ms
step:5/1770 train_time:416ms step_avg:83.24ms
step:6/1770 train_time:510ms step_avg:85.03ms
step:7/1770 train_time:604ms step_avg:86.31ms
step:8/1770 train_time:699ms step_avg:87.32ms
step:9/1770 train_time:793ms step_avg:88.09ms
step:10/1770 train_time:888ms step_avg:88.80ms
step:11/1770 train_time:982ms step_avg:89.27ms
step:12/1770 train_time:1076ms step_avg:89.70ms
step:13/1770 train_time:1173ms step_avg:90.23ms
step:14/1770 train_time:1270ms step_avg:90.70ms
step:15/1770 train_time:1366ms step_avg:91.05ms
step:16/1770 train_time:1460ms step_avg:91.28ms
step:17/1770 train_time:1554ms step_avg:91.43ms
step:18/1770 train_time:1649ms step_avg:91.60ms
step:19/1770 train_time:1743ms step_avg:91.76ms
step:20/1770 train_time:1837ms step_avg:91.86ms
step:21/1770 train_time:1932ms step_avg:91.99ms
step:22/1770 train_time:2027ms step_avg:92.12ms
step:23/1770 train_time:2122ms step_avg:92.26ms
step:24/1770 train_time:2216ms step_avg:92.34ms
step:25/1770 train_time:2311ms step_avg:92.45ms
step:26/1770 train_time:2407ms step_avg:92.58ms
step:27/1770 train_time:2502ms step_avg:92.66ms
step:28/1770 train_time:2596ms step_avg:92.71ms
step:29/1770 train_time:2691ms step_avg:92.80ms
step:30/1770 train_time:2786ms step_avg:92.86ms
step:31/1770 train_time:2880ms step_avg:92.90ms
step:32/1770 train_time:2974ms step_avg:92.93ms
step:33/1770 train_time:3068ms step_avg:92.98ms
step:34/1770 train_time:3163ms step_avg:93.04ms
step:35/1770 train_time:3257ms step_avg:93.07ms
step:36/1770 train_time:3351ms step_avg:93.10ms
step:37/1770 train_time:3446ms step_avg:93.13ms
step:38/1770 train_time:3541ms step_avg:93.17ms
step:39/1770 train_time:3635ms step_avg:93.20ms
step:40/1770 train_time:3731ms step_avg:93.27ms
step:41/1770 train_time:3826ms step_avg:93.31ms
step:42/1770 train_time:3920ms step_avg:93.34ms
step:43/1770 train_time:4014ms step_avg:93.35ms
step:44/1770 train_time:4109ms step_avg:93.38ms
step:45/1770 train_time:4204ms step_avg:93.41ms
step:46/1770 train_time:4298ms step_avg:93.44ms
step:47/1770 train_time:4393ms step_avg:93.47ms
step:48/1770 train_time:4488ms step_avg:93.51ms
step:49/1770 train_time:4584ms step_avg:93.54ms
step:50/1770 train_time:4678ms step_avg:93.55ms
step:51/1770 train_time:4772ms step_avg:93.57ms
step:52/1770 train_time:4868ms step_avg:93.61ms
step:53/1770 train_time:4962ms step_avg:93.63ms
step:54/1770 train_time:5056ms step_avg:93.64ms
step:55/1770 train_time:5151ms step_avg:93.66ms
step:56/1770 train_time:5247ms step_avg:93.69ms
step:57/1770 train_time:5341ms step_avg:93.70ms
step:58/1770 train_time:5435ms step_avg:93.71ms
step:59/1770 train_time:5531ms step_avg:93.74ms
step:60/1770 train_time:5625ms step_avg:93.76ms
step:61/1770 train_time:5719ms step_avg:93.76ms
step:62/1770 train_time:5815ms step_avg:93.78ms
step:63/1770 train_time:5909ms step_avg:93.80ms
step:64/1770 train_time:6004ms step_avg:93.82ms
step:65/1770 train_time:6098ms step_avg:93.82ms
step:66/1770 train_time:6192ms step_avg:93.82ms
step:67/1770 train_time:6288ms step_avg:93.85ms
step:68/1770 train_time:6382ms step_avg:93.86ms
step:69/1770 train_time:6476ms step_avg:93.86ms
step:70/1770 train_time:6571ms step_avg:93.87ms
step:71/1770 train_time:6665ms step_avg:93.88ms
step:72/1770 train_time:6760ms step_avg:93.89ms
step:73/1770 train_time:6855ms step_avg:93.90ms
step:74/1770 train_time:6949ms step_avg:93.91ms
step:75/1770 train_time:7044ms step_avg:93.92ms
step:76/1770 train_time:7139ms step_avg:93.93ms
step:77/1770 train_time:7233ms step_avg:93.93ms
step:78/1770 train_time:7328ms step_avg:93.94ms
step:79/1770 train_time:7423ms step_avg:93.96ms
step:80/1770 train_time:7517ms step_avg:93.96ms
step:81/1770 train_time:7611ms step_avg:93.97ms
step:82/1770 train_time:7706ms step_avg:93.98ms
step:83/1770 train_time:7801ms step_avg:93.99ms
step:84/1770 train_time:7896ms step_avg:93.99ms
step:85/1770 train_time:7990ms step_avg:94.00ms
step:86/1770 train_time:8085ms step_avg:94.02ms
step:87/1770 train_time:8179ms step_avg:94.02ms
step:88/1770 train_time:8273ms step_avg:94.01ms
step:89/1770 train_time:8368ms step_avg:94.02ms
step:90/1770 train_time:8462ms step_avg:94.03ms
step:91/1770 train_time:8557ms step_avg:94.04ms
step:92/1770 train_time:8652ms step_avg:94.04ms
step:93/1770 train_time:8747ms step_avg:94.05ms
step:94/1770 train_time:8841ms step_avg:94.05ms
step:95/1770 train_time:8935ms step_avg:94.05ms
step:96/1770 train_time:9029ms step_avg:94.06ms
step:97/1770 train_time:9124ms step_avg:94.07ms
step:98/1770 train_time:9219ms step_avg:94.07ms
step:99/1770 train_time:9314ms step_avg:94.08ms
step:100/1770 train_time:9409ms step_avg:94.09ms
step:101/1770 train_time:9504ms step_avg:94.10ms
step:102/1770 train_time:9598ms step_avg:94.10ms
step:103/1770 train_time:9692ms step_avg:94.09ms
step:104/1770 train_time:9787ms step_avg:94.10ms
step:105/1770 train_time:9881ms step_avg:94.11ms
step:106/1770 train_time:9975ms step_avg:94.10ms
step:107/1770 train_time:10069ms step_avg:94.10ms
step:108/1770 train_time:10165ms step_avg:94.12ms
step:109/1770 train_time:10260ms step_avg:94.13ms
step:110/1770 train_time:10354ms step_avg:94.13ms
step:111/1770 train_time:10449ms step_avg:94.14ms
step:112/1770 train_time:10544ms step_avg:94.15ms
step:113/1770 train_time:10638ms step_avg:94.14ms
step:114/1770 train_time:10732ms step_avg:94.14ms
step:115/1770 train_time:10828ms step_avg:94.16ms
step:116/1770 train_time:10923ms step_avg:94.17ms
step:117/1770 train_time:11017ms step_avg:94.16ms
step:118/1770 train_time:11112ms step_avg:94.17ms
step:119/1770 train_time:11206ms step_avg:94.17ms
step:120/1770 train_time:11301ms step_avg:94.17ms
step:121/1770 train_time:11395ms step_avg:94.17ms
step:122/1770 train_time:11491ms step_avg:94.19ms
step:123/1770 train_time:11586ms step_avg:94.20ms
step:124/1770 train_time:11680ms step_avg:94.19ms
step:125/1770 train_time:11774ms step_avg:94.19ms
step:125/1770 val_loss:4.6503 train_time:11868ms step_avg:94.94ms
step:126/1770 train_time:11887ms step_avg:94.34ms
step:127/1770 train_time:11966ms step_avg:94.22ms
step:128/1770 train_time:12064ms step_avg:94.25ms
step:129/1770 train_time:12165ms step_avg:94.30ms
step:130/1770 train_time:12261ms step_avg:94.31ms
step:131/1770 train_time:12355ms step_avg:94.31ms
step:132/1770 train_time:12449ms step_avg:94.31ms
step:133/1770 train_time:12543ms step_avg:94.30ms
step:134/1770 train_time:12637ms step_avg:94.31ms
step:135/1770 train_time:12731ms step_avg:94.31ms
step:136/1770 train_time:12827ms step_avg:94.31ms
step:137/1770 train_time:12922ms step_avg:94.32ms
step:138/1770 train_time:13017ms step_avg:94.33ms
step:139/1770 train_time:13112ms step_avg:94.33ms
step:140/1770 train_time:13209ms step_avg:94.35ms
step:141/1770 train_time:13306ms step_avg:94.37ms
step:142/1770 train_time:13402ms step_avg:94.38ms
step:143/1770 train_time:13496ms step_avg:94.38ms
step:144/1770 train_time:13591ms step_avg:94.38ms
step:145/1770 train_time:13686ms step_avg:94.38ms
step:146/1770 train_time:13781ms step_avg:94.39ms
step:147/1770 train_time:13876ms step_avg:94.39ms
step:148/1770 train_time:13971ms step_avg:94.40ms
step:149/1770 train_time:14067ms step_avg:94.41ms
step:150/1770 train_time:14162ms step_avg:94.41ms
step:151/1770 train_time:14258ms step_avg:94.42ms
step:152/1770 train_time:14353ms step_avg:94.43ms
step:153/1770 train_time:14449ms step_avg:94.44ms
step:154/1770 train_time:14545ms step_avg:94.45ms
step:155/1770 train_time:14640ms step_avg:94.45ms
step:156/1770 train_time:14735ms step_avg:94.46ms
step:157/1770 train_time:14830ms step_avg:94.46ms
step:158/1770 train_time:14925ms step_avg:94.46ms
step:159/1770 train_time:15020ms step_avg:94.47ms
step:160/1770 train_time:15115ms step_avg:94.47ms
step:161/1770 train_time:15210ms step_avg:94.47ms
step:162/1770 train_time:15306ms step_avg:94.48ms
step:163/1770 train_time:15402ms step_avg:94.49ms
step:164/1770 train_time:15497ms step_avg:94.49ms
step:165/1770 train_time:15591ms step_avg:94.49ms
step:166/1770 train_time:15686ms step_avg:94.50ms
step:167/1770 train_time:15782ms step_avg:94.50ms
step:168/1770 train_time:15878ms step_avg:94.51ms
step:169/1770 train_time:15972ms step_avg:94.51ms
step:170/1770 train_time:16068ms step_avg:94.52ms
step:171/1770 train_time:16163ms step_avg:94.52ms
step:172/1770 train_time:16258ms step_avg:94.52ms
step:173/1770 train_time:16353ms step_avg:94.53ms
step:174/1770 train_time:16449ms step_avg:94.54ms
step:175/1770 train_time:16546ms step_avg:94.55ms
step:176/1770 train_time:16641ms step_avg:94.55ms
step:177/1770 train_time:16736ms step_avg:94.55ms
step:178/1770 train_time:16831ms step_avg:94.55ms
step:179/1770 train_time:16926ms step_avg:94.56ms
step:180/1770 train_time:17022ms step_avg:94.56ms
step:181/1770 train_time:17116ms step_avg:94.56ms
step:182/1770 train_time:17211ms step_avg:94.56ms
step:183/1770 train_time:17306ms step_avg:94.57ms
step:184/1770 train_time:17403ms step_avg:94.58ms
step:185/1770 train_time:17498ms step_avg:94.58ms
step:186/1770 train_time:17593ms step_avg:94.59ms
step:187/1770 train_time:17689ms step_avg:94.59ms
step:188/1770 train_time:17785ms step_avg:94.60ms
step:189/1770 train_time:17880ms step_avg:94.60ms
step:190/1770 train_time:17974ms step_avg:94.60ms
step:191/1770 train_time:18070ms step_avg:94.61ms
step:192/1770 train_time:18166ms step_avg:94.61ms
step:193/1770 train_time:18261ms step_avg:94.62ms
step:194/1770 train_time:18357ms step_avg:94.62ms
step:195/1770 train_time:18451ms step_avg:94.62ms
step:196/1770 train_time:18547ms step_avg:94.63ms
step:197/1770 train_time:18642ms step_avg:94.63ms
step:198/1770 train_time:18738ms step_avg:94.63ms
step:199/1770 train_time:18833ms step_avg:94.64ms
step:200/1770 train_time:18929ms step_avg:94.65ms
step:201/1770 train_time:19025ms step_avg:94.65ms
step:202/1770 train_time:19120ms step_avg:94.65ms
step:203/1770 train_time:19214ms step_avg:94.65ms
step:204/1770 train_time:19309ms step_avg:94.65ms
step:205/1770 train_time:19405ms step_avg:94.66ms
step:206/1770 train_time:19500ms step_avg:94.66ms
step:207/1770 train_time:19595ms step_avg:94.66ms
step:208/1770 train_time:19690ms step_avg:94.66ms
step:209/1770 train_time:19786ms step_avg:94.67ms
step:210/1770 train_time:19881ms step_avg:94.67ms
step:211/1770 train_time:19976ms step_avg:94.67ms
step:212/1770 train_time:20071ms step_avg:94.68ms
step:213/1770 train_time:20167ms step_avg:94.68ms
step:214/1770 train_time:20262ms step_avg:94.68ms
step:215/1770 train_time:20357ms step_avg:94.68ms
step:216/1770 train_time:20453ms step_avg:94.69ms
step:217/1770 train_time:20549ms step_avg:94.70ms
step:218/1770 train_time:20645ms step_avg:94.70ms
step:219/1770 train_time:20741ms step_avg:94.71ms
step:220/1770 train_time:20836ms step_avg:94.71ms
step:221/1770 train_time:20931ms step_avg:94.71ms
step:222/1770 train_time:21026ms step_avg:94.71ms
step:223/1770 train_time:21122ms step_avg:94.72ms
step:224/1770 train_time:21216ms step_avg:94.72ms
step:225/1770 train_time:21311ms step_avg:94.72ms
step:226/1770 train_time:21407ms step_avg:94.72ms
step:227/1770 train_time:21502ms step_avg:94.72ms
step:228/1770 train_time:21597ms step_avg:94.73ms
step:229/1770 train_time:21693ms step_avg:94.73ms
step:230/1770 train_time:21789ms step_avg:94.73ms
step:231/1770 train_time:21886ms step_avg:94.74ms
step:232/1770 train_time:21981ms step_avg:94.75ms
step:233/1770 train_time:22076ms step_avg:94.75ms
step:234/1770 train_time:22171ms step_avg:94.75ms
step:235/1770 train_time:22266ms step_avg:94.75ms
step:236/1770 train_time:22361ms step_avg:94.75ms
step:237/1770 train_time:22456ms step_avg:94.75ms
step:238/1770 train_time:22551ms step_avg:94.75ms
step:239/1770 train_time:22647ms step_avg:94.76ms
step:240/1770 train_time:22742ms step_avg:94.76ms
step:241/1770 train_time:22838ms step_avg:94.76ms
step:242/1770 train_time:22934ms step_avg:94.77ms
step:243/1770 train_time:23029ms step_avg:94.77ms
step:244/1770 train_time:23125ms step_avg:94.77ms
step:245/1770 train_time:23219ms step_avg:94.77ms
step:246/1770 train_time:23314ms step_avg:94.77ms
step:247/1770 train_time:23409ms step_avg:94.77ms
step:248/1770 train_time:23504ms step_avg:94.77ms
step:249/1770 train_time:23599ms step_avg:94.78ms
step:250/1770 train_time:23694ms step_avg:94.77ms
step:250/1770 val_loss:4.1008 train_time:23789ms step_avg:95.15ms
step:251/1770 train_time:23807ms step_avg:94.85ms
step:252/1770 train_time:23891ms step_avg:94.81ms
step:253/1770 train_time:23989ms step_avg:94.82ms
step:254/1770 train_time:24085ms step_avg:94.82ms
step:255/1770 train_time:24179ms step_avg:94.82ms
step:256/1770 train_time:24274ms step_avg:94.82ms
step:257/1770 train_time:24368ms step_avg:94.82ms
step:258/1770 train_time:24463ms step_avg:94.82ms
step:259/1770 train_time:24558ms step_avg:94.82ms
step:260/1770 train_time:24652ms step_avg:94.82ms
step:261/1770 train_time:24747ms step_avg:94.81ms
step:262/1770 train_time:24842ms step_avg:94.82ms
step:263/1770 train_time:24939ms step_avg:94.83ms
step:264/1770 train_time:25036ms step_avg:94.83ms
step:265/1770 train_time:25133ms step_avg:94.84ms
step:266/1770 train_time:25228ms step_avg:94.84ms
step:267/1770 train_time:25323ms step_avg:94.84ms
step:268/1770 train_time:25418ms step_avg:94.84ms
step:269/1770 train_time:25514ms step_avg:94.85ms
step:270/1770 train_time:25609ms step_avg:94.85ms
step:271/1770 train_time:25704ms step_avg:94.85ms
step:272/1770 train_time:25800ms step_avg:94.85ms
step:273/1770 train_time:25897ms step_avg:94.86ms
step:274/1770 train_time:25994ms step_avg:94.87ms
step:275/1770 train_time:26090ms step_avg:94.87ms
step:276/1770 train_time:26186ms step_avg:94.88ms
step:277/1770 train_time:26281ms step_avg:94.88ms
step:278/1770 train_time:26377ms step_avg:94.88ms
step:279/1770 train_time:26473ms step_avg:94.89ms
step:280/1770 train_time:26569ms step_avg:94.89ms
step:281/1770 train_time:26663ms step_avg:94.89ms
step:282/1770 train_time:26759ms step_avg:94.89ms
step:283/1770 train_time:26854ms step_avg:94.89ms
step:284/1770 train_time:26950ms step_avg:94.89ms
step:285/1770 train_time:27045ms step_avg:94.90ms
step:286/1770 train_time:27141ms step_avg:94.90ms
step:287/1770 train_time:27237ms step_avg:94.90ms
step:288/1770 train_time:27333ms step_avg:94.91ms
step:289/1770 train_time:27428ms step_avg:94.91ms
step:290/1770 train_time:27524ms step_avg:94.91ms
step:291/1770 train_time:27619ms step_avg:94.91ms
step:292/1770 train_time:27715ms step_avg:94.91ms
step:293/1770 train_time:27810ms step_avg:94.91ms
step:294/1770 train_time:27905ms step_avg:94.92ms
step:295/1770 train_time:28001ms step_avg:94.92ms
step:296/1770 train_time:28098ms step_avg:94.93ms
step:297/1770 train_time:28194ms step_avg:94.93ms
step:298/1770 train_time:28289ms step_avg:94.93ms
step:299/1770 train_time:28385ms step_avg:94.93ms
step:300/1770 train_time:28480ms step_avg:94.93ms
step:301/1770 train_time:28576ms step_avg:94.94ms
step:302/1770 train_time:28671ms step_avg:94.94ms
step:303/1770 train_time:28766ms step_avg:94.94ms
step:304/1770 train_time:28862ms step_avg:94.94ms
step:305/1770 train_time:28958ms step_avg:94.94ms
step:306/1770 train_time:29054ms step_avg:94.95ms
step:307/1770 train_time:29149ms step_avg:94.95ms
step:308/1770 train_time:29244ms step_avg:94.95ms
step:309/1770 train_time:29340ms step_avg:94.95ms
step:310/1770 train_time:29436ms step_avg:94.95ms
step:311/1770 train_time:29532ms step_avg:94.96ms
step:312/1770 train_time:29626ms step_avg:94.96ms
step:313/1770 train_time:29721ms step_avg:94.96ms
step:314/1770 train_time:29817ms step_avg:94.96ms
step:315/1770 train_time:29913ms step_avg:94.96ms
step:316/1770 train_time:30008ms step_avg:94.96ms
step:317/1770 train_time:30103ms step_avg:94.96ms
step:318/1770 train_time:30199ms step_avg:94.97ms
step:319/1770 train_time:30296ms step_avg:94.97ms
step:320/1770 train_time:30391ms step_avg:94.97ms
step:321/1770 train_time:30486ms step_avg:94.97ms
step:322/1770 train_time:30581ms step_avg:94.97ms
step:323/1770 train_time:30677ms step_avg:94.97ms
step:324/1770 train_time:30773ms step_avg:94.98ms
step:325/1770 train_time:30869ms step_avg:94.98ms
step:326/1770 train_time:30964ms step_avg:94.98ms
step:327/1770 train_time:31060ms step_avg:94.98ms
step:328/1770 train_time:31156ms step_avg:94.99ms
step:329/1770 train_time:31252ms step_avg:94.99ms
step:330/1770 train_time:31347ms step_avg:94.99ms
step:331/1770 train_time:31443ms step_avg:94.99ms
step:332/1770 train_time:31539ms step_avg:95.00ms
step:333/1770 train_time:31634ms step_avg:95.00ms
step:334/1770 train_time:31729ms step_avg:95.00ms
step:335/1770 train_time:31824ms step_avg:95.00ms
step:336/1770 train_time:31920ms step_avg:95.00ms
step:337/1770 train_time:32016ms step_avg:95.00ms
step:338/1770 train_time:32111ms step_avg:95.00ms
step:339/1770 train_time:32206ms step_avg:95.00ms
step:340/1770 train_time:32302ms step_avg:95.01ms
step:341/1770 train_time:32399ms step_avg:95.01ms
step:342/1770 train_time:32496ms step_avg:95.02ms
step:343/1770 train_time:32591ms step_avg:95.02ms
step:344/1770 train_time:32686ms step_avg:95.02ms
step:345/1770 train_time:32781ms step_avg:95.02ms
step:346/1770 train_time:32878ms step_avg:95.02ms
step:347/1770 train_time:32973ms step_avg:95.02ms
step:348/1770 train_time:33068ms step_avg:95.02ms
step:349/1770 train_time:33164ms step_avg:95.02ms
step:350/1770 train_time:33260ms step_avg:95.03ms
step:351/1770 train_time:33356ms step_avg:95.03ms
step:352/1770 train_time:33452ms step_avg:95.03ms
step:353/1770 train_time:33547ms step_avg:95.03ms
step:354/1770 train_time:33643ms step_avg:95.04ms
step:355/1770 train_time:33738ms step_avg:95.04ms
step:356/1770 train_time:33835ms step_avg:95.04ms
step:357/1770 train_time:33930ms step_avg:95.04ms
step:358/1770 train_time:34025ms step_avg:95.04ms
step:359/1770 train_time:34120ms step_avg:95.04ms
step:360/1770 train_time:34217ms step_avg:95.05ms
step:361/1770 train_time:34314ms step_avg:95.05ms
step:362/1770 train_time:34410ms step_avg:95.06ms
step:363/1770 train_time:34506ms step_avg:95.06ms
step:364/1770 train_time:34602ms step_avg:95.06ms
step:365/1770 train_time:34698ms step_avg:95.06ms
step:366/1770 train_time:34793ms step_avg:95.06ms
step:367/1770 train_time:34889ms step_avg:95.06ms
step:368/1770 train_time:34984ms step_avg:95.07ms
step:369/1770 train_time:35080ms step_avg:95.07ms
step:370/1770 train_time:35176ms step_avg:95.07ms
step:371/1770 train_time:35271ms step_avg:95.07ms
step:372/1770 train_time:35367ms step_avg:95.07ms
step:373/1770 train_time:35463ms step_avg:95.07ms
step:374/1770 train_time:35559ms step_avg:95.08ms
step:375/1770 train_time:35654ms step_avg:95.08ms
step:375/1770 val_loss:3.8934 train_time:35749ms step_avg:95.33ms
step:376/1770 train_time:35767ms step_avg:95.12ms
step:377/1770 train_time:35852ms step_avg:95.10ms
step:378/1770 train_time:35954ms step_avg:95.12ms
step:379/1770 train_time:36049ms step_avg:95.12ms
step:380/1770 train_time:36144ms step_avg:95.12ms
step:381/1770 train_time:36239ms step_avg:95.12ms
step:382/1770 train_time:36334ms step_avg:95.12ms
step:383/1770 train_time:36429ms step_avg:95.11ms
step:384/1770 train_time:36524ms step_avg:95.11ms
step:385/1770 train_time:36619ms step_avg:95.11ms
step:386/1770 train_time:36713ms step_avg:95.11ms
step:387/1770 train_time:36809ms step_avg:95.11ms
step:388/1770 train_time:36907ms step_avg:95.12ms
step:389/1770 train_time:37005ms step_avg:95.13ms
step:390/1770 train_time:37101ms step_avg:95.13ms
step:391/1770 train_time:37197ms step_avg:95.13ms
step:392/1770 train_time:37292ms step_avg:95.13ms
step:393/1770 train_time:37387ms step_avg:95.13ms
step:394/1770 train_time:37482ms step_avg:95.13ms
step:395/1770 train_time:37577ms step_avg:95.13ms
step:396/1770 train_time:37674ms step_avg:95.14ms
step:397/1770 train_time:37771ms step_avg:95.14ms
step:398/1770 train_time:37869ms step_avg:95.15ms
step:399/1770 train_time:37968ms step_avg:95.16ms
step:400/1770 train_time:38068ms step_avg:95.17ms
step:401/1770 train_time:38167ms step_avg:95.18ms
step:402/1770 train_time:38266ms step_avg:95.19ms
step:403/1770 train_time:38364ms step_avg:95.19ms
step:404/1770 train_time:38462ms step_avg:95.20ms
step:405/1770 train_time:38559ms step_avg:95.21ms
step:406/1770 train_time:38657ms step_avg:95.21ms
step:407/1770 train_time:38754ms step_avg:95.22ms
step:408/1770 train_time:38851ms step_avg:95.22ms
step:409/1770 train_time:38949ms step_avg:95.23ms
step:410/1770 train_time:39047ms step_avg:95.24ms
step:411/1770 train_time:39145ms step_avg:95.24ms
step:412/1770 train_time:39244ms step_avg:95.25ms
step:413/1770 train_time:39342ms step_avg:95.26ms
step:414/1770 train_time:39440ms step_avg:95.27ms
step:415/1770 train_time:39539ms step_avg:95.27ms
step:416/1770 train_time:39637ms step_avg:95.28ms
step:417/1770 train_time:39735ms step_avg:95.29ms
step:418/1770 train_time:39832ms step_avg:95.29ms
step:419/1770 train_time:39929ms step_avg:95.30ms
step:420/1770 train_time:40027ms step_avg:95.30ms
step:421/1770 train_time:40125ms step_avg:95.31ms
step:422/1770 train_time:40223ms step_avg:95.31ms
step:423/1770 train_time:40321ms step_avg:95.32ms
step:424/1770 train_time:40418ms step_avg:95.32ms
step:425/1770 train_time:40516ms step_avg:95.33ms
step:426/1770 train_time:40614ms step_avg:95.34ms
step:427/1770 train_time:40712ms step_avg:95.34ms
step:428/1770 train_time:40810ms step_avg:95.35ms
step:429/1770 train_time:40908ms step_avg:95.36ms
step:430/1770 train_time:41005ms step_avg:95.36ms
step:431/1770 train_time:41103ms step_avg:95.37ms
step:432/1770 train_time:41200ms step_avg:95.37ms
step:433/1770 train_time:41298ms step_avg:95.38ms
step:434/1770 train_time:41395ms step_avg:95.38ms
step:435/1770 train_time:41493ms step_avg:95.39ms
step:436/1770 train_time:41592ms step_avg:95.39ms
step:437/1770 train_time:41690ms step_avg:95.40ms
step:438/1770 train_time:41788ms step_avg:95.41ms
step:439/1770 train_time:41886ms step_avg:95.41ms
step:440/1770 train_time:41985ms step_avg:95.42ms
step:441/1770 train_time:42083ms step_avg:95.43ms
step:442/1770 train_time:42181ms step_avg:95.43ms
step:443/1770 train_time:42278ms step_avg:95.44ms
step:444/1770 train_time:42376ms step_avg:95.44ms
step:445/1770 train_time:42474ms step_avg:95.45ms
step:446/1770 train_time:42572ms step_avg:95.45ms
step:447/1770 train_time:42670ms step_avg:95.46ms
step:448/1770 train_time:42768ms step_avg:95.46ms
step:449/1770 train_time:42866ms step_avg:95.47ms
step:450/1770 train_time:42964ms step_avg:95.48ms
step:451/1770 train_time:43062ms step_avg:95.48ms
step:452/1770 train_time:43161ms step_avg:95.49ms
step:453/1770 train_time:43259ms step_avg:95.49ms
step:454/1770 train_time:43357ms step_avg:95.50ms
step:455/1770 train_time:43455ms step_avg:95.51ms
step:456/1770 train_time:43553ms step_avg:95.51ms
step:457/1770 train_time:43651ms step_avg:95.52ms
step:458/1770 train_time:43749ms step_avg:95.52ms
step:459/1770 train_time:43847ms step_avg:95.53ms
step:460/1770 train_time:43945ms step_avg:95.53ms
step:461/1770 train_time:44043ms step_avg:95.54ms
step:462/1770 train_time:44141ms step_avg:95.54ms
step:463/1770 train_time:44239ms step_avg:95.55ms
step:464/1770 train_time:44336ms step_avg:95.55ms
step:465/1770 train_time:44434ms step_avg:95.56ms
step:466/1770 train_time:44531ms step_avg:95.56ms
step:467/1770 train_time:44630ms step_avg:95.57ms
step:468/1770 train_time:44728ms step_avg:95.57ms
step:469/1770 train_time:44826ms step_avg:95.58ms
step:470/1770 train_time:44925ms step_avg:95.58ms
step:471/1770 train_time:45023ms step_avg:95.59ms
step:472/1770 train_time:45121ms step_avg:95.60ms
step:473/1770 train_time:45219ms step_avg:95.60ms
step:474/1770 train_time:45317ms step_avg:95.60ms
step:475/1770 train_time:45414ms step_avg:95.61ms
step:476/1770 train_time:45511ms step_avg:95.61ms
step:477/1770 train_time:45610ms step_avg:95.62ms
step:478/1770 train_time:45708ms step_avg:95.62ms
step:479/1770 train_time:45805ms step_avg:95.63ms
step:480/1770 train_time:45903ms step_avg:95.63ms
step:481/1770 train_time:46002ms step_avg:95.64ms
step:482/1770 train_time:46101ms step_avg:95.64ms
step:483/1770 train_time:46198ms step_avg:95.65ms
step:484/1770 train_time:46296ms step_avg:95.65ms
step:485/1770 train_time:46394ms step_avg:95.66ms
step:486/1770 train_time:46492ms step_avg:95.66ms
step:487/1770 train_time:46589ms step_avg:95.67ms
step:488/1770 train_time:46687ms step_avg:95.67ms
step:489/1770 train_time:46786ms step_avg:95.68ms
step:490/1770 train_time:46884ms step_avg:95.68ms
step:491/1770 train_time:46982ms step_avg:95.69ms
step:492/1770 train_time:47081ms step_avg:95.69ms
step:493/1770 train_time:47180ms step_avg:95.70ms
step:494/1770 train_time:47278ms step_avg:95.71ms
step:495/1770 train_time:47376ms step_avg:95.71ms
step:496/1770 train_time:47473ms step_avg:95.71ms
step:497/1770 train_time:47571ms step_avg:95.72ms
step:498/1770 train_time:47668ms step_avg:95.72ms
step:499/1770 train_time:47767ms step_avg:95.73ms
step:500/1770 train_time:47866ms step_avg:95.73ms
step:500/1770 val_loss:3.7464 train_time:47963ms step_avg:95.93ms
step:501/1770 train_time:47981ms step_avg:95.77ms
step:502/1770 train_time:48071ms step_avg:95.76ms
step:503/1770 train_time:48171ms step_avg:95.77ms
step:504/1770 train_time:48269ms step_avg:95.77ms
step:505/1770 train_time:48367ms step_avg:95.78ms
step:506/1770 train_time:48464ms step_avg:95.78ms
step:507/1770 train_time:48561ms step_avg:95.78ms
step:508/1770 train_time:48659ms step_avg:95.79ms
step:509/1770 train_time:48757ms step_avg:95.79ms
step:510/1770 train_time:48854ms step_avg:95.79ms
step:511/1770 train_time:48953ms step_avg:95.80ms
step:512/1770 train_time:49052ms step_avg:95.80ms
step:513/1770 train_time:49151ms step_avg:95.81ms
step:514/1770 train_time:49250ms step_avg:95.82ms
step:515/1770 train_time:49348ms step_avg:95.82ms
step:516/1770 train_time:49446ms step_avg:95.83ms
step:517/1770 train_time:49544ms step_avg:95.83ms
step:518/1770 train_time:49641ms step_avg:95.83ms
step:519/1770 train_time:49739ms step_avg:95.84ms
step:520/1770 train_time:49837ms step_avg:95.84ms
step:521/1770 train_time:49935ms step_avg:95.84ms
step:522/1770 train_time:50033ms step_avg:95.85ms
step:523/1770 train_time:50131ms step_avg:95.85ms
step:524/1770 train_time:50229ms step_avg:95.86ms
step:525/1770 train_time:50327ms step_avg:95.86ms
step:526/1770 train_time:50425ms step_avg:95.86ms
step:527/1770 train_time:50523ms step_avg:95.87ms
step:528/1770 train_time:50621ms step_avg:95.87ms
step:529/1770 train_time:50719ms step_avg:95.88ms
step:530/1770 train_time:50817ms step_avg:95.88ms
step:531/1770 train_time:50915ms step_avg:95.89ms
step:532/1770 train_time:51014ms step_avg:95.89ms
step:533/1770 train_time:51114ms step_avg:95.90ms
step:534/1770 train_time:51214ms step_avg:95.91ms
step:535/1770 train_time:51315ms step_avg:95.92ms
step:536/1770 train_time:51416ms step_avg:95.93ms
step:537/1770 train_time:51515ms step_avg:95.93ms
step:538/1770 train_time:51614ms step_avg:95.94ms
step:539/1770 train_time:51713ms step_avg:95.94ms
step:540/1770 train_time:51812ms step_avg:95.95ms
step:541/1770 train_time:51909ms step_avg:95.95ms
step:542/1770 train_time:52007ms step_avg:95.95ms
step:543/1770 train_time:52105ms step_avg:95.96ms
step:544/1770 train_time:52203ms step_avg:95.96ms
step:545/1770 train_time:52302ms step_avg:95.97ms
step:546/1770 train_time:52400ms step_avg:95.97ms
step:547/1770 train_time:52499ms step_avg:95.98ms
step:548/1770 train_time:52598ms step_avg:95.98ms
step:549/1770 train_time:52697ms step_avg:95.99ms
step:550/1770 train_time:52797ms step_avg:95.99ms
step:551/1770 train_time:52896ms step_avg:96.00ms
step:552/1770 train_time:52995ms step_avg:96.01ms
step:553/1770 train_time:53094ms step_avg:96.01ms
step:554/1770 train_time:53194ms step_avg:96.02ms
step:555/1770 train_time:53293ms step_avg:96.02ms
step:556/1770 train_time:53392ms step_avg:96.03ms
step:557/1770 train_time:53491ms step_avg:96.03ms
step:558/1770 train_time:53589ms step_avg:96.04ms
step:559/1770 train_time:53687ms step_avg:96.04ms
step:560/1770 train_time:53786ms step_avg:96.05ms
step:561/1770 train_time:53884ms step_avg:96.05ms
step:562/1770 train_time:53982ms step_avg:96.05ms
step:563/1770 train_time:54080ms step_avg:96.06ms
step:564/1770 train_time:54178ms step_avg:96.06ms
step:565/1770 train_time:54278ms step_avg:96.07ms
step:566/1770 train_time:54376ms step_avg:96.07ms
step:567/1770 train_time:54475ms step_avg:96.08ms
step:568/1770 train_time:54574ms step_avg:96.08ms
step:569/1770 train_time:54673ms step_avg:96.09ms
step:570/1770 train_time:54772ms step_avg:96.09ms
step:571/1770 train_time:54871ms step_avg:96.10ms
step:572/1770 train_time:54970ms step_avg:96.10ms
step:573/1770 train_time:55068ms step_avg:96.10ms
step:574/1770 train_time:55167ms step_avg:96.11ms
step:575/1770 train_time:55265ms step_avg:96.11ms
step:576/1770 train_time:55364ms step_avg:96.12ms
step:577/1770 train_time:55461ms step_avg:96.12ms
step:578/1770 train_time:55559ms step_avg:96.12ms
step:579/1770 train_time:55657ms step_avg:96.13ms
step:580/1770 train_time:55755ms step_avg:96.13ms
step:581/1770 train_time:55853ms step_avg:96.13ms
step:582/1770 train_time:55952ms step_avg:96.14ms
step:583/1770 train_time:56052ms step_avg:96.14ms
step:584/1770 train_time:56151ms step_avg:96.15ms
step:585/1770 train_time:56250ms step_avg:96.15ms
step:586/1770 train_time:56349ms step_avg:96.16ms
step:587/1770 train_time:56447ms step_avg:96.16ms
step:588/1770 train_time:56545ms step_avg:96.17ms
step:589/1770 train_time:56643ms step_avg:96.17ms
step:590/1770 train_time:56741ms step_avg:96.17ms
step:591/1770 train_time:56838ms step_avg:96.17ms
step:592/1770 train_time:56936ms step_avg:96.18ms
step:593/1770 train_time:57036ms step_avg:96.18ms
step:594/1770 train_time:57135ms step_avg:96.19ms
step:595/1770 train_time:57234ms step_avg:96.19ms
step:596/1770 train_time:57333ms step_avg:96.20ms
step:597/1770 train_time:57432ms step_avg:96.20ms
step:598/1770 train_time:57532ms step_avg:96.21ms
step:599/1770 train_time:57630ms step_avg:96.21ms
step:600/1770 train_time:57729ms step_avg:96.22ms
step:601/1770 train_time:57827ms step_avg:96.22ms
step:602/1770 train_time:57925ms step_avg:96.22ms
step:603/1770 train_time:58023ms step_avg:96.22ms
step:604/1770 train_time:58120ms step_avg:96.23ms
step:605/1770 train_time:58218ms step_avg:96.23ms
step:606/1770 train_time:58317ms step_avg:96.23ms
step:607/1770 train_time:58416ms step_avg:96.24ms
step:608/1770 train_time:58515ms step_avg:96.24ms
step:609/1770 train_time:58615ms step_avg:96.25ms
step:610/1770 train_time:58714ms step_avg:96.25ms
step:611/1770 train_time:58814ms step_avg:96.26ms
step:612/1770 train_time:58914ms step_avg:96.27ms
step:613/1770 train_time:59013ms step_avg:96.27ms
step:614/1770 train_time:59112ms step_avg:96.27ms
step:615/1770 train_time:59211ms step_avg:96.28ms
step:616/1770 train_time:59309ms step_avg:96.28ms
step:617/1770 train_time:59408ms step_avg:96.28ms
step:618/1770 train_time:59506ms step_avg:96.29ms
step:619/1770 train_time:59605ms step_avg:96.29ms
step:620/1770 train_time:59704ms step_avg:96.30ms
step:621/1770 train_time:59802ms step_avg:96.30ms
step:622/1770 train_time:59900ms step_avg:96.30ms
step:623/1770 train_time:59998ms step_avg:96.30ms
step:624/1770 train_time:60096ms step_avg:96.31ms
step:625/1770 train_time:60195ms step_avg:96.31ms
step:625/1770 val_loss:3.6626 train_time:60293ms step_avg:96.47ms
step:626/1770 train_time:60311ms step_avg:96.34ms
step:627/1770 train_time:60400ms step_avg:96.33ms
step:628/1770 train_time:60500ms step_avg:96.34ms
step:629/1770 train_time:60599ms step_avg:96.34ms
step:630/1770 train_time:60698ms step_avg:96.35ms
step:631/1770 train_time:60797ms step_avg:96.35ms
step:632/1770 train_time:60894ms step_avg:96.35ms
step:633/1770 train_time:60991ms step_avg:96.35ms
step:634/1770 train_time:61089ms step_avg:96.35ms
step:635/1770 train_time:61186ms step_avg:96.36ms
step:636/1770 train_time:61284ms step_avg:96.36ms
step:637/1770 train_time:61384ms step_avg:96.36ms
step:638/1770 train_time:61483ms step_avg:96.37ms
step:639/1770 train_time:61581ms step_avg:96.37ms
step:640/1770 train_time:61681ms step_avg:96.38ms
step:641/1770 train_time:61780ms step_avg:96.38ms
step:642/1770 train_time:61879ms step_avg:96.38ms
step:643/1770 train_time:61977ms step_avg:96.39ms
step:644/1770 train_time:62075ms step_avg:96.39ms
step:645/1770 train_time:62174ms step_avg:96.39ms
step:646/1770 train_time:62272ms step_avg:96.40ms
step:647/1770 train_time:62370ms step_avg:96.40ms
step:648/1770 train_time:62469ms step_avg:96.40ms
step:649/1770 train_time:62567ms step_avg:96.40ms
step:650/1770 train_time:62665ms step_avg:96.41ms
step:651/1770 train_time:62763ms step_avg:96.41ms
step:652/1770 train_time:62861ms step_avg:96.41ms
step:653/1770 train_time:62960ms step_avg:96.42ms
step:654/1770 train_time:63059ms step_avg:96.42ms
step:655/1770 train_time:63158ms step_avg:96.42ms
step:656/1770 train_time:63257ms step_avg:96.43ms
step:657/1770 train_time:63355ms step_avg:96.43ms
step:658/1770 train_time:63456ms step_avg:96.44ms
step:659/1770 train_time:63557ms step_avg:96.45ms
step:660/1770 train_time:63659ms step_avg:96.45ms
step:661/1770 train_time:63760ms step_avg:96.46ms
step:662/1770 train_time:63861ms step_avg:96.47ms
step:663/1770 train_time:63961ms step_avg:96.47ms
step:664/1770 train_time:64062ms step_avg:96.48ms
step:665/1770 train_time:64162ms step_avg:96.48ms
step:666/1770 train_time:64263ms step_avg:96.49ms
step:667/1770 train_time:64364ms step_avg:96.50ms
step:668/1770 train_time:64465ms step_avg:96.50ms
step:669/1770 train_time:64566ms step_avg:96.51ms
step:670/1770 train_time:64666ms step_avg:96.52ms
step:671/1770 train_time:64767ms step_avg:96.52ms
step:672/1770 train_time:64867ms step_avg:96.53ms
step:673/1770 train_time:64966ms step_avg:96.53ms
step:674/1770 train_time:65065ms step_avg:96.54ms
step:675/1770 train_time:65165ms step_avg:96.54ms
step:676/1770 train_time:65266ms step_avg:96.55ms
step:677/1770 train_time:65366ms step_avg:96.55ms
step:678/1770 train_time:65467ms step_avg:96.56ms
step:679/1770 train_time:65567ms step_avg:96.56ms
step:680/1770 train_time:65666ms step_avg:96.57ms
step:681/1770 train_time:65766ms step_avg:96.57ms
step:682/1770 train_time:65866ms step_avg:96.58ms
step:683/1770 train_time:65965ms step_avg:96.58ms
step:684/1770 train_time:66065ms step_avg:96.59ms
step:685/1770 train_time:66165ms step_avg:96.59ms
step:686/1770 train_time:66265ms step_avg:96.60ms
step:687/1770 train_time:66365ms step_avg:96.60ms
step:688/1770 train_time:66466ms step_avg:96.61ms
step:689/1770 train_time:66566ms step_avg:96.61ms
step:690/1770 train_time:66666ms step_avg:96.62ms
step:691/1770 train_time:66766ms step_avg:96.62ms
step:692/1770 train_time:66866ms step_avg:96.63ms
step:693/1770 train_time:66966ms step_avg:96.63ms
step:694/1770 train_time:67065ms step_avg:96.64ms
step:695/1770 train_time:67165ms step_avg:96.64ms
step:696/1770 train_time:67265ms step_avg:96.65ms
step:697/1770 train_time:67365ms step_avg:96.65ms
step:698/1770 train_time:67465ms step_avg:96.65ms
step:699/1770 train_time:67565ms step_avg:96.66ms
step:700/1770 train_time:67666ms step_avg:96.67ms
step:701/1770 train_time:67767ms step_avg:96.67ms
step:702/1770 train_time:67867ms step_avg:96.68ms
step:703/1770 train_time:67967ms step_avg:96.68ms
step:704/1770 train_time:68067ms step_avg:96.69ms
step:705/1770 train_time:68167ms step_avg:96.69ms
step:706/1770 train_time:68267ms step_avg:96.70ms
step:707/1770 train_time:68367ms step_avg:96.70ms
step:708/1770 train_time:68467ms step_avg:96.70ms
step:709/1770 train_time:68566ms step_avg:96.71ms
step:710/1770 train_time:68666ms step_avg:96.71ms
step:711/1770 train_time:68765ms step_avg:96.72ms
step:712/1770 train_time:68866ms step_avg:96.72ms
step:713/1770 train_time:68965ms step_avg:96.73ms
step:714/1770 train_time:69065ms step_avg:96.73ms
step:715/1770 train_time:69165ms step_avg:96.73ms
step:716/1770 train_time:69265ms step_avg:96.74ms
step:717/1770 train_time:69365ms step_avg:96.74ms
step:718/1770 train_time:69465ms step_avg:96.75ms
step:719/1770 train_time:69565ms step_avg:96.75ms
step:720/1770 train_time:69666ms step_avg:96.76ms
step:721/1770 train_time:69767ms step_avg:96.76ms
step:722/1770 train_time:69867ms step_avg:96.77ms
step:723/1770 train_time:69967ms step_avg:96.77ms
step:724/1770 train_time:70067ms step_avg:96.78ms
step:725/1770 train_time:70167ms step_avg:96.78ms
step:726/1770 train_time:70266ms step_avg:96.78ms
step:727/1770 train_time:70365ms step_avg:96.79ms
step:728/1770 train_time:70465ms step_avg:96.79ms
step:729/1770 train_time:70565ms step_avg:96.80ms
step:730/1770 train_time:70666ms step_avg:96.80ms
step:731/1770 train_time:70765ms step_avg:96.81ms
step:732/1770 train_time:70866ms step_avg:96.81ms
step:733/1770 train_time:70967ms step_avg:96.82ms
step:734/1770 train_time:71067ms step_avg:96.82ms
step:735/1770 train_time:71167ms step_avg:96.83ms
step:736/1770 train_time:71267ms step_avg:96.83ms
step:737/1770 train_time:71367ms step_avg:96.83ms
step:738/1770 train_time:71466ms step_avg:96.84ms
step:739/1770 train_time:71566ms step_avg:96.84ms
step:740/1770 train_time:71666ms step_avg:96.85ms
step:741/1770 train_time:71766ms step_avg:96.85ms
step:742/1770 train_time:71866ms step_avg:96.85ms
step:743/1770 train_time:71966ms step_avg:96.86ms
step:744/1770 train_time:72066ms step_avg:96.86ms
step:745/1770 train_time:72166ms step_avg:96.87ms
step:746/1770 train_time:72266ms step_avg:96.87ms
step:747/1770 train_time:72366ms step_avg:96.88ms
step:748/1770 train_time:72466ms step_avg:96.88ms
step:749/1770 train_time:72565ms step_avg:96.88ms
step:750/1770 train_time:72665ms step_avg:96.89ms
step:750/1770 val_loss:3.5996 train_time:72765ms step_avg:97.02ms
step:751/1770 train_time:72783ms step_avg:96.92ms
step:752/1770 train_time:72873ms step_avg:96.91ms
step:753/1770 train_time:72976ms step_avg:96.91ms
step:754/1770 train_time:73077ms step_avg:96.92ms
step:755/1770 train_time:73177ms step_avg:96.92ms
step:756/1770 train_time:73277ms step_avg:96.93ms
step:757/1770 train_time:73376ms step_avg:96.93ms
step:758/1770 train_time:73476ms step_avg:96.93ms
step:759/1770 train_time:73575ms step_avg:96.94ms
step:760/1770 train_time:73676ms step_avg:96.94ms
step:761/1770 train_time:73779ms step_avg:96.95ms
step:762/1770 train_time:73883ms step_avg:96.96ms
step:763/1770 train_time:73983ms step_avg:96.96ms
step:764/1770 train_time:74082ms step_avg:96.97ms
step:765/1770 train_time:74182ms step_avg:96.97ms
step:766/1770 train_time:74281ms step_avg:96.97ms
step:767/1770 train_time:74380ms step_avg:96.98ms
step:768/1770 train_time:74480ms step_avg:96.98ms
step:769/1770 train_time:74579ms step_avg:96.98ms
step:770/1770 train_time:74679ms step_avg:96.99ms
step:771/1770 train_time:74781ms step_avg:96.99ms
step:772/1770 train_time:74881ms step_avg:97.00ms
step:773/1770 train_time:74982ms step_avg:97.00ms
step:774/1770 train_time:75083ms step_avg:97.01ms
step:775/1770 train_time:75182ms step_avg:97.01ms
step:776/1770 train_time:75281ms step_avg:97.01ms
step:777/1770 train_time:75380ms step_avg:97.01ms
step:778/1770 train_time:75479ms step_avg:97.02ms
step:779/1770 train_time:75579ms step_avg:97.02ms
step:780/1770 train_time:75678ms step_avg:97.02ms
step:781/1770 train_time:75778ms step_avg:97.03ms
step:782/1770 train_time:75879ms step_avg:97.03ms
step:783/1770 train_time:75980ms step_avg:97.04ms
step:784/1770 train_time:76081ms step_avg:97.04ms
step:785/1770 train_time:76181ms step_avg:97.05ms
step:786/1770 train_time:76281ms step_avg:97.05ms
step:787/1770 train_time:76380ms step_avg:97.05ms
step:788/1770 train_time:76480ms step_avg:97.06ms
step:789/1770 train_time:76579ms step_avg:97.06ms
step:790/1770 train_time:76680ms step_avg:97.06ms
step:791/1770 train_time:76780ms step_avg:97.07ms
step:792/1770 train_time:76880ms step_avg:97.07ms
step:793/1770 train_time:76980ms step_avg:97.07ms
step:794/1770 train_time:77081ms step_avg:97.08ms
step:795/1770 train_time:77182ms step_avg:97.08ms
step:796/1770 train_time:77282ms step_avg:97.09ms
step:797/1770 train_time:77382ms step_avg:97.09ms
step:798/1770 train_time:77482ms step_avg:97.09ms
step:799/1770 train_time:77581ms step_avg:97.10ms
step:800/1770 train_time:77681ms step_avg:97.10ms
step:801/1770 train_time:77781ms step_avg:97.10ms
step:802/1770 train_time:77881ms step_avg:97.11ms
step:803/1770 train_time:77981ms step_avg:97.11ms
step:804/1770 train_time:78081ms step_avg:97.12ms
step:805/1770 train_time:78182ms step_avg:97.12ms
step:806/1770 train_time:78282ms step_avg:97.12ms
step:807/1770 train_time:78383ms step_avg:97.13ms
step:808/1770 train_time:78483ms step_avg:97.13ms
step:809/1770 train_time:78582ms step_avg:97.14ms
step:810/1770 train_time:78683ms step_avg:97.14ms
step:811/1770 train_time:78782ms step_avg:97.14ms
step:812/1770 train_time:78882ms step_avg:97.15ms
step:813/1770 train_time:78982ms step_avg:97.15ms
step:814/1770 train_time:79083ms step_avg:97.15ms
step:815/1770 train_time:79183ms step_avg:97.16ms
step:816/1770 train_time:79283ms step_avg:97.16ms
step:817/1770 train_time:79383ms step_avg:97.16ms
step:818/1770 train_time:79483ms step_avg:97.17ms
step:819/1770 train_time:79583ms step_avg:97.17ms
step:820/1770 train_time:79683ms step_avg:97.17ms
step:821/1770 train_time:79782ms step_avg:97.18ms
step:822/1770 train_time:79883ms step_avg:97.18ms
step:823/1770 train_time:79983ms step_avg:97.18ms
step:824/1770 train_time:80083ms step_avg:97.19ms
step:825/1770 train_time:80183ms step_avg:97.19ms
step:826/1770 train_time:80283ms step_avg:97.19ms
step:827/1770 train_time:80383ms step_avg:97.20ms
step:828/1770 train_time:80483ms step_avg:97.20ms
step:829/1770 train_time:80583ms step_avg:97.21ms
step:830/1770 train_time:80683ms step_avg:97.21ms
step:831/1770 train_time:80783ms step_avg:97.21ms
step:832/1770 train_time:80882ms step_avg:97.21ms
step:833/1770 train_time:80982ms step_avg:97.22ms
step:834/1770 train_time:81082ms step_avg:97.22ms
step:835/1770 train_time:81182ms step_avg:97.22ms
step:836/1770 train_time:81283ms step_avg:97.23ms
step:837/1770 train_time:81383ms step_avg:97.23ms
step:838/1770 train_time:81483ms step_avg:97.24ms
step:839/1770 train_time:81583ms step_avg:97.24ms
step:840/1770 train_time:81683ms step_avg:97.24ms
step:841/1770 train_time:81782ms step_avg:97.24ms
step:842/1770 train_time:81882ms step_avg:97.25ms
step:843/1770 train_time:81982ms step_avg:97.25ms
step:844/1770 train_time:82081ms step_avg:97.25ms
step:845/1770 train_time:82181ms step_avg:97.26ms
step:846/1770 train_time:82281ms step_avg:97.26ms
step:847/1770 train_time:82381ms step_avg:97.26ms
step:848/1770 train_time:82481ms step_avg:97.27ms
step:849/1770 train_time:82581ms step_avg:97.27ms
step:850/1770 train_time:82682ms step_avg:97.27ms
step:851/1770 train_time:82782ms step_avg:97.28ms
step:852/1770 train_time:82882ms step_avg:97.28ms
step:853/1770 train_time:82981ms step_avg:97.28ms
step:854/1770 train_time:83081ms step_avg:97.28ms
step:855/1770 train_time:83182ms step_avg:97.29ms
step:856/1770 train_time:83281ms step_avg:97.29ms
step:857/1770 train_time:83381ms step_avg:97.29ms
step:858/1770 train_time:83480ms step_avg:97.30ms
step:859/1770 train_time:83581ms step_avg:97.30ms
step:860/1770 train_time:83681ms step_avg:97.30ms
step:861/1770 train_time:83781ms step_avg:97.31ms
step:862/1770 train_time:83881ms step_avg:97.31ms
step:863/1770 train_time:83981ms step_avg:97.31ms
step:864/1770 train_time:84081ms step_avg:97.32ms
step:865/1770 train_time:84182ms step_avg:97.32ms
step:866/1770 train_time:84282ms step_avg:97.32ms
step:867/1770 train_time:84381ms step_avg:97.33ms
step:868/1770 train_time:84482ms step_avg:97.33ms
step:869/1770 train_time:84582ms step_avg:97.33ms
step:870/1770 train_time:84683ms step_avg:97.34ms
step:871/1770 train_time:84783ms step_avg:97.34ms
step:872/1770 train_time:84882ms step_avg:97.34ms
step:873/1770 train_time:84982ms step_avg:97.34ms
step:874/1770 train_time:85082ms step_avg:97.35ms
step:875/1770 train_time:85182ms step_avg:97.35ms
step:875/1770 val_loss:3.5517 train_time:85282ms step_avg:97.46ms
step:876/1770 train_time:85300ms step_avg:97.37ms
step:877/1770 train_time:85390ms step_avg:97.37ms
step:878/1770 train_time:85494ms step_avg:97.37ms
step:879/1770 train_time:85596ms step_avg:97.38ms
step:880/1770 train_time:85696ms step_avg:97.38ms
step:881/1770 train_time:85796ms step_avg:97.38ms
step:882/1770 train_time:85895ms step_avg:97.39ms
step:883/1770 train_time:85995ms step_avg:97.39ms
step:884/1770 train_time:86096ms step_avg:97.39ms
step:885/1770 train_time:86196ms step_avg:97.40ms
step:886/1770 train_time:86299ms step_avg:97.40ms
step:887/1770 train_time:86401ms step_avg:97.41ms
step:888/1770 train_time:86502ms step_avg:97.41ms
step:889/1770 train_time:86602ms step_avg:97.42ms
step:890/1770 train_time:86701ms step_avg:97.42ms
step:891/1770 train_time:86801ms step_avg:97.42ms
step:892/1770 train_time:86901ms step_avg:97.42ms
step:893/1770 train_time:87002ms step_avg:97.43ms
step:894/1770 train_time:87102ms step_avg:97.43ms
step:895/1770 train_time:87203ms step_avg:97.43ms
step:896/1770 train_time:87304ms step_avg:97.44ms
step:897/1770 train_time:87405ms step_avg:97.44ms
step:898/1770 train_time:87506ms step_avg:97.45ms
step:899/1770 train_time:87606ms step_avg:97.45ms
step:900/1770 train_time:87707ms step_avg:97.45ms
step:901/1770 train_time:87808ms step_avg:97.46ms
step:902/1770 train_time:87909ms step_avg:97.46ms
step:903/1770 train_time:88010ms step_avg:97.46ms
step:904/1770 train_time:88113ms step_avg:97.47ms
step:905/1770 train_time:88214ms step_avg:97.47ms
step:906/1770 train_time:88315ms step_avg:97.48ms
step:907/1770 train_time:88416ms step_avg:97.48ms
step:908/1770 train_time:88516ms step_avg:97.48ms
step:909/1770 train_time:88616ms step_avg:97.49ms
step:910/1770 train_time:88717ms step_avg:97.49ms
step:911/1770 train_time:88817ms step_avg:97.49ms
step:912/1770 train_time:88918ms step_avg:97.50ms
step:913/1770 train_time:89018ms step_avg:97.50ms
step:914/1770 train_time:89118ms step_avg:97.50ms
step:915/1770 train_time:89218ms step_avg:97.51ms
step:916/1770 train_time:89318ms step_avg:97.51ms
step:917/1770 train_time:89418ms step_avg:97.51ms
step:918/1770 train_time:89518ms step_avg:97.51ms
step:919/1770 train_time:89619ms step_avg:97.52ms
step:920/1770 train_time:89721ms step_avg:97.52ms
step:921/1770 train_time:89823ms step_avg:97.53ms
step:922/1770 train_time:89925ms step_avg:97.53ms
step:923/1770 train_time:90025ms step_avg:97.54ms
step:924/1770 train_time:90128ms step_avg:97.54ms
step:925/1770 train_time:90230ms step_avg:97.55ms
step:926/1770 train_time:90332ms step_avg:97.55ms
step:927/1770 train_time:90435ms step_avg:97.56ms
step:928/1770 train_time:90537ms step_avg:97.56ms
step:929/1770 train_time:90638ms step_avg:97.57ms
step:930/1770 train_time:90740ms step_avg:97.57ms
step:931/1770 train_time:90841ms step_avg:97.57ms
step:932/1770 train_time:90942ms step_avg:97.58ms
step:933/1770 train_time:91044ms step_avg:97.58ms
step:934/1770 train_time:91145ms step_avg:97.59ms
step:935/1770 train_time:91246ms step_avg:97.59ms
step:936/1770 train_time:91349ms step_avg:97.59ms
step:937/1770 train_time:91452ms step_avg:97.60ms
step:938/1770 train_time:91554ms step_avg:97.61ms
step:939/1770 train_time:91656ms step_avg:97.61ms
step:940/1770 train_time:91758ms step_avg:97.62ms
step:941/1770 train_time:91861ms step_avg:97.62ms
step:942/1770 train_time:91962ms step_avg:97.62ms
step:943/1770 train_time:92064ms step_avg:97.63ms
step:944/1770 train_time:92164ms step_avg:97.63ms
step:945/1770 train_time:92266ms step_avg:97.64ms
step:946/1770 train_time:92368ms step_avg:97.64ms
step:947/1770 train_time:92471ms step_avg:97.65ms
step:948/1770 train_time:92573ms step_avg:97.65ms
step:949/1770 train_time:92677ms step_avg:97.66ms
step:950/1770 train_time:92779ms step_avg:97.66ms
step:951/1770 train_time:92881ms step_avg:97.67ms
step:952/1770 train_time:92982ms step_avg:97.67ms
step:953/1770 train_time:93084ms step_avg:97.67ms
step:954/1770 train_time:93185ms step_avg:97.68ms
step:955/1770 train_time:93286ms step_avg:97.68ms
step:956/1770 train_time:93388ms step_avg:97.69ms
step:957/1770 train_time:93490ms step_avg:97.69ms
step:958/1770 train_time:93593ms step_avg:97.70ms
step:959/1770 train_time:93696ms step_avg:97.70ms
step:960/1770 train_time:93798ms step_avg:97.71ms
step:961/1770 train_time:93899ms step_avg:97.71ms
step:962/1770 train_time:94001ms step_avg:97.71ms
step:963/1770 train_time:94102ms step_avg:97.72ms
step:964/1770 train_time:94203ms step_avg:97.72ms
step:965/1770 train_time:94305ms step_avg:97.72ms
step:966/1770 train_time:94406ms step_avg:97.73ms
step:967/1770 train_time:94509ms step_avg:97.73ms
step:968/1770 train_time:94612ms step_avg:97.74ms
step:969/1770 train_time:94716ms step_avg:97.75ms
step:970/1770 train_time:94819ms step_avg:97.75ms
step:971/1770 train_time:94921ms step_avg:97.76ms
step:972/1770 train_time:95022ms step_avg:97.76ms
step:973/1770 train_time:95122ms step_avg:97.76ms
step:974/1770 train_time:95223ms step_avg:97.77ms
step:975/1770 train_time:95325ms step_avg:97.77ms
step:976/1770 train_time:95426ms step_avg:97.77ms
step:977/1770 train_time:95530ms step_avg:97.78ms
step:978/1770 train_time:95632ms step_avg:97.78ms
step:979/1770 train_time:95735ms step_avg:97.79ms
step:980/1770 train_time:95837ms step_avg:97.79ms
step:981/1770 train_time:95939ms step_avg:97.80ms
step:982/1770 train_time:96041ms step_avg:97.80ms
step:983/1770 train_time:96142ms step_avg:97.80ms
step:984/1770 train_time:96243ms step_avg:97.81ms
step:985/1770 train_time:96345ms step_avg:97.81ms
step:986/1770 train_time:96446ms step_avg:97.82ms
step:987/1770 train_time:96548ms step_avg:97.82ms
step:988/1770 train_time:96652ms step_avg:97.83ms
step:989/1770 train_time:96756ms step_avg:97.83ms
step:990/1770 train_time:96859ms step_avg:97.84ms
step:991/1770 train_time:96960ms step_avg:97.84ms
step:992/1770 train_time:97061ms step_avg:97.84ms
step:993/1770 train_time:97163ms step_avg:97.85ms
step:994/1770 train_time:97264ms step_avg:97.85ms
step:995/1770 train_time:97365ms step_avg:97.85ms
step:996/1770 train_time:97467ms step_avg:97.86ms
step:997/1770 train_time:97570ms step_avg:97.86ms
step:998/1770 train_time:97673ms step_avg:97.87ms
step:999/1770 train_time:97775ms step_avg:97.87ms
step:1000/1770 train_time:97878ms step_avg:97.88ms
step:1000/1770 val_loss:3.5138 train_time:97978ms step_avg:97.98ms
step:1001/1770 train_time:97996ms step_avg:97.90ms
step:1002/1770 train_time:98088ms step_avg:97.89ms
step:1003/1770 train_time:98190ms step_avg:97.90ms
step:1004/1770 train_time:98293ms step_avg:97.90ms
step:1005/1770 train_time:98396ms step_avg:97.91ms
step:1006/1770 train_time:98497ms step_avg:97.91ms
step:1007/1770 train_time:98599ms step_avg:97.91ms
step:1008/1770 train_time:98700ms step_avg:97.92ms
step:1009/1770 train_time:98801ms step_avg:97.92ms
step:1010/1770 train_time:98901ms step_avg:97.92ms
step:1011/1770 train_time:99004ms step_avg:97.93ms
step:1012/1770 train_time:99106ms step_avg:97.93ms
step:1013/1770 train_time:99209ms step_avg:97.94ms
step:1014/1770 train_time:99313ms step_avg:97.94ms
step:1015/1770 train_time:99414ms step_avg:97.94ms
step:1016/1770 train_time:99516ms step_avg:97.95ms
step:1017/1770 train_time:99618ms step_avg:97.95ms
step:1018/1770 train_time:99719ms step_avg:97.96ms
step:1019/1770 train_time:99820ms step_avg:97.96ms
step:1020/1770 train_time:99922ms step_avg:97.96ms
step:1021/1770 train_time:100024ms step_avg:97.97ms
step:1022/1770 train_time:100125ms step_avg:97.97ms
step:1023/1770 train_time:100226ms step_avg:97.97ms
step:1024/1770 train_time:100329ms step_avg:97.98ms
step:1025/1770 train_time:100431ms step_avg:97.98ms
step:1026/1770 train_time:100534ms step_avg:97.99ms
step:1027/1770 train_time:100637ms step_avg:97.99ms
step:1028/1770 train_time:100740ms step_avg:98.00ms
step:1029/1770 train_time:100841ms step_avg:98.00ms
step:1030/1770 train_time:100943ms step_avg:98.00ms
step:1031/1770 train_time:101044ms step_avg:98.01ms
step:1032/1770 train_time:101145ms step_avg:98.01ms
step:1033/1770 train_time:101246ms step_avg:98.01ms
step:1034/1770 train_time:101347ms step_avg:98.01ms
step:1035/1770 train_time:101449ms step_avg:98.02ms
step:1036/1770 train_time:101552ms step_avg:98.02ms
step:1037/1770 train_time:101655ms step_avg:98.03ms
step:1038/1770 train_time:101757ms step_avg:98.03ms
step:1039/1770 train_time:101860ms step_avg:98.04ms
step:1040/1770 train_time:101961ms step_avg:98.04ms
step:1041/1770 train_time:102062ms step_avg:98.04ms
step:1042/1770 train_time:102164ms step_avg:98.05ms
step:1043/1770 train_time:102265ms step_avg:98.05ms
step:1044/1770 train_time:102366ms step_avg:98.05ms
step:1045/1770 train_time:102468ms step_avg:98.06ms
step:1046/1770 train_time:102570ms step_avg:98.06ms
step:1047/1770 train_time:102673ms step_avg:98.06ms
step:1048/1770 train_time:102776ms step_avg:98.07ms
step:1049/1770 train_time:102878ms step_avg:98.07ms
step:1050/1770 train_time:102980ms step_avg:98.08ms
step:1051/1770 train_time:103082ms step_avg:98.08ms
step:1052/1770 train_time:103183ms step_avg:98.08ms
step:1053/1770 train_time:103285ms step_avg:98.09ms
step:1054/1770 train_time:103385ms step_avg:98.09ms
step:1055/1770 train_time:103487ms step_avg:98.09ms
step:1056/1770 train_time:103589ms step_avg:98.10ms
step:1057/1770 train_time:103693ms step_avg:98.10ms
step:1058/1770 train_time:103797ms step_avg:98.11ms
step:1059/1770 train_time:103899ms step_avg:98.11ms
step:1060/1770 train_time:104002ms step_avg:98.11ms
step:1061/1770 train_time:104103ms step_avg:98.12ms
step:1062/1770 train_time:104205ms step_avg:98.12ms
step:1063/1770 train_time:104308ms step_avg:98.13ms
step:1064/1770 train_time:104410ms step_avg:98.13ms
step:1065/1770 train_time:104512ms step_avg:98.13ms
step:1066/1770 train_time:104614ms step_avg:98.14ms
step:1067/1770 train_time:104716ms step_avg:98.14ms
step:1068/1770 train_time:104819ms step_avg:98.15ms
step:1069/1770 train_time:104921ms step_avg:98.15ms
step:1070/1770 train_time:105023ms step_avg:98.15ms
step:1071/1770 train_time:105125ms step_avg:98.16ms
step:1072/1770 train_time:105227ms step_avg:98.16ms
step:1073/1770 train_time:105329ms step_avg:98.16ms
step:1074/1770 train_time:105431ms step_avg:98.17ms
step:1075/1770 train_time:105534ms step_avg:98.17ms
step:1076/1770 train_time:105637ms step_avg:98.18ms
step:1077/1770 train_time:105739ms step_avg:98.18ms
step:1078/1770 train_time:105841ms step_avg:98.18ms
step:1079/1770 train_time:105943ms step_avg:98.19ms
step:1080/1770 train_time:106044ms step_avg:98.19ms
step:1081/1770 train_time:106146ms step_avg:98.19ms
step:1082/1770 train_time:106247ms step_avg:98.20ms
step:1083/1770 train_time:106349ms step_avg:98.20ms
step:1084/1770 train_time:106452ms step_avg:98.20ms
step:1085/1770 train_time:106555ms step_avg:98.21ms
step:1086/1770 train_time:106657ms step_avg:98.21ms
step:1087/1770 train_time:106759ms step_avg:98.21ms
step:1088/1770 train_time:106862ms step_avg:98.22ms
step:1089/1770 train_time:106964ms step_avg:98.22ms
step:1090/1770 train_time:107065ms step_avg:98.23ms
step:1091/1770 train_time:107167ms step_avg:98.23ms
step:1092/1770 train_time:107269ms step_avg:98.23ms
step:1093/1770 train_time:107371ms step_avg:98.24ms
step:1094/1770 train_time:107474ms step_avg:98.24ms
step:1095/1770 train_time:107576ms step_avg:98.24ms
step:1096/1770 train_time:107679ms step_avg:98.25ms
step:1097/1770 train_time:107781ms step_avg:98.25ms
step:1098/1770 train_time:107883ms step_avg:98.25ms
step:1099/1770 train_time:107985ms step_avg:98.26ms
step:1100/1770 train_time:108086ms step_avg:98.26ms
step:1101/1770 train_time:108187ms step_avg:98.26ms
step:1102/1770 train_time:108289ms step_avg:98.27ms
step:1103/1770 train_time:108392ms step_avg:98.27ms
step:1104/1770 train_time:108495ms step_avg:98.27ms
step:1105/1770 train_time:108598ms step_avg:98.28ms
step:1106/1770 train_time:108701ms step_avg:98.28ms
step:1107/1770 train_time:108803ms step_avg:98.29ms
step:1108/1770 train_time:108907ms step_avg:98.29ms
step:1109/1770 train_time:109008ms step_avg:98.29ms
step:1110/1770 train_time:109110ms step_avg:98.30ms
step:1111/1770 train_time:109213ms step_avg:98.30ms
step:1112/1770 train_time:109315ms step_avg:98.31ms
step:1113/1770 train_time:109417ms step_avg:98.31ms
step:1114/1770 train_time:109519ms step_avg:98.31ms
step:1115/1770 train_time:109621ms step_avg:98.31ms
step:1116/1770 train_time:109723ms step_avg:98.32ms
step:1117/1770 train_time:109825ms step_avg:98.32ms
step:1118/1770 train_time:109927ms step_avg:98.32ms
step:1119/1770 train_time:110029ms step_avg:98.33ms
step:1120/1770 train_time:110132ms step_avg:98.33ms
step:1121/1770 train_time:110234ms step_avg:98.34ms
step:1122/1770 train_time:110337ms step_avg:98.34ms
step:1123/1770 train_time:110439ms step_avg:98.34ms
step:1124/1770 train_time:110541ms step_avg:98.35ms
step:1125/1770 train_time:110643ms step_avg:98.35ms
step:1125/1770 val_loss:3.4726 train_time:110744ms step_avg:98.44ms
step:1126/1770 train_time:110761ms step_avg:98.37ms
step:1127/1770 train_time:110854ms step_avg:98.36ms
step:1128/1770 train_time:110957ms step_avg:98.37ms
step:1129/1770 train_time:111058ms step_avg:98.37ms
step:1130/1770 train_time:111160ms step_avg:98.37ms
step:1131/1770 train_time:111261ms step_avg:98.37ms
step:1132/1770 train_time:111362ms step_avg:98.38ms
step:1133/1770 train_time:111463ms step_avg:98.38ms
step:1134/1770 train_time:111564ms step_avg:98.38ms
step:1135/1770 train_time:111665ms step_avg:98.38ms
step:1136/1770 train_time:111770ms step_avg:98.39ms
step:1137/1770 train_time:111875ms step_avg:98.39ms
step:1138/1770 train_time:111977ms step_avg:98.40ms
step:1139/1770 train_time:112078ms step_avg:98.40ms
step:1140/1770 train_time:112180ms step_avg:98.40ms
step:1141/1770 train_time:112281ms step_avg:98.41ms
step:1142/1770 train_time:112382ms step_avg:98.41ms
step:1143/1770 train_time:112483ms step_avg:98.41ms
step:1144/1770 train_time:112584ms step_avg:98.41ms
step:1145/1770 train_time:112685ms step_avg:98.42ms
step:1146/1770 train_time:112789ms step_avg:98.42ms
step:1147/1770 train_time:112894ms step_avg:98.43ms
step:1148/1770 train_time:112998ms step_avg:98.43ms
step:1149/1770 train_time:113099ms step_avg:98.43ms
step:1150/1770 train_time:113201ms step_avg:98.44ms
step:1151/1770 train_time:113302ms step_avg:98.44ms
step:1152/1770 train_time:113404ms step_avg:98.44ms
step:1153/1770 train_time:113505ms step_avg:98.44ms
step:1154/1770 train_time:113606ms step_avg:98.45ms
step:1155/1770 train_time:113708ms step_avg:98.45ms
step:1156/1770 train_time:113811ms step_avg:98.45ms
step:1157/1770 train_time:113916ms step_avg:98.46ms
step:1158/1770 train_time:114019ms step_avg:98.46ms
step:1159/1770 train_time:114120ms step_avg:98.46ms
step:1160/1770 train_time:114221ms step_avg:98.47ms
step:1161/1770 train_time:114323ms step_avg:98.47ms
step:1162/1770 train_time:114426ms step_avg:98.47ms
step:1163/1770 train_time:114528ms step_avg:98.48ms
step:1164/1770 train_time:114630ms step_avg:98.48ms
step:1165/1770 train_time:114732ms step_avg:98.48ms
step:1166/1770 train_time:114834ms step_avg:98.49ms
step:1167/1770 train_time:114936ms step_avg:98.49ms
step:1168/1770 train_time:115038ms step_avg:98.49ms
step:1169/1770 train_time:115140ms step_avg:98.49ms
step:1170/1770 train_time:115241ms step_avg:98.50ms
step:1171/1770 train_time:115343ms step_avg:98.50ms
step:1172/1770 train_time:115444ms step_avg:98.50ms
step:1173/1770 train_time:115546ms step_avg:98.50ms
step:1174/1770 train_time:115649ms step_avg:98.51ms
step:1175/1770 train_time:115752ms step_avg:98.51ms
step:1176/1770 train_time:115855ms step_avg:98.52ms
step:1177/1770 train_time:115958ms step_avg:98.52ms
step:1178/1770 train_time:116060ms step_avg:98.52ms
step:1179/1770 train_time:116163ms step_avg:98.53ms
step:1180/1770 train_time:116265ms step_avg:98.53ms
step:1181/1770 train_time:116366ms step_avg:98.53ms
step:1182/1770 train_time:116469ms step_avg:98.54ms
step:1183/1770 train_time:116572ms step_avg:98.54ms
step:1184/1770 train_time:116676ms step_avg:98.54ms
step:1185/1770 train_time:116779ms step_avg:98.55ms
step:1186/1770 train_time:116883ms step_avg:98.55ms
step:1187/1770 train_time:116987ms step_avg:98.56ms
step:1188/1770 train_time:117091ms step_avg:98.56ms
step:1189/1770 train_time:117195ms step_avg:98.57ms
step:1190/1770 train_time:117298ms step_avg:98.57ms
step:1191/1770 train_time:117401ms step_avg:98.57ms
step:1192/1770 train_time:117504ms step_avg:98.58ms
step:1193/1770 train_time:117607ms step_avg:98.58ms
step:1194/1770 train_time:117710ms step_avg:98.58ms
step:1195/1770 train_time:117814ms step_avg:98.59ms
step:1196/1770 train_time:117918ms step_avg:98.59ms
step:1197/1770 train_time:118021ms step_avg:98.60ms
step:1198/1770 train_time:118124ms step_avg:98.60ms
step:1199/1770 train_time:118228ms step_avg:98.61ms
step:1200/1770 train_time:118332ms step_avg:98.61ms
step:1201/1770 train_time:118437ms step_avg:98.62ms
step:1202/1770 train_time:118540ms step_avg:98.62ms
step:1203/1770 train_time:118643ms step_avg:98.62ms
step:1204/1770 train_time:118745ms step_avg:98.63ms
step:1205/1770 train_time:118849ms step_avg:98.63ms
step:1206/1770 train_time:118955ms step_avg:98.64ms
step:1207/1770 train_time:119058ms step_avg:98.64ms
step:1208/1770 train_time:119161ms step_avg:98.64ms
step:1209/1770 train_time:119263ms step_avg:98.65ms
step:1210/1770 train_time:119366ms step_avg:98.65ms
step:1211/1770 train_time:119469ms step_avg:98.65ms
step:1212/1770 train_time:119573ms step_avg:98.66ms
step:1213/1770 train_time:119676ms step_avg:98.66ms
step:1214/1770 train_time:119779ms step_avg:98.66ms
step:1215/1770 train_time:119882ms step_avg:98.67ms
step:1216/1770 train_time:119987ms step_avg:98.67ms
step:1217/1770 train_time:120091ms step_avg:98.68ms
step:1218/1770 train_time:120195ms step_avg:98.68ms
step:1219/1770 train_time:120298ms step_avg:98.69ms
step:1220/1770 train_time:120401ms step_avg:98.69ms
step:1221/1770 train_time:120504ms step_avg:98.69ms
step:1222/1770 train_time:120608ms step_avg:98.70ms
step:1223/1770 train_time:120711ms step_avg:98.70ms
step:1224/1770 train_time:120815ms step_avg:98.71ms
step:1225/1770 train_time:120919ms step_avg:98.71ms
step:1226/1770 train_time:121022ms step_avg:98.71ms
step:1227/1770 train_time:121125ms step_avg:98.72ms
step:1228/1770 train_time:121228ms step_avg:98.72ms
step:1229/1770 train_time:121333ms step_avg:98.72ms
step:1230/1770 train_time:121437ms step_avg:98.73ms
step:1231/1770 train_time:121540ms step_avg:98.73ms
step:1232/1770 train_time:121644ms step_avg:98.74ms
step:1233/1770 train_time:121747ms step_avg:98.74ms
step:1234/1770 train_time:121850ms step_avg:98.74ms
step:1235/1770 train_time:121954ms step_avg:98.75ms
step:1236/1770 train_time:122057ms step_avg:98.75ms
step:1237/1770 train_time:122160ms step_avg:98.76ms
step:1238/1770 train_time:122263ms step_avg:98.76ms
step:1239/1770 train_time:122367ms step_avg:98.76ms
step:1240/1770 train_time:122470ms step_avg:98.77ms
step:1241/1770 train_time:122574ms step_avg:98.77ms
step:1242/1770 train_time:122678ms step_avg:98.77ms
step:1243/1770 train_time:122781ms step_avg:98.78ms
step:1244/1770 train_time:122884ms step_avg:98.78ms
step:1245/1770 train_time:122988ms step_avg:98.79ms
step:1246/1770 train_time:123093ms step_avg:98.79ms
step:1247/1770 train_time:123196ms step_avg:98.79ms
step:1248/1770 train_time:123299ms step_avg:98.80ms
step:1249/1770 train_time:123402ms step_avg:98.80ms
step:1250/1770 train_time:123506ms step_avg:98.80ms
step:1250/1770 val_loss:3.4245 train_time:123610ms step_avg:98.89ms
step:1251/1770 train_time:123627ms step_avg:98.82ms
step:1252/1770 train_time:123720ms step_avg:98.82ms
step:1253/1770 train_time:123824ms step_avg:98.82ms
step:1254/1770 train_time:123928ms step_avg:98.83ms
step:1255/1770 train_time:124032ms step_avg:98.83ms
step:1256/1770 train_time:124135ms step_avg:98.83ms
step:1257/1770 train_time:124237ms step_avg:98.84ms
step:1258/1770 train_time:124338ms step_avg:98.84ms
step:1259/1770 train_time:124441ms step_avg:98.84ms
step:1260/1770 train_time:124543ms step_avg:98.84ms
step:1261/1770 train_time:124648ms step_avg:98.85ms
step:1262/1770 train_time:124753ms step_avg:98.85ms
step:1263/1770 train_time:124856ms step_avg:98.86ms
step:1264/1770 train_time:124960ms step_avg:98.86ms
step:1265/1770 train_time:125063ms step_avg:98.86ms
step:1266/1770 train_time:125168ms step_avg:98.87ms
step:1267/1770 train_time:125272ms step_avg:98.87ms
step:1268/1770 train_time:125376ms step_avg:98.88ms
step:1269/1770 train_time:125479ms step_avg:98.88ms
step:1270/1770 train_time:125582ms step_avg:98.88ms
step:1271/1770 train_time:125686ms step_avg:98.89ms
step:1272/1770 train_time:125791ms step_avg:98.89ms
step:1273/1770 train_time:125895ms step_avg:98.90ms
step:1274/1770 train_time:125998ms step_avg:98.90ms
step:1275/1770 train_time:126101ms step_avg:98.90ms
step:1276/1770 train_time:126204ms step_avg:98.91ms
step:1277/1770 train_time:126308ms step_avg:98.91ms
step:1278/1770 train_time:126411ms step_avg:98.91ms
step:1279/1770 train_time:126514ms step_avg:98.92ms
step:1280/1770 train_time:126619ms step_avg:98.92ms
step:1281/1770 train_time:126721ms step_avg:98.92ms
step:1282/1770 train_time:126826ms step_avg:98.93ms
step:1283/1770 train_time:126930ms step_avg:98.93ms
step:1284/1770 train_time:127034ms step_avg:98.94ms
step:1285/1770 train_time:127137ms step_avg:98.94ms
step:1286/1770 train_time:127241ms step_avg:98.94ms
step:1287/1770 train_time:127346ms step_avg:98.95ms
step:1288/1770 train_time:127449ms step_avg:98.95ms
step:1289/1770 train_time:127552ms step_avg:98.95ms
step:1290/1770 train_time:127656ms step_avg:98.96ms
step:1291/1770 train_time:127759ms step_avg:98.96ms
step:1292/1770 train_time:127861ms step_avg:98.96ms
step:1293/1770 train_time:127965ms step_avg:98.97ms
step:1294/1770 train_time:128070ms step_avg:98.97ms
step:1295/1770 train_time:128174ms step_avg:98.98ms
step:1296/1770 train_time:128277ms step_avg:98.98ms
step:1297/1770 train_time:128380ms step_avg:98.98ms
step:1298/1770 train_time:128483ms step_avg:98.99ms
step:1299/1770 train_time:128588ms step_avg:98.99ms
step:1300/1770 train_time:128692ms step_avg:98.99ms
step:1301/1770 train_time:128796ms step_avg:99.00ms
step:1302/1770 train_time:128899ms step_avg:99.00ms
step:1303/1770 train_time:129002ms step_avg:99.00ms
step:1304/1770 train_time:129106ms step_avg:99.01ms
step:1305/1770 train_time:129210ms step_avg:99.01ms
step:1306/1770 train_time:129313ms step_avg:99.01ms
step:1307/1770 train_time:129417ms step_avg:99.02ms
step:1308/1770 train_time:129519ms step_avg:99.02ms
step:1309/1770 train_time:129622ms step_avg:99.02ms
step:1310/1770 train_time:129726ms step_avg:99.03ms
step:1311/1770 train_time:129829ms step_avg:99.03ms
step:1312/1770 train_time:129932ms step_avg:99.03ms
step:1313/1770 train_time:130035ms step_avg:99.04ms
step:1314/1770 train_time:130138ms step_avg:99.04ms
step:1315/1770 train_time:130241ms step_avg:99.04ms
step:1316/1770 train_time:130345ms step_avg:99.05ms
step:1317/1770 train_time:130448ms step_avg:99.05ms
step:1318/1770 train_time:130554ms step_avg:99.05ms
step:1319/1770 train_time:130658ms step_avg:99.06ms
step:1320/1770 train_time:130760ms step_avg:99.06ms
step:1321/1770 train_time:130864ms step_avg:99.06ms
step:1322/1770 train_time:130967ms step_avg:99.07ms
step:1323/1770 train_time:131072ms step_avg:99.07ms
step:1324/1770 train_time:131175ms step_avg:99.07ms
step:1325/1770 train_time:131281ms step_avg:99.08ms
step:1326/1770 train_time:131383ms step_avg:99.08ms
step:1327/1770 train_time:131488ms step_avg:99.09ms
step:1328/1770 train_time:131592ms step_avg:99.09ms
step:1329/1770 train_time:131696ms step_avg:99.09ms
step:1330/1770 train_time:131798ms step_avg:99.10ms
step:1331/1770 train_time:131901ms step_avg:99.10ms
step:1332/1770 train_time:132004ms step_avg:99.10ms
step:1333/1770 train_time:132107ms step_avg:99.10ms
step:1334/1770 train_time:132210ms step_avg:99.11ms
step:1335/1770 train_time:132314ms step_avg:99.11ms
step:1336/1770 train_time:132418ms step_avg:99.12ms
step:1337/1770 train_time:132521ms step_avg:99.12ms
step:1338/1770 train_time:132624ms step_avg:99.12ms
step:1339/1770 train_time:132728ms step_avg:99.12ms
step:1340/1770 train_time:132833ms step_avg:99.13ms
step:1341/1770 train_time:132936ms step_avg:99.13ms
step:1342/1770 train_time:133039ms step_avg:99.14ms
step:1343/1770 train_time:133142ms step_avg:99.14ms
step:1344/1770 train_time:133246ms step_avg:99.14ms
step:1345/1770 train_time:133350ms step_avg:99.15ms
step:1346/1770 train_time:133454ms step_avg:99.15ms
step:1347/1770 train_time:133557ms step_avg:99.15ms
step:1348/1770 train_time:133662ms step_avg:99.16ms
step:1349/1770 train_time:133766ms step_avg:99.16ms
step:1350/1770 train_time:133869ms step_avg:99.16ms
step:1351/1770 train_time:133974ms step_avg:99.17ms
step:1352/1770 train_time:134076ms step_avg:99.17ms
step:1353/1770 train_time:134180ms step_avg:99.17ms
step:1354/1770 train_time:134283ms step_avg:99.18ms
step:1355/1770 train_time:134387ms step_avg:99.18ms
step:1356/1770 train_time:134492ms step_avg:99.18ms
step:1357/1770 train_time:134596ms step_avg:99.19ms
step:1358/1770 train_time:134699ms step_avg:99.19ms
step:1359/1770 train_time:134803ms step_avg:99.19ms
step:1360/1770 train_time:134907ms step_avg:99.20ms
step:1361/1770 train_time:135011ms step_avg:99.20ms
step:1362/1770 train_time:135115ms step_avg:99.20ms
step:1363/1770 train_time:135218ms step_avg:99.21ms
step:1364/1770 train_time:135321ms step_avg:99.21ms
step:1365/1770 train_time:135425ms step_avg:99.21ms
step:1366/1770 train_time:135530ms step_avg:99.22ms
step:1367/1770 train_time:135635ms step_avg:99.22ms
step:1368/1770 train_time:135737ms step_avg:99.22ms
step:1369/1770 train_time:135841ms step_avg:99.23ms
step:1370/1770 train_time:135943ms step_avg:99.23ms
step:1371/1770 train_time:136047ms step_avg:99.23ms
step:1372/1770 train_time:136152ms step_avg:99.24ms
step:1373/1770 train_time:136256ms step_avg:99.24ms
step:1374/1770 train_time:136359ms step_avg:99.24ms
step:1375/1770 train_time:136463ms step_avg:99.25ms
step:1375/1770 val_loss:3.3822 train_time:136566ms step_avg:99.32ms
step:1376/1770 train_time:136584ms step_avg:99.26ms
step:1377/1770 train_time:136677ms step_avg:99.26ms
step:1378/1770 train_time:136782ms step_avg:99.26ms
step:1379/1770 train_time:136884ms step_avg:99.26ms
step:1380/1770 train_time:136986ms step_avg:99.27ms
step:1381/1770 train_time:137090ms step_avg:99.27ms
step:1382/1770 train_time:137193ms step_avg:99.27ms
step:1383/1770 train_time:137296ms step_avg:99.27ms
step:1384/1770 train_time:137399ms step_avg:99.28ms
step:1385/1770 train_time:137503ms step_avg:99.28ms
step:1386/1770 train_time:137608ms step_avg:99.28ms
step:1387/1770 train_time:137714ms step_avg:99.29ms
step:1388/1770 train_time:137820ms step_avg:99.29ms
step:1389/1770 train_time:137924ms step_avg:99.30ms
step:1390/1770 train_time:138027ms step_avg:99.30ms
step:1391/1770 train_time:138130ms step_avg:99.30ms
step:1392/1770 train_time:138233ms step_avg:99.30ms
step:1393/1770 train_time:138336ms step_avg:99.31ms
step:1394/1770 train_time:138439ms step_avg:99.31ms
step:1395/1770 train_time:138543ms step_avg:99.31ms
step:1396/1770 train_time:138647ms step_avg:99.32ms
step:1397/1770 train_time:138751ms step_avg:99.32ms
step:1398/1770 train_time:138856ms step_avg:99.32ms
step:1399/1770 train_time:138960ms step_avg:99.33ms
step:1400/1770 train_time:139065ms step_avg:99.33ms
step:1401/1770 train_time:139167ms step_avg:99.33ms
step:1402/1770 train_time:139270ms step_avg:99.34ms
step:1403/1770 train_time:139374ms step_avg:99.34ms
step:1404/1770 train_time:139478ms step_avg:99.34ms
step:1405/1770 train_time:139581ms step_avg:99.35ms
step:1406/1770 train_time:139684ms step_avg:99.35ms
step:1407/1770 train_time:139787ms step_avg:99.35ms
step:1408/1770 train_time:139892ms step_avg:99.36ms
step:1409/1770 train_time:139995ms step_avg:99.36ms
step:1410/1770 train_time:140100ms step_avg:99.36ms
step:1411/1770 train_time:140203ms step_avg:99.36ms
step:1412/1770 train_time:140306ms step_avg:99.37ms
step:1413/1770 train_time:140409ms step_avg:99.37ms
step:1414/1770 train_time:140513ms step_avg:99.37ms
step:1415/1770 train_time:140616ms step_avg:99.38ms
step:1416/1770 train_time:140720ms step_avg:99.38ms
step:1417/1770 train_time:140824ms step_avg:99.38ms
step:1418/1770 train_time:140927ms step_avg:99.38ms
step:1419/1770 train_time:141032ms step_avg:99.39ms
step:1420/1770 train_time:141135ms step_avg:99.39ms
step:1421/1770 train_time:141240ms step_avg:99.39ms
step:1422/1770 train_time:141343ms step_avg:99.40ms
step:1423/1770 train_time:141446ms step_avg:99.40ms
step:1424/1770 train_time:141549ms step_avg:99.40ms
step:1425/1770 train_time:141654ms step_avg:99.41ms
step:1426/1770 train_time:141758ms step_avg:99.41ms
step:1427/1770 train_time:141862ms step_avg:99.41ms
step:1428/1770 train_time:141966ms step_avg:99.42ms
step:1429/1770 train_time:142070ms step_avg:99.42ms
step:1430/1770 train_time:142173ms step_avg:99.42ms
step:1431/1770 train_time:142279ms step_avg:99.43ms
step:1432/1770 train_time:142381ms step_avg:99.43ms
step:1433/1770 train_time:142484ms step_avg:99.43ms
step:1434/1770 train_time:142586ms step_avg:99.43ms
step:1435/1770 train_time:142690ms step_avg:99.44ms
step:1436/1770 train_time:142794ms step_avg:99.44ms
step:1437/1770 train_time:142900ms step_avg:99.44ms
step:1438/1770 train_time:143003ms step_avg:99.45ms
step:1439/1770 train_time:143106ms step_avg:99.45ms
step:1440/1770 train_time:143209ms step_avg:99.45ms
step:1441/1770 train_time:143314ms step_avg:99.45ms
step:1442/1770 train_time:143417ms step_avg:99.46ms
step:1443/1770 train_time:143521ms step_avg:99.46ms
step:1444/1770 train_time:143624ms step_avg:99.46ms
step:1445/1770 train_time:143728ms step_avg:99.47ms
step:1446/1770 train_time:143832ms step_avg:99.47ms
step:1447/1770 train_time:143937ms step_avg:99.47ms
step:1448/1770 train_time:144043ms step_avg:99.48ms
step:1449/1770 train_time:144148ms step_avg:99.48ms
step:1450/1770 train_time:144253ms step_avg:99.48ms
step:1451/1770 train_time:144357ms step_avg:99.49ms
step:1452/1770 train_time:144463ms step_avg:99.49ms
step:1453/1770 train_time:144567ms step_avg:99.50ms
step:1454/1770 train_time:144671ms step_avg:99.50ms
step:1455/1770 train_time:144777ms step_avg:99.50ms
step:1456/1770 train_time:144883ms step_avg:99.51ms
step:1457/1770 train_time:144989ms step_avg:99.51ms
step:1458/1770 train_time:145093ms step_avg:99.51ms
step:1459/1770 train_time:145197ms step_avg:99.52ms
step:1460/1770 train_time:145303ms step_avg:99.52ms
step:1461/1770 train_time:145408ms step_avg:99.53ms
step:1462/1770 train_time:145512ms step_avg:99.53ms
step:1463/1770 train_time:145616ms step_avg:99.53ms
step:1464/1770 train_time:145723ms step_avg:99.54ms
step:1465/1770 train_time:145828ms step_avg:99.54ms
step:1466/1770 train_time:145933ms step_avg:99.55ms
step:1467/1770 train_time:146041ms step_avg:99.55ms
step:1468/1770 train_time:146145ms step_avg:99.55ms
step:1469/1770 train_time:146248ms step_avg:99.56ms
step:1470/1770 train_time:146353ms step_avg:99.56ms
step:1471/1770 train_time:146458ms step_avg:99.56ms
step:1472/1770 train_time:146563ms step_avg:99.57ms
step:1473/1770 train_time:146668ms step_avg:99.57ms
step:1474/1770 train_time:146772ms step_avg:99.57ms
step:1475/1770 train_time:146876ms step_avg:99.58ms
step:1476/1770 train_time:146980ms step_avg:99.58ms
step:1477/1770 train_time:147086ms step_avg:99.58ms
step:1478/1770 train_time:147191ms step_avg:99.59ms
step:1479/1770 train_time:147296ms step_avg:99.59ms
step:1480/1770 train_time:147400ms step_avg:99.59ms
step:1481/1770 train_time:147508ms step_avg:99.60ms
step:1482/1770 train_time:147612ms step_avg:99.60ms
step:1483/1770 train_time:147717ms step_avg:99.61ms
step:1484/1770 train_time:147821ms step_avg:99.61ms
step:1485/1770 train_time:147925ms step_avg:99.61ms
step:1486/1770 train_time:148028ms step_avg:99.62ms
step:1487/1770 train_time:148133ms step_avg:99.62ms
step:1488/1770 train_time:148237ms step_avg:99.62ms
step:1489/1770 train_time:148343ms step_avg:99.63ms
step:1490/1770 train_time:148448ms step_avg:99.63ms
step:1491/1770 train_time:148552ms step_avg:99.63ms
step:1492/1770 train_time:148658ms step_avg:99.64ms
step:1493/1770 train_time:148765ms step_avg:99.64ms
step:1494/1770 train_time:148872ms step_avg:99.65ms
step:1495/1770 train_time:148977ms step_avg:99.65ms
step:1496/1770 train_time:149081ms step_avg:99.65ms
step:1497/1770 train_time:149185ms step_avg:99.66ms
step:1498/1770 train_time:149288ms step_avg:99.66ms
step:1499/1770 train_time:149392ms step_avg:99.66ms
step:1500/1770 train_time:149497ms step_avg:99.66ms
step:1500/1770 val_loss:3.3441 train_time:149601ms step_avg:99.73ms
step:1501/1770 train_time:149618ms step_avg:99.68ms
step:1502/1770 train_time:149713ms step_avg:99.68ms
step:1503/1770 train_time:149818ms step_avg:99.68ms
step:1504/1770 train_time:149923ms step_avg:99.68ms
step:1505/1770 train_time:150029ms step_avg:99.69ms
step:1506/1770 train_time:150133ms step_avg:99.69ms
step:1507/1770 train_time:150238ms step_avg:99.69ms
step:1508/1770 train_time:150342ms step_avg:99.70ms
step:1509/1770 train_time:150446ms step_avg:99.70ms
step:1510/1770 train_time:150550ms step_avg:99.70ms
step:1511/1770 train_time:150658ms step_avg:99.71ms
step:1512/1770 train_time:150762ms step_avg:99.71ms
step:1513/1770 train_time:150867ms step_avg:99.71ms
step:1514/1770 train_time:150971ms step_avg:99.72ms
step:1515/1770 train_time:151076ms step_avg:99.72ms
step:1516/1770 train_time:151182ms step_avg:99.72ms
step:1517/1770 train_time:151286ms step_avg:99.73ms
step:1518/1770 train_time:151391ms step_avg:99.73ms
step:1519/1770 train_time:151495ms step_avg:99.73ms
step:1520/1770 train_time:151601ms step_avg:99.74ms
step:1521/1770 train_time:151706ms step_avg:99.74ms
step:1522/1770 train_time:151811ms step_avg:99.74ms
step:1523/1770 train_time:151916ms step_avg:99.75ms
step:1524/1770 train_time:152021ms step_avg:99.75ms
step:1525/1770 train_time:152126ms step_avg:99.75ms
step:1526/1770 train_time:152230ms step_avg:99.76ms
step:1527/1770 train_time:152335ms step_avg:99.76ms
step:1528/1770 train_time:152441ms step_avg:99.77ms
step:1529/1770 train_time:152546ms step_avg:99.77ms
step:1530/1770 train_time:152650ms step_avg:99.77ms
step:1531/1770 train_time:152754ms step_avg:99.77ms
step:1532/1770 train_time:152861ms step_avg:99.78ms
step:1533/1770 train_time:152966ms step_avg:99.78ms
step:1534/1770 train_time:153071ms step_avg:99.79ms
step:1535/1770 train_time:153175ms step_avg:99.79ms
step:1536/1770 train_time:153281ms step_avg:99.79ms
step:1537/1770 train_time:153385ms step_avg:99.80ms
step:1538/1770 train_time:153491ms step_avg:99.80ms
step:1539/1770 train_time:153594ms step_avg:99.80ms
step:1540/1770 train_time:153701ms step_avg:99.81ms
step:1541/1770 train_time:153807ms step_avg:99.81ms
step:1542/1770 train_time:153912ms step_avg:99.81ms
step:1543/1770 train_time:154016ms step_avg:99.82ms
step:1544/1770 train_time:154123ms step_avg:99.82ms
step:1545/1770 train_time:154227ms step_avg:99.82ms
step:1546/1770 train_time:154332ms step_avg:99.83ms
step:1547/1770 train_time:154436ms step_avg:99.83ms
step:1548/1770 train_time:154541ms step_avg:99.83ms
step:1549/1770 train_time:154646ms step_avg:99.84ms
step:1550/1770 train_time:154751ms step_avg:99.84ms
step:1551/1770 train_time:154855ms step_avg:99.84ms
step:1552/1770 train_time:154961ms step_avg:99.85ms
step:1553/1770 train_time:155065ms step_avg:99.85ms
step:1554/1770 train_time:155169ms step_avg:99.85ms
step:1555/1770 train_time:155274ms step_avg:99.85ms
step:1556/1770 train_time:155378ms step_avg:99.86ms
step:1557/1770 train_time:155484ms step_avg:99.86ms
step:1558/1770 train_time:155588ms step_avg:99.86ms
step:1559/1770 train_time:155693ms step_avg:99.87ms
step:1560/1770 train_time:155797ms step_avg:99.87ms
step:1561/1770 train_time:155904ms step_avg:99.87ms
step:1562/1770 train_time:156009ms step_avg:99.88ms
step:1563/1770 train_time:156114ms step_avg:99.88ms
step:1564/1770 train_time:156218ms step_avg:99.88ms
step:1565/1770 train_time:156322ms step_avg:99.89ms
step:1566/1770 train_time:156427ms step_avg:99.89ms
step:1567/1770 train_time:156532ms step_avg:99.89ms
step:1568/1770 train_time:156636ms step_avg:99.90ms
step:1569/1770 train_time:156745ms step_avg:99.90ms
step:1570/1770 train_time:156849ms step_avg:99.90ms
step:1571/1770 train_time:156955ms step_avg:99.91ms
step:1572/1770 train_time:157060ms step_avg:99.91ms
step:1573/1770 train_time:157166ms step_avg:99.91ms
step:1574/1770 train_time:157270ms step_avg:99.92ms
step:1575/1770 train_time:157374ms step_avg:99.92ms
step:1576/1770 train_time:157479ms step_avg:99.92ms
step:1577/1770 train_time:157585ms step_avg:99.93ms
step:1578/1770 train_time:157691ms step_avg:99.93ms
step:1579/1770 train_time:157796ms step_avg:99.93ms
step:1580/1770 train_time:157901ms step_avg:99.94ms
step:1581/1770 train_time:158008ms step_avg:99.94ms
step:1582/1770 train_time:158113ms step_avg:99.95ms
step:1583/1770 train_time:158219ms step_avg:99.95ms
step:1584/1770 train_time:158324ms step_avg:99.95ms
step:1585/1770 train_time:158428ms step_avg:99.95ms
step:1586/1770 train_time:158536ms step_avg:99.96ms
step:1587/1770 train_time:158642ms step_avg:99.96ms
step:1588/1770 train_time:158747ms step_avg:99.97ms
step:1589/1770 train_time:158853ms step_avg:99.97ms
step:1590/1770 train_time:158958ms step_avg:99.97ms
step:1591/1770 train_time:159062ms step_avg:99.98ms
step:1592/1770 train_time:159168ms step_avg:99.98ms
step:1593/1770 train_time:159272ms step_avg:99.98ms
step:1594/1770 train_time:159377ms step_avg:99.99ms
step:1595/1770 train_time:159481ms step_avg:99.99ms
step:1596/1770 train_time:159587ms step_avg:99.99ms
step:1597/1770 train_time:159691ms step_avg:99.99ms
step:1598/1770 train_time:159796ms step_avg:100.00ms
step:1599/1770 train_time:159903ms step_avg:100.00ms
step:1600/1770 train_time:160009ms step_avg:100.01ms
step:1601/1770 train_time:160114ms step_avg:100.01ms
step:1602/1770 train_time:160220ms step_avg:100.01ms
step:1603/1770 train_time:160325ms step_avg:100.02ms
step:1604/1770 train_time:160429ms step_avg:100.02ms
step:1605/1770 train_time:160533ms step_avg:100.02ms
step:1606/1770 train_time:160638ms step_avg:100.02ms
step:1607/1770 train_time:160746ms step_avg:100.03ms
step:1608/1770 train_time:160851ms step_avg:100.03ms
step:1609/1770 train_time:160954ms step_avg:100.03ms
step:1610/1770 train_time:161061ms step_avg:100.04ms
step:1611/1770 train_time:161167ms step_avg:100.04ms
step:1612/1770 train_time:161272ms step_avg:100.04ms
step:1613/1770 train_time:161377ms step_avg:100.05ms
step:1614/1770 train_time:161482ms step_avg:100.05ms
step:1615/1770 train_time:161586ms step_avg:100.05ms
step:1616/1770 train_time:161689ms step_avg:100.06ms
step:1617/1770 train_time:161795ms step_avg:100.06ms
step:1618/1770 train_time:161902ms step_avg:100.06ms
step:1619/1770 train_time:162006ms step_avg:100.07ms
step:1620/1770 train_time:162111ms step_avg:100.07ms
step:1621/1770 train_time:162216ms step_avg:100.07ms
step:1622/1770 train_time:162322ms step_avg:100.08ms
step:1623/1770 train_time:162429ms step_avg:100.08ms
step:1624/1770 train_time:162535ms step_avg:100.08ms
step:1625/1770 train_time:162638ms step_avg:100.09ms
step:1625/1770 val_loss:3.3104 train_time:162742ms step_avg:100.15ms
step:1626/1770 train_time:162760ms step_avg:100.10ms
step:1627/1770 train_time:162856ms step_avg:100.10ms
step:1628/1770 train_time:162960ms step_avg:100.10ms
step:1629/1770 train_time:163065ms step_avg:100.10ms
step:1630/1770 train_time:163169ms step_avg:100.10ms
step:1631/1770 train_time:163273ms step_avg:100.11ms
step:1632/1770 train_time:163378ms step_avg:100.11ms
step:1633/1770 train_time:163482ms step_avg:100.11ms
step:1634/1770 train_time:163588ms step_avg:100.11ms
step:1635/1770 train_time:163693ms step_avg:100.12ms
step:1636/1770 train_time:163800ms step_avg:100.12ms
step:1637/1770 train_time:163906ms step_avg:100.13ms
step:1638/1770 train_time:164010ms step_avg:100.13ms
step:1639/1770 train_time:164115ms step_avg:100.13ms
step:1640/1770 train_time:164219ms step_avg:100.13ms
step:1641/1770 train_time:164325ms step_avg:100.14ms
step:1642/1770 train_time:164428ms step_avg:100.14ms
step:1643/1770 train_time:164532ms step_avg:100.14ms
step:1644/1770 train_time:164638ms step_avg:100.14ms
step:1645/1770 train_time:164743ms step_avg:100.15ms
step:1646/1770 train_time:164852ms step_avg:100.15ms
step:1647/1770 train_time:164957ms step_avg:100.16ms
step:1648/1770 train_time:165062ms step_avg:100.16ms
step:1649/1770 train_time:165167ms step_avg:100.16ms
step:1650/1770 train_time:165272ms step_avg:100.16ms
step:1651/1770 train_time:165375ms step_avg:100.17ms
step:1652/1770 train_time:165480ms step_avg:100.17ms
step:1653/1770 train_time:165585ms step_avg:100.17ms
step:1654/1770 train_time:165692ms step_avg:100.18ms
step:1655/1770 train_time:165800ms step_avg:100.18ms
step:1656/1770 train_time:165904ms step_avg:100.18ms
step:1657/1770 train_time:166011ms step_avg:100.19ms
step:1658/1770 train_time:166115ms step_avg:100.19ms
step:1659/1770 train_time:166221ms step_avg:100.19ms
step:1660/1770 train_time:166325ms step_avg:100.20ms
step:1661/1770 train_time:166430ms step_avg:100.20ms
step:1662/1770 train_time:166534ms step_avg:100.20ms
step:1663/1770 train_time:166638ms step_avg:100.20ms
step:1664/1770 train_time:166744ms step_avg:100.21ms
step:1665/1770 train_time:166848ms step_avg:100.21ms
step:1666/1770 train_time:166954ms step_avg:100.21ms
step:1667/1770 train_time:167058ms step_avg:100.21ms
step:1668/1770 train_time:167163ms step_avg:100.22ms
step:1669/1770 train_time:167268ms step_avg:100.22ms
step:1670/1770 train_time:167372ms step_avg:100.22ms
step:1671/1770 train_time:167477ms step_avg:100.23ms
step:1672/1770 train_time:167582ms step_avg:100.23ms
step:1673/1770 train_time:167688ms step_avg:100.23ms
step:1674/1770 train_time:167794ms step_avg:100.24ms
step:1675/1770 train_time:167898ms step_avg:100.24ms
step:1676/1770 train_time:168004ms step_avg:100.24ms
step:1677/1770 train_time:168112ms step_avg:100.25ms
step:1678/1770 train_time:168216ms step_avg:100.25ms
step:1679/1770 train_time:168322ms step_avg:100.25ms
step:1680/1770 train_time:168427ms step_avg:100.25ms
step:1681/1770 train_time:168532ms step_avg:100.26ms
step:1682/1770 train_time:168639ms step_avg:100.26ms
step:1683/1770 train_time:168744ms step_avg:100.26ms
step:1684/1770 train_time:168849ms step_avg:100.27ms
step:1685/1770 train_time:168953ms step_avg:100.27ms
step:1686/1770 train_time:169060ms step_avg:100.27ms
step:1687/1770 train_time:169166ms step_avg:100.28ms
step:1688/1770 train_time:169272ms step_avg:100.28ms
step:1689/1770 train_time:169376ms step_avg:100.28ms
step:1690/1770 train_time:169482ms step_avg:100.29ms
step:1691/1770 train_time:169587ms step_avg:100.29ms
step:1692/1770 train_time:169692ms step_avg:100.29ms
step:1693/1770 train_time:169797ms step_avg:100.29ms
step:1694/1770 train_time:169901ms step_avg:100.30ms
step:1695/1770 train_time:170008ms step_avg:100.30ms
step:1696/1770 train_time:170114ms step_avg:100.30ms
step:1697/1770 train_time:170220ms step_avg:100.31ms
step:1698/1770 train_time:170327ms step_avg:100.31ms
step:1699/1770 train_time:170431ms step_avg:100.31ms
step:1700/1770 train_time:170535ms step_avg:100.31ms
step:1701/1770 train_time:170639ms step_avg:100.32ms
step:1702/1770 train_time:170744ms step_avg:100.32ms
step:1703/1770 train_time:170848ms step_avg:100.32ms
step:1704/1770 train_time:170953ms step_avg:100.32ms
step:1705/1770 train_time:171057ms step_avg:100.33ms
step:1706/1770 train_time:171161ms step_avg:100.33ms
step:1707/1770 train_time:171269ms step_avg:100.33ms
step:1708/1770 train_time:171374ms step_avg:100.34ms
step:1709/1770 train_time:171480ms step_avg:100.34ms
step:1710/1770 train_time:171588ms step_avg:100.34ms
step:1711/1770 train_time:171695ms step_avg:100.35ms
step:1712/1770 train_time:171800ms step_avg:100.35ms
step:1713/1770 train_time:171905ms step_avg:100.35ms
step:1714/1770 train_time:172011ms step_avg:100.36ms
step:1715/1770 train_time:172115ms step_avg:100.36ms
step:1716/1770 train_time:172222ms step_avg:100.36ms
step:1717/1770 train_time:172328ms step_avg:100.37ms
step:1718/1770 train_time:172433ms step_avg:100.37ms
step:1719/1770 train_time:172539ms step_avg:100.37ms
step:1720/1770 train_time:172646ms step_avg:100.38ms
step:1721/1770 train_time:172751ms step_avg:100.38ms
step:1722/1770 train_time:172859ms step_avg:100.38ms
step:1723/1770 train_time:172965ms step_avg:100.39ms
step:1724/1770 train_time:173072ms step_avg:100.39ms
step:1725/1770 train_time:173180ms step_avg:100.39ms
step:1726/1770 train_time:173286ms step_avg:100.40ms
step:1727/1770 train_time:173391ms step_avg:100.40ms
step:1728/1770 train_time:173498ms step_avg:100.40ms
step:1729/1770 train_time:173603ms step_avg:100.41ms
step:1730/1770 train_time:173710ms step_avg:100.41ms
step:1731/1770 train_time:173817ms step_avg:100.41ms
step:1732/1770 train_time:173923ms step_avg:100.42ms
step:1733/1770 train_time:174029ms step_avg:100.42ms
step:1734/1770 train_time:174134ms step_avg:100.42ms
step:1735/1770 train_time:174240ms step_avg:100.43ms
step:1736/1770 train_time:174345ms step_avg:100.43ms
step:1737/1770 train_time:174451ms step_avg:100.43ms
step:1738/1770 train_time:174556ms step_avg:100.44ms
step:1739/1770 train_time:174662ms step_avg:100.44ms
step:1740/1770 train_time:174768ms step_avg:100.44ms
step:1741/1770 train_time:174877ms step_avg:100.45ms
step:1742/1770 train_time:174984ms step_avg:100.45ms
step:1743/1770 train_time:175091ms step_avg:100.45ms
step:1744/1770 train_time:175196ms step_avg:100.46ms
step:1745/1770 train_time:175301ms step_avg:100.46ms
step:1746/1770 train_time:175409ms step_avg:100.46ms
step:1747/1770 train_time:175513ms step_avg:100.47ms
step:1748/1770 train_time:175621ms step_avg:100.47ms
step:1749/1770 train_time:175729ms step_avg:100.47ms
step:1750/1770 train_time:175835ms step_avg:100.48ms
step:1750/1770 val_loss:3.2835 train_time:175940ms step_avg:100.54ms
step:1751/1770 train_time:175957ms step_avg:100.49ms
step:1752/1770 train_time:176055ms step_avg:100.49ms
step:1753/1770 train_time:176162ms step_avg:100.49ms
step:1754/1770 train_time:176268ms step_avg:100.50ms
step:1755/1770 train_time:176373ms step_avg:100.50ms
step:1756/1770 train_time:176478ms step_avg:100.50ms
step:1757/1770 train_time:176584ms step_avg:100.50ms
step:1758/1770 train_time:176689ms step_avg:100.51ms
step:1759/1770 train_time:176794ms step_avg:100.51ms
step:1760/1770 train_time:176899ms step_avg:100.51ms
step:1761/1770 train_time:177007ms step_avg:100.52ms
step:1762/1770 train_time:177116ms step_avg:100.52ms
step:1763/1770 train_time:177221ms step_avg:100.52ms
step:1764/1770 train_time:177327ms step_avg:100.53ms
step:1765/1770 train_time:177432ms step_avg:100.53ms
step:1766/1770 train_time:177541ms step_avg:100.53ms
step:1767/1770 train_time:177645ms step_avg:100.54ms
step:1768/1770 train_time:177750ms step_avg:100.54ms
step:1769/1770 train_time:177854ms step_avg:100.54ms
step:1770/1770 train_time:177960ms step_avg:100.54ms
step:1770/1770 val_loss:3.2804 train_time:178066ms step_avg:100.60ms
peak memory allocated: 30724 MiB reserved: 46492 MiB
