import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
import copy
import glob
from dataclasses import dataclass
from functools import lru_cache, partial # Added partial for hook registration
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
#torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params: list[Tensor] = [*params]
        param_groups = []
        for size in {p.numel() for p in params}:
            b = torch.empty(world_size, size, dtype=torch.bfloat16, device="cuda")
            group = dict(params=[p for p in params if p.numel() == size],
                         update_buffer=b, update_buffer_views=[b[i] for i in range(world_size)])
            param_groups.append(group)
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            update_buffer: Tensor = group["update_buffer"]
            update_buffer_views: list[Tensor] = group["update_buffer_views"]
            # generate weight updates in distributed fashion
            params: list[Tensor] = group["params"]
            handle = None
            params_world = None
            def update_prev(): # optimized Muon implementation contributed by @YouJiacheng
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffer_views):
                    p_world.add_(g_world.view_as(p_world),
                                 alpha=-group["lr"] * max(1, p_world.size(-2) / p_world.size(-1))**0.5)
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if "momentum_buffer" not in state:
                        state["momentum_buffer"] = torch.zeros_like(g)
                    buf: Tensor = state["momentum_buffer"]
                    buf.lerp_(g, 1 - group["momentum"])
                    g = g.lerp_(buf, group["momentum"]) if group["nesterov"] else buf
                    g = zeropower_via_newtonschulz5(g, steps=group["ns_steps"]).flatten()
                else:
                    g = update_buffer_views[self.rank]
                if base_i > 0:
                    update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=0.12).transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, block_mask: BlockMask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, max_seq_len, i) for i in range(num_layers)])
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128),
                                    use_fp8=True, x_s=(model_dim**0.5)/448, w_s=24/448, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        self.skip_weights = nn.Parameter(torch.ones(num_layers//2))

    def create_blockmasks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        docs = (input_seq == 50256).cumsum(0)

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_blockmask: Tensor):
            num_blocks = dense_blockmask.sum(dim=-1, dtype=torch.int32)
            indices = dense_blockmask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
        causal_blockmask_any = block_idx[:, None] >= block_idx
        causal_blockmask_all = block_idx[:, None] > block_idx
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
        document_blockmask_any = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
        document_blockmask_all = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
        blockmask_any = causal_blockmask_any & document_blockmask_any
        blockmask_all = causal_blockmask_all & document_blockmask_all
        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(blockmask_any & ~blockmask_all)
        full_kv_num_blocks, full_kv_indices = dense_to_ordered(blockmask_all)
        def build_bm(window_size_blocks: Tensor) -> BlockMask:
            return BlockMask.from_kv_blocks(
                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(window_size_blocks - full_kv_num_blocks, 1)),
                partial_kv_indices,
                torch.clamp_max(full_kv_num_blocks, window_size_blocks - 1),
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        long_bm, short_bm = self.create_blockmasks(input_seq, sliding_window_num_blocks)
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, long_bm, short_bm, short_bm, short_bm, long_bm]
        assert len(block_masks) == len(self.blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977

        # U-net design by @brendanh0gan
        skip_connections = []
        n = len(self.skip_weights)
        for i in range(len(self.blocks)):
            if i >= n:
                x = x + self.skip_weights[i - n] * skip_connections.pop()
            x = self.blocks[i](x, ve[i], x0, block_masks[i])
            if i < n:
                skip_connections.append(x)

        x = norm(x)
        logits = self.lm_head(x).float()
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / (7.5 * x.size(-1)**0.5))
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq, reduction='sum' if self.training else 'mean')
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn't helpful.
        pos += batch_size
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_seq_len = 48*1024 # FlexAttention sequence length
    val_seq_len = 4*64*1024 # FlexAttention sequence length for validation
    # optimization
    num_iterations = 1770 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    # architecture
    vocab_size = 50257
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert world_size == 8 # this code is designed for 8xH100
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs("logs_bucket", exist_ok=True)
    logfile = f"logs_bucket/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

########################################
#    Construct model and optimizer     #
########################################

model: nn.Module = GPT(vocab_size=args.vocab_size, num_layers=12, num_heads=6, model_dim=768,
                       max_seq_len=max(args.train_seq_len, args.val_seq_len)).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
adam_params = [dict(params=head_params, lr=0.22), dict(params=embed_params, lr=0.6), dict(params=scalar_params, lr=0.04)]
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = torch.optim.Adam(adam_params, betas=(0.8, 0.95), eps=1e-10, fused=True)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then decay
def get_lr(step: int):
    x = step / args.num_iterations # progress in training
    assert 0 <= x < 1
    if x < 1 - args.cooldown_frac:
        return 1.0
    else:
        w = (1 - x) / args.cooldown_frac
        return w * 1.0 + (1 - w) * 0.1

# attention window size schedule: linearly increase
@lru_cache(1)
def get_window_size_blocks_helper(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)
def get_window_size_blocks(step: int):
    x = step / args.num_iterations # progress in training
    assert 0 <= x <= 1
    # Linearly increase the block-wise sliding window size over training 128 -> 1792
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * x, n=128)
    return get_window_size_blocks_helper(window_size)

model: nn.Module = torch.compile(model, dynamic=False)

########################################
#     Warmup kernel and Layerwise Bucketing   #
########################################

# Track the gradient readiness order during warmup
gradient_ready_order = []

def record_grad_order_hook(param):
    def hook(*_):
        if param.grad is not None and all(param is not p for p in gradient_ready_order):
            gradient_ready_order.append(param)
    return hook

# Register temp hooks to track when each parameter's gradient is ready
tmp_handles = []
for p in model.parameters():
    if p.requires_grad:
        handle = p.register_post_accumulate_grad_hook(record_grad_order_hook(p))
        tmp_handles.append(handle)

warmup_steps = 10
initial_state = dict(
    model=copy.deepcopy(model.state_dict()),
    optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]
)

for step in range(warmup_steps):
    # Create a local generator with fixed, step-dependent seed
    g = torch.Generator(device="cuda")
    g.manual_seed(1337 + step)

    # Use this generator to sample inputs
    inputs = targets = torch.randint(
        low=0,
        high=args.vocab_size,
        size=(args.train_seq_len,),
        device="cuda",
        dtype=torch.long,
        generator=g,  # <- Local generator ensures deterministic and isolated RNG
    )
    model(inputs.to(torch.int32), targets, get_window_size_blocks(0)).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)

# Remove hooks after warmup
for h in tmp_handles:
    h.remove()

# Restore model/optimizer state to ensure a clean start
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del initial_state

print0(f"Collected {len(gradient_ready_order)} parameters in gradient-ready order")

########################################
#      Overlap Communication Setup     #
########################################

# Layer-aware bucketing using gradient-ready order
def create_buckets_layerwise(ordered_params, bucket_size_mb=25):
    """Group parameters into buckets of ~bucket_size_mb MB, in gradient ready order"""
    buckets = []
    current_bucket = []
    current_size = 0

    for param in ordered_params:
        if not param.requires_grad:
            continue
        param_size_mb = param.numel() * param.element_size() / (1024 * 1024)

        if current_size + param_size_mb > bucket_size_mb and current_bucket:
            buckets.append(current_bucket)
            current_bucket = [param]
            current_size = param_size_mb
        else:
            current_bucket.append(param)
            current_size += param_size_mb

    if current_bucket:
        buckets.append(current_bucket)

    return buckets

# Create buckets for all parameters
all_params = [p for p in model.parameters() if p.requires_grad]
param_buckets = create_buckets_layerwise(gradient_ready_order)
print0(f"Created {len(param_buckets)} gradient buckets")
for i, bucket in enumerate(param_buckets):
    total_size = sum(p.numel() * p.element_size() for p in bucket) / (1024 * 1024)
    print0(f"Bucket {i}: {len(bucket)} params, {total_size:.1f} MB")

# Bucket state tracking
bucket_ready_count = [0] * len(param_buckets)
bucket_handles = [None] * len(param_buckets)
param_to_bucket = {}

# Map each parameter to its bucket index
for bucket_idx, bucket in enumerate(param_buckets):
    for param in bucket:
        param_to_bucket[param] = bucket_idx

def _gradient_hook(param: Tensor):
    """Called when a parameter's gradient is ready"""
    if param.grad is None:
        return

    bucket_idx = param_to_bucket[param]
    bucket_ready_count[bucket_idx] += 1

    # Check if all parameters in this bucket are ready
    if bucket_ready_count[bucket_idx] == len(param_buckets[bucket_idx]):
        # All-reduce this bucket
        bucket_grads = [p.grad for p in param_buckets[bucket_idx]]

        # For multi-tensor operations, we can reduce them together
        if len(bucket_grads) == 1:
            handle = dist.all_reduce(bucket_grads[0], op=dist.ReduceOp.AVG, async_op=True)
        else:
            # Use multi-tensor all-reduce for efficiency
            handle = dist.all_reduce_coalesced(bucket_grads, op=dist.ReduceOp.AVG, async_op=True)

        bucket_handles[bucket_idx] = handle

# Register hooks for all parameters
print0("Registering bucketed gradient hooks...")
for param in all_params:
    param.register_post_accumulate_grad_hook(_gradient_hook)

def wait_for_gradients():
    """Wait for all gradient reductions to complete and reset bucket state"""
    for handle in bucket_handles:
        if handle is not None:
            handle.wait()

    # Reset state for next iteration
    for i in range(len(bucket_ready_count)):
        bucket_ready_count[i] = 0
        bucket_handles[i] = None

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, world_size * args.train_seq_len, rank, world_size)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_batch_size = world_size * args.val_seq_len
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        val_loader = distributed_data_generator(args.val_files, val_batch_size, rank, world_size)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets = next(val_loader)
                val_loss += model(inputs, targets, get_window_size_blocks(step))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs_bucket/{run_id}", exist_ok=True)
            torch.save(log, f"logs_bucket/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    inputs, targets = next(train_loader)
    model(inputs, targets, get_window_size_blocks(step)).backward()
    #for param in model.parameters():
    #    dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
    wait_for_gradients() # does the same thing as commented two lines above, but faster

    # set optimization hyperparameters
    for opt in optimizers:
        for group in opt.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)
    for group in optimizer2.param_groups:
        frac = min(step / 300, 1) # momentum warmup for muon
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers
    for opt in optimizers:
        opt.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0]
Running PyTorch 2.8.0.dev20250612+cu126 compiled for CUDA 12.6
Fri Jun 13 06:31:28 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   38C    P0            117W /  700W |    5824MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   29C    P0            110W /  700W |    1514MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   29C    P0            113W /  700W |    1514MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   37C    P0            121W /  700W |    1514MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   38C    P0            117W /  700W |    1514MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   30C    P0            113W /  700W |    1514MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   37C    P0            119W /  700W |    1514MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   30C    P0            114W /  700W |    1514MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
Collected 75 parameters in gradient-ready order
Created 21 gradient buckets
Bucket 0: 1 params, 73.6 MB
Bucket 1: 1 params, 73.6 MB
Bucket 2: 4 params, 15.8 MB
Bucket 3: 1 params, 147.4 MB
Bucket 4: 4 params, 20.3 MB
Bucket 5: 4 params, 24.8 MB
Bucket 6: 4 params, 18.0 MB
Bucket 7: 5 params, 18.0 MB
Bucket 8: 4 params, 20.3 MB
Bucket 9: 5 params, 24.8 MB
Bucket 10: 4 params, 20.3 MB
Bucket 11: 4 params, 24.8 MB
Bucket 12: 5 params, 18.0 MB
Bucket 13: 5 params, 20.3 MB
Bucket 14: 5 params, 18.0 MB
Bucket 15: 4 params, 20.3 MB
Bucket 16: 5 params, 24.8 MB
Bucket 17: 4 params, 24.8 MB
Bucket 18: 4 params, 2.3 MB
Bucket 19: 1 params, 73.6 MB
Bucket 20: 1 params, 73.6 MB
Registering bucketed gradient hooks...
step:0/1770 val_loss:10.8258 train_time:0ms step_avg:0.02ms
step:1/1770 train_time:57ms step_avg:57.35ms
step:2/1770 train_time:133ms step_avg:66.62ms
step:3/1770 train_time:222ms step_avg:73.87ms
step:4/1770 train_time:315ms step_avg:78.81ms
step:5/1770 train_time:409ms step_avg:81.87ms
step:6/1770 train_time:503ms step_avg:83.86ms
step:7/1770 train_time:597ms step_avg:85.30ms
step:8/1770 train_time:691ms step_avg:86.35ms
step:9/1770 train_time:784ms step_avg:87.14ms
step:10/1770 train_time:879ms step_avg:87.90ms
step:11/1770 train_time:973ms step_avg:88.45ms
step:12/1770 train_time:1067ms step_avg:88.95ms
step:13/1770 train_time:1163ms step_avg:89.48ms
step:14/1770 train_time:1260ms step_avg:90.00ms
step:15/1770 train_time:1356ms step_avg:90.41ms
step:16/1770 train_time:1451ms step_avg:90.72ms
step:17/1770 train_time:1545ms step_avg:90.89ms
step:18/1770 train_time:1640ms step_avg:91.14ms
step:19/1770 train_time:1735ms step_avg:91.30ms
step:20/1770 train_time:1829ms step_avg:91.43ms
step:21/1770 train_time:1923ms step_avg:91.56ms
step:22/1770 train_time:2018ms step_avg:91.72ms
step:23/1770 train_time:2113ms step_avg:91.86ms
step:24/1770 train_time:2207ms step_avg:91.98ms
step:25/1770 train_time:2302ms step_avg:92.09ms
step:26/1770 train_time:2398ms step_avg:92.23ms
step:27/1770 train_time:2493ms step_avg:92.33ms
step:28/1770 train_time:2588ms step_avg:92.42ms
step:29/1770 train_time:2683ms step_avg:92.52ms
step:30/1770 train_time:2778ms step_avg:92.59ms
step:31/1770 train_time:2872ms step_avg:92.64ms
step:32/1770 train_time:2966ms step_avg:92.68ms
step:33/1770 train_time:3061ms step_avg:92.74ms
step:34/1770 train_time:3156ms step_avg:92.81ms
step:35/1770 train_time:3250ms step_avg:92.86ms
step:36/1770 train_time:3345ms step_avg:92.90ms
step:37/1770 train_time:3441ms step_avg:92.99ms
step:38/1770 train_time:3537ms step_avg:93.08ms
step:39/1770 train_time:3632ms step_avg:93.12ms
step:40/1770 train_time:3726ms step_avg:93.16ms
step:41/1770 train_time:3820ms step_avg:93.18ms
step:42/1770 train_time:3915ms step_avg:93.21ms
step:43/1770 train_time:4010ms step_avg:93.25ms
step:44/1770 train_time:4103ms step_avg:93.26ms
step:45/1770 train_time:4198ms step_avg:93.29ms
step:46/1770 train_time:4293ms step_avg:93.32ms
step:47/1770 train_time:4387ms step_avg:93.34ms
step:48/1770 train_time:4481ms step_avg:93.36ms
step:49/1770 train_time:4576ms step_avg:93.39ms
step:50/1770 train_time:4671ms step_avg:93.42ms
step:51/1770 train_time:4765ms step_avg:93.43ms
step:52/1770 train_time:4860ms step_avg:93.47ms
step:53/1770 train_time:4956ms step_avg:93.51ms
step:54/1770 train_time:5050ms step_avg:93.52ms
step:55/1770 train_time:5146ms step_avg:93.56ms
step:56/1770 train_time:5241ms step_avg:93.58ms
step:57/1770 train_time:5336ms step_avg:93.61ms
step:58/1770 train_time:5430ms step_avg:93.63ms
step:59/1770 train_time:5525ms step_avg:93.64ms
step:60/1770 train_time:5620ms step_avg:93.67ms
step:61/1770 train_time:5715ms step_avg:93.68ms
step:62/1770 train_time:5809ms step_avg:93.69ms
step:63/1770 train_time:5903ms step_avg:93.69ms
step:64/1770 train_time:5998ms step_avg:93.72ms
step:65/1770 train_time:6092ms step_avg:93.73ms
step:66/1770 train_time:6186ms step_avg:93.73ms
step:67/1770 train_time:6281ms step_avg:93.75ms
step:68/1770 train_time:6376ms step_avg:93.77ms
step:69/1770 train_time:6471ms step_avg:93.78ms
step:70/1770 train_time:6565ms step_avg:93.79ms
step:71/1770 train_time:6661ms step_avg:93.81ms
step:72/1770 train_time:6755ms step_avg:93.82ms
step:73/1770 train_time:6849ms step_avg:93.83ms
step:74/1770 train_time:6944ms step_avg:93.84ms
step:75/1770 train_time:7039ms step_avg:93.86ms
step:76/1770 train_time:7134ms step_avg:93.87ms
step:77/1770 train_time:7228ms step_avg:93.87ms
step:78/1770 train_time:7322ms step_avg:93.87ms
step:79/1770 train_time:7417ms step_avg:93.88ms
step:80/1770 train_time:7511ms step_avg:93.89ms
step:81/1770 train_time:7605ms step_avg:93.89ms
step:82/1770 train_time:7700ms step_avg:93.90ms
step:83/1770 train_time:7795ms step_avg:93.91ms
step:84/1770 train_time:7889ms step_avg:93.92ms
step:85/1770 train_time:7983ms step_avg:93.92ms
step:86/1770 train_time:8078ms step_avg:93.93ms
step:87/1770 train_time:8174ms step_avg:93.96ms
step:88/1770 train_time:8268ms step_avg:93.96ms
step:89/1770 train_time:8362ms step_avg:93.96ms
step:90/1770 train_time:8458ms step_avg:93.98ms
step:91/1770 train_time:8553ms step_avg:93.99ms
step:92/1770 train_time:8647ms step_avg:93.99ms
step:93/1770 train_time:8741ms step_avg:93.99ms
step:94/1770 train_time:8836ms step_avg:94.00ms
step:95/1770 train_time:8931ms step_avg:94.01ms
step:96/1770 train_time:9025ms step_avg:94.01ms
step:97/1770 train_time:9120ms step_avg:94.02ms
step:98/1770 train_time:9214ms step_avg:94.03ms
step:99/1770 train_time:9309ms step_avg:94.03ms
step:100/1770 train_time:9402ms step_avg:94.02ms
step:101/1770 train_time:9497ms step_avg:94.03ms
step:102/1770 train_time:9591ms step_avg:94.03ms
step:103/1770 train_time:9685ms step_avg:94.03ms
step:104/1770 train_time:9780ms step_avg:94.04ms
step:105/1770 train_time:9875ms step_avg:94.05ms
step:106/1770 train_time:9970ms step_avg:94.06ms
step:107/1770 train_time:10064ms step_avg:94.06ms
step:108/1770 train_time:10159ms step_avg:94.06ms
step:109/1770 train_time:10254ms step_avg:94.07ms
step:110/1770 train_time:10348ms step_avg:94.07ms
step:111/1770 train_time:10443ms step_avg:94.08ms
step:112/1770 train_time:10538ms step_avg:94.09ms
step:113/1770 train_time:10632ms step_avg:94.09ms
step:114/1770 train_time:10727ms step_avg:94.09ms
step:115/1770 train_time:10821ms step_avg:94.10ms
step:116/1770 train_time:10916ms step_avg:94.10ms
step:117/1770 train_time:11010ms step_avg:94.10ms
step:118/1770 train_time:11104ms step_avg:94.10ms
step:119/1770 train_time:11199ms step_avg:94.11ms
step:120/1770 train_time:11294ms step_avg:94.11ms
step:121/1770 train_time:11388ms step_avg:94.12ms
step:122/1770 train_time:11483ms step_avg:94.12ms
step:123/1770 train_time:11578ms step_avg:94.13ms
step:124/1770 train_time:11672ms step_avg:94.13ms
step:125/1770 train_time:11766ms step_avg:94.13ms
step:125/1770 val_loss:4.6302 train_time:11860ms step_avg:94.88ms
step:126/1770 train_time:11880ms step_avg:94.28ms
step:127/1770 train_time:11964ms step_avg:94.20ms
step:128/1770 train_time:12068ms step_avg:94.28ms
step:129/1770 train_time:12165ms step_avg:94.30ms
step:130/1770 train_time:12260ms step_avg:94.31ms
step:131/1770 train_time:12355ms step_avg:94.31ms
step:132/1770 train_time:12449ms step_avg:94.31ms
step:133/1770 train_time:12542ms step_avg:94.30ms
step:134/1770 train_time:12637ms step_avg:94.31ms
step:135/1770 train_time:12731ms step_avg:94.31ms
step:136/1770 train_time:12826ms step_avg:94.31ms
step:137/1770 train_time:12921ms step_avg:94.32ms
step:138/1770 train_time:13018ms step_avg:94.33ms
step:139/1770 train_time:13114ms step_avg:94.34ms
step:140/1770 train_time:13209ms step_avg:94.35ms
step:141/1770 train_time:13305ms step_avg:94.36ms
step:142/1770 train_time:13401ms step_avg:94.38ms
step:143/1770 train_time:13496ms step_avg:94.38ms
step:144/1770 train_time:13591ms step_avg:94.38ms
step:145/1770 train_time:13685ms step_avg:94.38ms
step:146/1770 train_time:13780ms step_avg:94.39ms
step:147/1770 train_time:13875ms step_avg:94.39ms
step:148/1770 train_time:13970ms step_avg:94.39ms
step:149/1770 train_time:14065ms step_avg:94.40ms
step:150/1770 train_time:14161ms step_avg:94.41ms
step:151/1770 train_time:14256ms step_avg:94.41ms
step:152/1770 train_time:14351ms step_avg:94.42ms
step:153/1770 train_time:14446ms step_avg:94.42ms
step:154/1770 train_time:14542ms step_avg:94.43ms
step:155/1770 train_time:14638ms step_avg:94.44ms
step:156/1770 train_time:14732ms step_avg:94.44ms
step:157/1770 train_time:14827ms step_avg:94.44ms
step:158/1770 train_time:14922ms step_avg:94.45ms
step:159/1770 train_time:15018ms step_avg:94.45ms
step:160/1770 train_time:15113ms step_avg:94.45ms
step:161/1770 train_time:15207ms step_avg:94.46ms
step:162/1770 train_time:15302ms step_avg:94.46ms
step:163/1770 train_time:15397ms step_avg:94.46ms
step:164/1770 train_time:15492ms step_avg:94.47ms
step:165/1770 train_time:15587ms step_avg:94.47ms
step:166/1770 train_time:15683ms step_avg:94.48ms
step:167/1770 train_time:15779ms step_avg:94.49ms
step:168/1770 train_time:15874ms step_avg:94.49ms
step:169/1770 train_time:15969ms step_avg:94.49ms
step:170/1770 train_time:16065ms step_avg:94.50ms
step:171/1770 train_time:16162ms step_avg:94.51ms
step:172/1770 train_time:16258ms step_avg:94.52ms
step:173/1770 train_time:16353ms step_avg:94.53ms
step:174/1770 train_time:16449ms step_avg:94.53ms
step:175/1770 train_time:16543ms step_avg:94.53ms
step:176/1770 train_time:16638ms step_avg:94.54ms
step:177/1770 train_time:16733ms step_avg:94.54ms
step:178/1770 train_time:16828ms step_avg:94.54ms
step:179/1770 train_time:16923ms step_avg:94.54ms
step:180/1770 train_time:17018ms step_avg:94.54ms
step:181/1770 train_time:17113ms step_avg:94.55ms
step:182/1770 train_time:17208ms step_avg:94.55ms
step:183/1770 train_time:17303ms step_avg:94.55ms
step:184/1770 train_time:17400ms step_avg:94.56ms
step:185/1770 train_time:17495ms step_avg:94.57ms
step:186/1770 train_time:17590ms step_avg:94.57ms
step:187/1770 train_time:17685ms step_avg:94.57ms
step:188/1770 train_time:17781ms step_avg:94.58ms
step:189/1770 train_time:17876ms step_avg:94.58ms
step:190/1770 train_time:17970ms step_avg:94.58ms
step:191/1770 train_time:18066ms step_avg:94.59ms
step:192/1770 train_time:18162ms step_avg:94.59ms
step:193/1770 train_time:18257ms step_avg:94.59ms
step:194/1770 train_time:18352ms step_avg:94.60ms
step:195/1770 train_time:18447ms step_avg:94.60ms
step:196/1770 train_time:18543ms step_avg:94.61ms
step:197/1770 train_time:18638ms step_avg:94.61ms
step:198/1770 train_time:18732ms step_avg:94.61ms
step:199/1770 train_time:18828ms step_avg:94.61ms
step:200/1770 train_time:18923ms step_avg:94.61ms
step:201/1770 train_time:19019ms step_avg:94.62ms
step:202/1770 train_time:19114ms step_avg:94.62ms
step:203/1770 train_time:19208ms step_avg:94.62ms
step:204/1770 train_time:19303ms step_avg:94.62ms
step:205/1770 train_time:19399ms step_avg:94.63ms
step:206/1770 train_time:19495ms step_avg:94.63ms
step:207/1770 train_time:19589ms step_avg:94.63ms
step:208/1770 train_time:19684ms step_avg:94.64ms
step:209/1770 train_time:19780ms step_avg:94.64ms
step:210/1770 train_time:19875ms step_avg:94.64ms
step:211/1770 train_time:19969ms step_avg:94.64ms
step:212/1770 train_time:20065ms step_avg:94.65ms
step:213/1770 train_time:20160ms step_avg:94.65ms
step:214/1770 train_time:20255ms step_avg:94.65ms
step:215/1770 train_time:20350ms step_avg:94.65ms
step:216/1770 train_time:20446ms step_avg:94.66ms
step:217/1770 train_time:20542ms step_avg:94.66ms
step:218/1770 train_time:20637ms step_avg:94.66ms
step:219/1770 train_time:20732ms step_avg:94.67ms
step:220/1770 train_time:20826ms step_avg:94.67ms
step:221/1770 train_time:20922ms step_avg:94.67ms
step:222/1770 train_time:21017ms step_avg:94.67ms
step:223/1770 train_time:21113ms step_avg:94.68ms
step:224/1770 train_time:21208ms step_avg:94.68ms
step:225/1770 train_time:21303ms step_avg:94.68ms
step:226/1770 train_time:21398ms step_avg:94.68ms
step:227/1770 train_time:21493ms step_avg:94.68ms
step:228/1770 train_time:21588ms step_avg:94.68ms
step:229/1770 train_time:21683ms step_avg:94.69ms
step:230/1770 train_time:21778ms step_avg:94.69ms
step:231/1770 train_time:21873ms step_avg:94.69ms
step:232/1770 train_time:21967ms step_avg:94.69ms
step:233/1770 train_time:22062ms step_avg:94.69ms
step:234/1770 train_time:22157ms step_avg:94.69ms
step:235/1770 train_time:22252ms step_avg:94.69ms
step:236/1770 train_time:22347ms step_avg:94.69ms
step:237/1770 train_time:22444ms step_avg:94.70ms
step:238/1770 train_time:22539ms step_avg:94.70ms
step:239/1770 train_time:22634ms step_avg:94.70ms
step:240/1770 train_time:22729ms step_avg:94.70ms
step:241/1770 train_time:22824ms step_avg:94.71ms
step:242/1770 train_time:22920ms step_avg:94.71ms
step:243/1770 train_time:23015ms step_avg:94.71ms
step:244/1770 train_time:23110ms step_avg:94.71ms
step:245/1770 train_time:23205ms step_avg:94.71ms
step:246/1770 train_time:23301ms step_avg:94.72ms
step:247/1770 train_time:23396ms step_avg:94.72ms
step:248/1770 train_time:23491ms step_avg:94.72ms
step:249/1770 train_time:23586ms step_avg:94.72ms
step:250/1770 train_time:23682ms step_avg:94.73ms
step:250/1770 val_loss:4.0969 train_time:23776ms step_avg:95.11ms
step:251/1770 train_time:23794ms step_avg:94.80ms
step:252/1770 train_time:23878ms step_avg:94.75ms
step:253/1770 train_time:23975ms step_avg:94.76ms
step:254/1770 train_time:24071ms step_avg:94.77ms
step:255/1770 train_time:24165ms step_avg:94.77ms
step:256/1770 train_time:24259ms step_avg:94.76ms
step:257/1770 train_time:24354ms step_avg:94.76ms
step:258/1770 train_time:24448ms step_avg:94.76ms
step:259/1770 train_time:24542ms step_avg:94.76ms
step:260/1770 train_time:24637ms step_avg:94.76ms
step:261/1770 train_time:24732ms step_avg:94.76ms
step:262/1770 train_time:24827ms step_avg:94.76ms
step:263/1770 train_time:24923ms step_avg:94.76ms
step:264/1770 train_time:25019ms step_avg:94.77ms
step:265/1770 train_time:25116ms step_avg:94.78ms
step:266/1770 train_time:25212ms step_avg:94.78ms
step:267/1770 train_time:25307ms step_avg:94.78ms
step:268/1770 train_time:25401ms step_avg:94.78ms
step:269/1770 train_time:25496ms step_avg:94.78ms
step:270/1770 train_time:25591ms step_avg:94.78ms
step:271/1770 train_time:25686ms step_avg:94.78ms
step:272/1770 train_time:25782ms step_avg:94.79ms
step:273/1770 train_time:25878ms step_avg:94.79ms
step:274/1770 train_time:25974ms step_avg:94.80ms
step:275/1770 train_time:26070ms step_avg:94.80ms
step:276/1770 train_time:26165ms step_avg:94.80ms
step:277/1770 train_time:26261ms step_avg:94.80ms
step:278/1770 train_time:26356ms step_avg:94.81ms
step:279/1770 train_time:26452ms step_avg:94.81ms
step:280/1770 train_time:26547ms step_avg:94.81ms
step:281/1770 train_time:26642ms step_avg:94.81ms
step:282/1770 train_time:26738ms step_avg:94.82ms
step:283/1770 train_time:26835ms step_avg:94.82ms
step:284/1770 train_time:26930ms step_avg:94.82ms
step:285/1770 train_time:27026ms step_avg:94.83ms
step:286/1770 train_time:27121ms step_avg:94.83ms
step:287/1770 train_time:27217ms step_avg:94.83ms
step:288/1770 train_time:27313ms step_avg:94.84ms
step:289/1770 train_time:27409ms step_avg:94.84ms
step:290/1770 train_time:27504ms step_avg:94.84ms
step:291/1770 train_time:27599ms step_avg:94.84ms
step:292/1770 train_time:27695ms step_avg:94.85ms
step:293/1770 train_time:27791ms step_avg:94.85ms
step:294/1770 train_time:27886ms step_avg:94.85ms
step:295/1770 train_time:27982ms step_avg:94.86ms
step:296/1770 train_time:28079ms step_avg:94.86ms
step:297/1770 train_time:28175ms step_avg:94.87ms
step:298/1770 train_time:28270ms step_avg:94.87ms
step:299/1770 train_time:28365ms step_avg:94.87ms
step:300/1770 train_time:28461ms step_avg:94.87ms
step:301/1770 train_time:28556ms step_avg:94.87ms
step:302/1770 train_time:28652ms step_avg:94.87ms
step:303/1770 train_time:28747ms step_avg:94.87ms
step:304/1770 train_time:28843ms step_avg:94.88ms
step:305/1770 train_time:28938ms step_avg:94.88ms
step:306/1770 train_time:29034ms step_avg:94.88ms
step:307/1770 train_time:29130ms step_avg:94.89ms
step:308/1770 train_time:29225ms step_avg:94.89ms
step:309/1770 train_time:29320ms step_avg:94.89ms
step:310/1770 train_time:29416ms step_avg:94.89ms
step:311/1770 train_time:29512ms step_avg:94.89ms
step:312/1770 train_time:29608ms step_avg:94.90ms
step:313/1770 train_time:29703ms step_avg:94.90ms
step:314/1770 train_time:29800ms step_avg:94.90ms
step:315/1770 train_time:29896ms step_avg:94.91ms
step:316/1770 train_time:29993ms step_avg:94.91ms
step:317/1770 train_time:30088ms step_avg:94.92ms
step:318/1770 train_time:30185ms step_avg:94.92ms
step:319/1770 train_time:30281ms step_avg:94.92ms
step:320/1770 train_time:30377ms step_avg:94.93ms
step:321/1770 train_time:30473ms step_avg:94.93ms
step:322/1770 train_time:30569ms step_avg:94.93ms
step:323/1770 train_time:30664ms step_avg:94.93ms
step:324/1770 train_time:30759ms step_avg:94.94ms
step:325/1770 train_time:30855ms step_avg:94.94ms
step:326/1770 train_time:30952ms step_avg:94.94ms
step:327/1770 train_time:31048ms step_avg:94.95ms
step:328/1770 train_time:31144ms step_avg:94.95ms
step:329/1770 train_time:31239ms step_avg:94.95ms
step:330/1770 train_time:31335ms step_avg:94.96ms
step:331/1770 train_time:31431ms step_avg:94.96ms
step:332/1770 train_time:31527ms step_avg:94.96ms
step:333/1770 train_time:31622ms step_avg:94.96ms
step:334/1770 train_time:31719ms step_avg:94.97ms
step:335/1770 train_time:31814ms step_avg:94.97ms
step:336/1770 train_time:31910ms step_avg:94.97ms
step:337/1770 train_time:32005ms step_avg:94.97ms
step:338/1770 train_time:32101ms step_avg:94.97ms
step:339/1770 train_time:32198ms step_avg:94.98ms
step:340/1770 train_time:32293ms step_avg:94.98ms
step:341/1770 train_time:32388ms step_avg:94.98ms
step:342/1770 train_time:32484ms step_avg:94.98ms
step:343/1770 train_time:32579ms step_avg:94.98ms
step:344/1770 train_time:32675ms step_avg:94.98ms
step:345/1770 train_time:32770ms step_avg:94.99ms
step:346/1770 train_time:32865ms step_avg:94.99ms
step:347/1770 train_time:32961ms step_avg:94.99ms
step:348/1770 train_time:33056ms step_avg:94.99ms
step:349/1770 train_time:33153ms step_avg:94.99ms
step:350/1770 train_time:33248ms step_avg:94.99ms
step:351/1770 train_time:33343ms step_avg:95.00ms
step:352/1770 train_time:33439ms step_avg:95.00ms
step:353/1770 train_time:33535ms step_avg:95.00ms
step:354/1770 train_time:33630ms step_avg:95.00ms
step:355/1770 train_time:33726ms step_avg:95.00ms
step:356/1770 train_time:33821ms step_avg:95.00ms
step:357/1770 train_time:33918ms step_avg:95.01ms
step:358/1770 train_time:34013ms step_avg:95.01ms
step:359/1770 train_time:34109ms step_avg:95.01ms
step:360/1770 train_time:34204ms step_avg:95.01ms
step:361/1770 train_time:34301ms step_avg:95.02ms
step:362/1770 train_time:34397ms step_avg:95.02ms
step:363/1770 train_time:34493ms step_avg:95.02ms
step:364/1770 train_time:34588ms step_avg:95.02ms
step:365/1770 train_time:34683ms step_avg:95.02ms
step:366/1770 train_time:34778ms step_avg:95.02ms
step:367/1770 train_time:34874ms step_avg:95.03ms
step:368/1770 train_time:34970ms step_avg:95.03ms
step:369/1770 train_time:35065ms step_avg:95.03ms
step:370/1770 train_time:35160ms step_avg:95.03ms
step:371/1770 train_time:35257ms step_avg:95.03ms
step:372/1770 train_time:35353ms step_avg:95.03ms
step:373/1770 train_time:35448ms step_avg:95.04ms
step:374/1770 train_time:35543ms step_avg:95.04ms
step:375/1770 train_time:35639ms step_avg:95.04ms
step:375/1770 val_loss:3.9014 train_time:35735ms step_avg:95.29ms
step:376/1770 train_time:35752ms step_avg:95.09ms
step:377/1770 train_time:35839ms step_avg:95.06ms
step:378/1770 train_time:35936ms step_avg:95.07ms
step:379/1770 train_time:36032ms step_avg:95.07ms
step:380/1770 train_time:36128ms step_avg:95.07ms
step:381/1770 train_time:36223ms step_avg:95.07ms
step:382/1770 train_time:36319ms step_avg:95.08ms
step:383/1770 train_time:36413ms step_avg:95.07ms
step:384/1770 train_time:36508ms step_avg:95.07ms
step:385/1770 train_time:36603ms step_avg:95.07ms
step:386/1770 train_time:36698ms step_avg:95.07ms
step:387/1770 train_time:36795ms step_avg:95.08ms
step:388/1770 train_time:36892ms step_avg:95.08ms
step:389/1770 train_time:36988ms step_avg:95.09ms
step:390/1770 train_time:37084ms step_avg:95.09ms
step:391/1770 train_time:37180ms step_avg:95.09ms
step:392/1770 train_time:37275ms step_avg:95.09ms
step:393/1770 train_time:37372ms step_avg:95.09ms
step:394/1770 train_time:37467ms step_avg:95.09ms
step:395/1770 train_time:37562ms step_avg:95.09ms
step:396/1770 train_time:37659ms step_avg:95.10ms
step:397/1770 train_time:37757ms step_avg:95.10ms
step:398/1770 train_time:37856ms step_avg:95.12ms
step:399/1770 train_time:37954ms step_avg:95.12ms
step:400/1770 train_time:38053ms step_avg:95.13ms
step:401/1770 train_time:38151ms step_avg:95.14ms
step:402/1770 train_time:38250ms step_avg:95.15ms
step:403/1770 train_time:38348ms step_avg:95.16ms
step:404/1770 train_time:38445ms step_avg:95.16ms
step:405/1770 train_time:38543ms step_avg:95.17ms
step:406/1770 train_time:38640ms step_avg:95.17ms
step:407/1770 train_time:38737ms step_avg:95.18ms
step:408/1770 train_time:38835ms step_avg:95.18ms
step:409/1770 train_time:38934ms step_avg:95.19ms
step:410/1770 train_time:39033ms step_avg:95.20ms
step:411/1770 train_time:39132ms step_avg:95.21ms
step:412/1770 train_time:39230ms step_avg:95.22ms
step:413/1770 train_time:39327ms step_avg:95.22ms
step:414/1770 train_time:39425ms step_avg:95.23ms
step:415/1770 train_time:39523ms step_avg:95.24ms
step:416/1770 train_time:39622ms step_avg:95.24ms
step:417/1770 train_time:39719ms step_avg:95.25ms
step:418/1770 train_time:39817ms step_avg:95.26ms
step:419/1770 train_time:39915ms step_avg:95.26ms
step:420/1770 train_time:40013ms step_avg:95.27ms
step:421/1770 train_time:40111ms step_avg:95.28ms
step:422/1770 train_time:40209ms step_avg:95.28ms
step:423/1770 train_time:40308ms step_avg:95.29ms
step:424/1770 train_time:40405ms step_avg:95.30ms
step:425/1770 train_time:40503ms step_avg:95.30ms
step:426/1770 train_time:40601ms step_avg:95.31ms
step:427/1770 train_time:40699ms step_avg:95.31ms
step:428/1770 train_time:40796ms step_avg:95.32ms
step:429/1770 train_time:40894ms step_avg:95.32ms
step:430/1770 train_time:40992ms step_avg:95.33ms
step:431/1770 train_time:41090ms step_avg:95.34ms
step:432/1770 train_time:41188ms step_avg:95.34ms
step:433/1770 train_time:41285ms step_avg:95.35ms
step:434/1770 train_time:41383ms step_avg:95.35ms
step:435/1770 train_time:41481ms step_avg:95.36ms
step:436/1770 train_time:41579ms step_avg:95.36ms
step:437/1770 train_time:41678ms step_avg:95.37ms
step:438/1770 train_time:41775ms step_avg:95.38ms
step:439/1770 train_time:41873ms step_avg:95.38ms
step:440/1770 train_time:41972ms step_avg:95.39ms
step:441/1770 train_time:42069ms step_avg:95.40ms
step:442/1770 train_time:42167ms step_avg:95.40ms
step:443/1770 train_time:42266ms step_avg:95.41ms
step:444/1770 train_time:42364ms step_avg:95.41ms
step:445/1770 train_time:42462ms step_avg:95.42ms
step:446/1770 train_time:42561ms step_avg:95.43ms
step:447/1770 train_time:42661ms step_avg:95.44ms
step:448/1770 train_time:42758ms step_avg:95.44ms
step:449/1770 train_time:42856ms step_avg:95.45ms
step:450/1770 train_time:42954ms step_avg:95.45ms
step:451/1770 train_time:43052ms step_avg:95.46ms
step:452/1770 train_time:43150ms step_avg:95.46ms
step:453/1770 train_time:43247ms step_avg:95.47ms
step:454/1770 train_time:43345ms step_avg:95.47ms
step:455/1770 train_time:43443ms step_avg:95.48ms
step:456/1770 train_time:43541ms step_avg:95.48ms
step:457/1770 train_time:43638ms step_avg:95.49ms
step:458/1770 train_time:43736ms step_avg:95.49ms
step:459/1770 train_time:43834ms step_avg:95.50ms
step:460/1770 train_time:43932ms step_avg:95.50ms
step:461/1770 train_time:44029ms step_avg:95.51ms
step:462/1770 train_time:44127ms step_avg:95.51ms
step:463/1770 train_time:44225ms step_avg:95.52ms
step:464/1770 train_time:44322ms step_avg:95.52ms
step:465/1770 train_time:44420ms step_avg:95.53ms
step:466/1770 train_time:44519ms step_avg:95.53ms
step:467/1770 train_time:44617ms step_avg:95.54ms
step:468/1770 train_time:44715ms step_avg:95.54ms
step:469/1770 train_time:44812ms step_avg:95.55ms
step:470/1770 train_time:44910ms step_avg:95.55ms
step:471/1770 train_time:45008ms step_avg:95.56ms
step:472/1770 train_time:45105ms step_avg:95.56ms
step:473/1770 train_time:45202ms step_avg:95.57ms
step:474/1770 train_time:45300ms step_avg:95.57ms
step:475/1770 train_time:45398ms step_avg:95.58ms
step:476/1770 train_time:45497ms step_avg:95.58ms
step:477/1770 train_time:45596ms step_avg:95.59ms
step:478/1770 train_time:45694ms step_avg:95.59ms
step:479/1770 train_time:45792ms step_avg:95.60ms
step:480/1770 train_time:45890ms step_avg:95.60ms
step:481/1770 train_time:45988ms step_avg:95.61ms
step:482/1770 train_time:46086ms step_avg:95.61ms
step:483/1770 train_time:46184ms step_avg:95.62ms
step:484/1770 train_time:46282ms step_avg:95.62ms
step:485/1770 train_time:46380ms step_avg:95.63ms
step:486/1770 train_time:46478ms step_avg:95.63ms
step:487/1770 train_time:46576ms step_avg:95.64ms
step:488/1770 train_time:46674ms step_avg:95.64ms
step:489/1770 train_time:46772ms step_avg:95.65ms
step:490/1770 train_time:46871ms step_avg:95.65ms
step:491/1770 train_time:46969ms step_avg:95.66ms
step:492/1770 train_time:47067ms step_avg:95.67ms
step:493/1770 train_time:47166ms step_avg:95.67ms
step:494/1770 train_time:47264ms step_avg:95.68ms
step:495/1770 train_time:47363ms step_avg:95.68ms
step:496/1770 train_time:47461ms step_avg:95.69ms
step:497/1770 train_time:47559ms step_avg:95.69ms
step:498/1770 train_time:47656ms step_avg:95.70ms
step:499/1770 train_time:47754ms step_avg:95.70ms
step:500/1770 train_time:47853ms step_avg:95.71ms
step:500/1770 val_loss:3.7519 train_time:47950ms step_avg:95.90ms
step:501/1770 train_time:47968ms step_avg:95.75ms
step:502/1770 train_time:48054ms step_avg:95.72ms
step:503/1770 train_time:48152ms step_avg:95.73ms
step:504/1770 train_time:48250ms step_avg:95.73ms
step:505/1770 train_time:48349ms step_avg:95.74ms
step:506/1770 train_time:48447ms step_avg:95.74ms
step:507/1770 train_time:48545ms step_avg:95.75ms
step:508/1770 train_time:48643ms step_avg:95.75ms
step:509/1770 train_time:48740ms step_avg:95.76ms
step:510/1770 train_time:48837ms step_avg:95.76ms
step:511/1770 train_time:48935ms step_avg:95.76ms
step:512/1770 train_time:49035ms step_avg:95.77ms
step:513/1770 train_time:49133ms step_avg:95.78ms
step:514/1770 train_time:49231ms step_avg:95.78ms
step:515/1770 train_time:49329ms step_avg:95.78ms
step:516/1770 train_time:49427ms step_avg:95.79ms
step:517/1770 train_time:49525ms step_avg:95.79ms
step:518/1770 train_time:49623ms step_avg:95.80ms
step:519/1770 train_time:49720ms step_avg:95.80ms
step:520/1770 train_time:49818ms step_avg:95.80ms
step:521/1770 train_time:49916ms step_avg:95.81ms
step:522/1770 train_time:50014ms step_avg:95.81ms
step:523/1770 train_time:50112ms step_avg:95.82ms
step:524/1770 train_time:50210ms step_avg:95.82ms
step:525/1770 train_time:50308ms step_avg:95.82ms
step:526/1770 train_time:50406ms step_avg:95.83ms
step:527/1770 train_time:50506ms step_avg:95.84ms
step:528/1770 train_time:50604ms step_avg:95.84ms
step:529/1770 train_time:50703ms step_avg:95.85ms
step:530/1770 train_time:50802ms step_avg:95.85ms
step:531/1770 train_time:50902ms step_avg:95.86ms
step:532/1770 train_time:51002ms step_avg:95.87ms
step:533/1770 train_time:51101ms step_avg:95.87ms
step:534/1770 train_time:51200ms step_avg:95.88ms
step:535/1770 train_time:51300ms step_avg:95.89ms
step:536/1770 train_time:51399ms step_avg:95.89ms
step:537/1770 train_time:51497ms step_avg:95.90ms
step:538/1770 train_time:51596ms step_avg:95.90ms
step:539/1770 train_time:51693ms step_avg:95.91ms
step:540/1770 train_time:51791ms step_avg:95.91ms
step:541/1770 train_time:51889ms step_avg:95.91ms
step:542/1770 train_time:51988ms step_avg:95.92ms
step:543/1770 train_time:52087ms step_avg:95.92ms
step:544/1770 train_time:52187ms step_avg:95.93ms
step:545/1770 train_time:52287ms step_avg:95.94ms
step:546/1770 train_time:52386ms step_avg:95.94ms
step:547/1770 train_time:52485ms step_avg:95.95ms
step:548/1770 train_time:52585ms step_avg:95.96ms
step:549/1770 train_time:52684ms step_avg:95.96ms
step:550/1770 train_time:52784ms step_avg:95.97ms
step:551/1770 train_time:52882ms step_avg:95.97ms
step:552/1770 train_time:52980ms step_avg:95.98ms
step:553/1770 train_time:53078ms step_avg:95.98ms
step:554/1770 train_time:53176ms step_avg:95.99ms
step:555/1770 train_time:53274ms step_avg:95.99ms
step:556/1770 train_time:53372ms step_avg:95.99ms
step:557/1770 train_time:53470ms step_avg:96.00ms
step:558/1770 train_time:53569ms step_avg:96.00ms
step:559/1770 train_time:53668ms step_avg:96.01ms
step:560/1770 train_time:53768ms step_avg:96.01ms
step:561/1770 train_time:53866ms step_avg:96.02ms
step:562/1770 train_time:53964ms step_avg:96.02ms
step:563/1770 train_time:54063ms step_avg:96.03ms
step:564/1770 train_time:54162ms step_avg:96.03ms
step:565/1770 train_time:54261ms step_avg:96.04ms
step:566/1770 train_time:54360ms step_avg:96.04ms
step:567/1770 train_time:54459ms step_avg:96.05ms
step:568/1770 train_time:54558ms step_avg:96.05ms
step:569/1770 train_time:54656ms step_avg:96.06ms
step:570/1770 train_time:54754ms step_avg:96.06ms
step:571/1770 train_time:54853ms step_avg:96.06ms
step:572/1770 train_time:54951ms step_avg:96.07ms
step:573/1770 train_time:55048ms step_avg:96.07ms
step:574/1770 train_time:55146ms step_avg:96.07ms
step:575/1770 train_time:55245ms step_avg:96.08ms
step:576/1770 train_time:55343ms step_avg:96.08ms
step:577/1770 train_time:55442ms step_avg:96.09ms
step:578/1770 train_time:55542ms step_avg:96.09ms
step:579/1770 train_time:55641ms step_avg:96.10ms
step:580/1770 train_time:55740ms step_avg:96.10ms
step:581/1770 train_time:55838ms step_avg:96.11ms
step:582/1770 train_time:55937ms step_avg:96.11ms
step:583/1770 train_time:56035ms step_avg:96.12ms
step:584/1770 train_time:56133ms step_avg:96.12ms
step:585/1770 train_time:56231ms step_avg:96.12ms
step:586/1770 train_time:56329ms step_avg:96.12ms
step:587/1770 train_time:56427ms step_avg:96.13ms
step:588/1770 train_time:56525ms step_avg:96.13ms
step:589/1770 train_time:56624ms step_avg:96.14ms
step:590/1770 train_time:56724ms step_avg:96.14ms
step:591/1770 train_time:56823ms step_avg:96.15ms
step:592/1770 train_time:56921ms step_avg:96.15ms
step:593/1770 train_time:57020ms step_avg:96.16ms
step:594/1770 train_time:57120ms step_avg:96.16ms
step:595/1770 train_time:57219ms step_avg:96.17ms
step:596/1770 train_time:57317ms step_avg:96.17ms
step:597/1770 train_time:57415ms step_avg:96.17ms
step:598/1770 train_time:57513ms step_avg:96.18ms
step:599/1770 train_time:57611ms step_avg:96.18ms
step:600/1770 train_time:57709ms step_avg:96.18ms
step:601/1770 train_time:57808ms step_avg:96.19ms
step:602/1770 train_time:57906ms step_avg:96.19ms
step:603/1770 train_time:58006ms step_avg:96.20ms
step:604/1770 train_time:58105ms step_avg:96.20ms
step:605/1770 train_time:58204ms step_avg:96.20ms
step:606/1770 train_time:58303ms step_avg:96.21ms
step:607/1770 train_time:58402ms step_avg:96.21ms
step:608/1770 train_time:58502ms step_avg:96.22ms
step:609/1770 train_time:58601ms step_avg:96.22ms
step:610/1770 train_time:58700ms step_avg:96.23ms
step:611/1770 train_time:58798ms step_avg:96.23ms
step:612/1770 train_time:58896ms step_avg:96.23ms
step:613/1770 train_time:58993ms step_avg:96.24ms
step:614/1770 train_time:59091ms step_avg:96.24ms
step:615/1770 train_time:59189ms step_avg:96.24ms
step:616/1770 train_time:59287ms step_avg:96.25ms
step:617/1770 train_time:59387ms step_avg:96.25ms
step:618/1770 train_time:59485ms step_avg:96.25ms
step:619/1770 train_time:59583ms step_avg:96.26ms
step:620/1770 train_time:59683ms step_avg:96.26ms
step:621/1770 train_time:59781ms step_avg:96.27ms
step:622/1770 train_time:59881ms step_avg:96.27ms
step:623/1770 train_time:59980ms step_avg:96.28ms
step:624/1770 train_time:60078ms step_avg:96.28ms
step:625/1770 train_time:60176ms step_avg:96.28ms
step:625/1770 val_loss:3.6628 train_time:60273ms step_avg:96.44ms
step:626/1770 train_time:60291ms step_avg:96.31ms
step:627/1770 train_time:60380ms step_avg:96.30ms
step:628/1770 train_time:60481ms step_avg:96.31ms
step:629/1770 train_time:60580ms step_avg:96.31ms
step:630/1770 train_time:60679ms step_avg:96.32ms
step:631/1770 train_time:60777ms step_avg:96.32ms
step:632/1770 train_time:60875ms step_avg:96.32ms
step:633/1770 train_time:60973ms step_avg:96.32ms
step:634/1770 train_time:61070ms step_avg:96.33ms
step:635/1770 train_time:61168ms step_avg:96.33ms
step:636/1770 train_time:61266ms step_avg:96.33ms
step:637/1770 train_time:61365ms step_avg:96.33ms
step:638/1770 train_time:61464ms step_avg:96.34ms
step:639/1770 train_time:61562ms step_avg:96.34ms
step:640/1770 train_time:61660ms step_avg:96.34ms
step:641/1770 train_time:61759ms step_avg:96.35ms
step:642/1770 train_time:61856ms step_avg:96.35ms
step:643/1770 train_time:61955ms step_avg:96.35ms
step:644/1770 train_time:62053ms step_avg:96.35ms
step:645/1770 train_time:62151ms step_avg:96.36ms
step:646/1770 train_time:62250ms step_avg:96.36ms
step:647/1770 train_time:62349ms step_avg:96.37ms
step:648/1770 train_time:62447ms step_avg:96.37ms
step:649/1770 train_time:62545ms step_avg:96.37ms
step:650/1770 train_time:62643ms step_avg:96.37ms
step:651/1770 train_time:62741ms step_avg:96.38ms
step:652/1770 train_time:62839ms step_avg:96.38ms
step:653/1770 train_time:62938ms step_avg:96.38ms
step:654/1770 train_time:63036ms step_avg:96.39ms
step:655/1770 train_time:63135ms step_avg:96.39ms
step:656/1770 train_time:63234ms step_avg:96.39ms
step:657/1770 train_time:63333ms step_avg:96.40ms
step:658/1770 train_time:63433ms step_avg:96.40ms
step:659/1770 train_time:63533ms step_avg:96.41ms
step:660/1770 train_time:63634ms step_avg:96.42ms
step:661/1770 train_time:63735ms step_avg:96.42ms
step:662/1770 train_time:63836ms step_avg:96.43ms
step:663/1770 train_time:63937ms step_avg:96.44ms
step:664/1770 train_time:64037ms step_avg:96.44ms
step:665/1770 train_time:64137ms step_avg:96.45ms
step:666/1770 train_time:64238ms step_avg:96.45ms
step:667/1770 train_time:64340ms step_avg:96.46ms
step:668/1770 train_time:64442ms step_avg:96.47ms
step:669/1770 train_time:64542ms step_avg:96.48ms
step:670/1770 train_time:64643ms step_avg:96.48ms
step:671/1770 train_time:64744ms step_avg:96.49ms
step:672/1770 train_time:64844ms step_avg:96.49ms
step:673/1770 train_time:64944ms step_avg:96.50ms
step:674/1770 train_time:65044ms step_avg:96.50ms
step:675/1770 train_time:65144ms step_avg:96.51ms
step:676/1770 train_time:65244ms step_avg:96.52ms
step:677/1770 train_time:65344ms step_avg:96.52ms
step:678/1770 train_time:65444ms step_avg:96.53ms
step:679/1770 train_time:65545ms step_avg:96.53ms
step:680/1770 train_time:65645ms step_avg:96.54ms
step:681/1770 train_time:65745ms step_avg:96.54ms
step:682/1770 train_time:65845ms step_avg:96.55ms
step:683/1770 train_time:65945ms step_avg:96.55ms
step:684/1770 train_time:66046ms step_avg:96.56ms
step:685/1770 train_time:66146ms step_avg:96.56ms
step:686/1770 train_time:66245ms step_avg:96.57ms
step:687/1770 train_time:66344ms step_avg:96.57ms
step:688/1770 train_time:66444ms step_avg:96.58ms
step:689/1770 train_time:66544ms step_avg:96.58ms
step:690/1770 train_time:66645ms step_avg:96.59ms
step:691/1770 train_time:66746ms step_avg:96.59ms
step:692/1770 train_time:66845ms step_avg:96.60ms
step:693/1770 train_time:66945ms step_avg:96.60ms
step:694/1770 train_time:67045ms step_avg:96.61ms
step:695/1770 train_time:67145ms step_avg:96.61ms
step:696/1770 train_time:67245ms step_avg:96.62ms
step:697/1770 train_time:67344ms step_avg:96.62ms
step:698/1770 train_time:67444ms step_avg:96.62ms
step:699/1770 train_time:67543ms step_avg:96.63ms
step:700/1770 train_time:67643ms step_avg:96.63ms
step:701/1770 train_time:67743ms step_avg:96.64ms
step:702/1770 train_time:67843ms step_avg:96.64ms
step:703/1770 train_time:67943ms step_avg:96.65ms
step:704/1770 train_time:68043ms step_avg:96.65ms
step:705/1770 train_time:68143ms step_avg:96.66ms
step:706/1770 train_time:68244ms step_avg:96.66ms
step:707/1770 train_time:68344ms step_avg:96.67ms
step:708/1770 train_time:68443ms step_avg:96.67ms
step:709/1770 train_time:68543ms step_avg:96.68ms
step:710/1770 train_time:68644ms step_avg:96.68ms
step:711/1770 train_time:68744ms step_avg:96.69ms
step:712/1770 train_time:68845ms step_avg:96.69ms
step:713/1770 train_time:68945ms step_avg:96.70ms
step:714/1770 train_time:69045ms step_avg:96.70ms
step:715/1770 train_time:69145ms step_avg:96.71ms
step:716/1770 train_time:69244ms step_avg:96.71ms
step:717/1770 train_time:69344ms step_avg:96.71ms
step:718/1770 train_time:69444ms step_avg:96.72ms
step:719/1770 train_time:69544ms step_avg:96.72ms
step:720/1770 train_time:69644ms step_avg:96.73ms
step:721/1770 train_time:69744ms step_avg:96.73ms
step:722/1770 train_time:69844ms step_avg:96.74ms
step:723/1770 train_time:69945ms step_avg:96.74ms
step:724/1770 train_time:70045ms step_avg:96.75ms
step:725/1770 train_time:70145ms step_avg:96.75ms
step:726/1770 train_time:70244ms step_avg:96.76ms
step:727/1770 train_time:70344ms step_avg:96.76ms
step:728/1770 train_time:70445ms step_avg:96.76ms
step:729/1770 train_time:70545ms step_avg:96.77ms
step:730/1770 train_time:70644ms step_avg:96.77ms
step:731/1770 train_time:70744ms step_avg:96.78ms
step:732/1770 train_time:70844ms step_avg:96.78ms
step:733/1770 train_time:70944ms step_avg:96.79ms
step:734/1770 train_time:71044ms step_avg:96.79ms
step:735/1770 train_time:71144ms step_avg:96.79ms
step:736/1770 train_time:71245ms step_avg:96.80ms
step:737/1770 train_time:71344ms step_avg:96.80ms
step:738/1770 train_time:71443ms step_avg:96.81ms
step:739/1770 train_time:71545ms step_avg:96.81ms
step:740/1770 train_time:71645ms step_avg:96.82ms
step:741/1770 train_time:71744ms step_avg:96.82ms
step:742/1770 train_time:71844ms step_avg:96.82ms
step:743/1770 train_time:71943ms step_avg:96.83ms
step:744/1770 train_time:72044ms step_avg:96.83ms
step:745/1770 train_time:72144ms step_avg:96.84ms
step:746/1770 train_time:72244ms step_avg:96.84ms
step:747/1770 train_time:72343ms step_avg:96.85ms
step:748/1770 train_time:72444ms step_avg:96.85ms
step:749/1770 train_time:72545ms step_avg:96.86ms
step:750/1770 train_time:72645ms step_avg:96.86ms
step:750/1770 val_loss:3.5982 train_time:72745ms step_avg:96.99ms
step:751/1770 train_time:72762ms step_avg:96.89ms
step:752/1770 train_time:72854ms step_avg:96.88ms
step:753/1770 train_time:72955ms step_avg:96.89ms
step:754/1770 train_time:73055ms step_avg:96.89ms
step:755/1770 train_time:73154ms step_avg:96.89ms
step:756/1770 train_time:73254ms step_avg:96.90ms
step:757/1770 train_time:73353ms step_avg:96.90ms
step:758/1770 train_time:73452ms step_avg:96.90ms
step:759/1770 train_time:73552ms step_avg:96.91ms
step:760/1770 train_time:73652ms step_avg:96.91ms
step:761/1770 train_time:73754ms step_avg:96.92ms
step:762/1770 train_time:73857ms step_avg:96.92ms
step:763/1770 train_time:73959ms step_avg:96.93ms
step:764/1770 train_time:74060ms step_avg:96.94ms
step:765/1770 train_time:74162ms step_avg:96.94ms
step:766/1770 train_time:74262ms step_avg:96.95ms
step:767/1770 train_time:74362ms step_avg:96.95ms
step:768/1770 train_time:74462ms step_avg:96.96ms
step:769/1770 train_time:74562ms step_avg:96.96ms
step:770/1770 train_time:74662ms step_avg:96.96ms
step:771/1770 train_time:74763ms step_avg:96.97ms
step:772/1770 train_time:74864ms step_avg:96.97ms
step:773/1770 train_time:74966ms step_avg:96.98ms
step:774/1770 train_time:75066ms step_avg:96.98ms
step:775/1770 train_time:75166ms step_avg:96.99ms
step:776/1770 train_time:75266ms step_avg:96.99ms
step:777/1770 train_time:75365ms step_avg:96.99ms
step:778/1770 train_time:75464ms step_avg:97.00ms
step:779/1770 train_time:75564ms step_avg:97.00ms
step:780/1770 train_time:75664ms step_avg:97.00ms
step:781/1770 train_time:75764ms step_avg:97.01ms
step:782/1770 train_time:75864ms step_avg:97.01ms
step:783/1770 train_time:75965ms step_avg:97.02ms
step:784/1770 train_time:76067ms step_avg:97.02ms
step:785/1770 train_time:76166ms step_avg:97.03ms
step:786/1770 train_time:76266ms step_avg:97.03ms
step:787/1770 train_time:76366ms step_avg:97.03ms
step:788/1770 train_time:76465ms step_avg:97.04ms
step:789/1770 train_time:76565ms step_avg:97.04ms
step:790/1770 train_time:76665ms step_avg:97.04ms
step:791/1770 train_time:76766ms step_avg:97.05ms
step:792/1770 train_time:76866ms step_avg:97.05ms
step:793/1770 train_time:76967ms step_avg:97.06ms
step:794/1770 train_time:77067ms step_avg:97.06ms
step:795/1770 train_time:77168ms step_avg:97.07ms
step:796/1770 train_time:77269ms step_avg:97.07ms
step:797/1770 train_time:77369ms step_avg:97.07ms
step:798/1770 train_time:77468ms step_avg:97.08ms
step:799/1770 train_time:77568ms step_avg:97.08ms
step:800/1770 train_time:77669ms step_avg:97.09ms
step:801/1770 train_time:77770ms step_avg:97.09ms
step:802/1770 train_time:77871ms step_avg:97.10ms
step:803/1770 train_time:77971ms step_avg:97.10ms
step:804/1770 train_time:78072ms step_avg:97.10ms
step:805/1770 train_time:78173ms step_avg:97.11ms
step:806/1770 train_time:78274ms step_avg:97.11ms
step:807/1770 train_time:78376ms step_avg:97.12ms
step:808/1770 train_time:78477ms step_avg:97.12ms
step:809/1770 train_time:78578ms step_avg:97.13ms
step:810/1770 train_time:78680ms step_avg:97.14ms
step:811/1770 train_time:78781ms step_avg:97.14ms
step:812/1770 train_time:78883ms step_avg:97.15ms
step:813/1770 train_time:78984ms step_avg:97.15ms
step:814/1770 train_time:79085ms step_avg:97.16ms
step:815/1770 train_time:79185ms step_avg:97.16ms
step:816/1770 train_time:79287ms step_avg:97.17ms
step:817/1770 train_time:79388ms step_avg:97.17ms
step:818/1770 train_time:79488ms step_avg:97.17ms
step:819/1770 train_time:79588ms step_avg:97.18ms
step:820/1770 train_time:79687ms step_avg:97.18ms
step:821/1770 train_time:79787ms step_avg:97.18ms
step:822/1770 train_time:79887ms step_avg:97.19ms
step:823/1770 train_time:79987ms step_avg:97.19ms
step:824/1770 train_time:80087ms step_avg:97.19ms
step:825/1770 train_time:80186ms step_avg:97.20ms
step:826/1770 train_time:80286ms step_avg:97.20ms
step:827/1770 train_time:80387ms step_avg:97.20ms
step:828/1770 train_time:80487ms step_avg:97.21ms
step:829/1770 train_time:80587ms step_avg:97.21ms
step:830/1770 train_time:80687ms step_avg:97.21ms
step:831/1770 train_time:80786ms step_avg:97.22ms
step:832/1770 train_time:80886ms step_avg:97.22ms
step:833/1770 train_time:80986ms step_avg:97.22ms
step:834/1770 train_time:81086ms step_avg:97.22ms
step:835/1770 train_time:81185ms step_avg:97.23ms
step:836/1770 train_time:81286ms step_avg:97.23ms
step:837/1770 train_time:81386ms step_avg:97.24ms
step:838/1770 train_time:81486ms step_avg:97.24ms
step:839/1770 train_time:81586ms step_avg:97.24ms
step:840/1770 train_time:81686ms step_avg:97.25ms
step:841/1770 train_time:81786ms step_avg:97.25ms
step:842/1770 train_time:81885ms step_avg:97.25ms
step:843/1770 train_time:81986ms step_avg:97.26ms
step:844/1770 train_time:82087ms step_avg:97.26ms
step:845/1770 train_time:82186ms step_avg:97.26ms
step:846/1770 train_time:82286ms step_avg:97.26ms
step:847/1770 train_time:82386ms step_avg:97.27ms
step:848/1770 train_time:82485ms step_avg:97.27ms
step:849/1770 train_time:82585ms step_avg:97.27ms
step:850/1770 train_time:82685ms step_avg:97.28ms
step:851/1770 train_time:82785ms step_avg:97.28ms
step:852/1770 train_time:82886ms step_avg:97.28ms
step:853/1770 train_time:82986ms step_avg:97.29ms
step:854/1770 train_time:83085ms step_avg:97.29ms
step:855/1770 train_time:83185ms step_avg:97.29ms
step:856/1770 train_time:83285ms step_avg:97.30ms
step:857/1770 train_time:83385ms step_avg:97.30ms
step:858/1770 train_time:83486ms step_avg:97.30ms
step:859/1770 train_time:83586ms step_avg:97.31ms
step:860/1770 train_time:83686ms step_avg:97.31ms
step:861/1770 train_time:83787ms step_avg:97.31ms
step:862/1770 train_time:83887ms step_avg:97.32ms
step:863/1770 train_time:83987ms step_avg:97.32ms
step:864/1770 train_time:84087ms step_avg:97.32ms
step:865/1770 train_time:84186ms step_avg:97.32ms
step:866/1770 train_time:84286ms step_avg:97.33ms
step:867/1770 train_time:84386ms step_avg:97.33ms
step:868/1770 train_time:84485ms step_avg:97.33ms
step:869/1770 train_time:84585ms step_avg:97.34ms
step:870/1770 train_time:84685ms step_avg:97.34ms
step:871/1770 train_time:84785ms step_avg:97.34ms
step:872/1770 train_time:84886ms step_avg:97.35ms
step:873/1770 train_time:84986ms step_avg:97.35ms
step:874/1770 train_time:85085ms step_avg:97.35ms
step:875/1770 train_time:85185ms step_avg:97.35ms
step:875/1770 val_loss:3.5491 train_time:85284ms step_avg:97.47ms
step:876/1770 train_time:85302ms step_avg:97.38ms
step:877/1770 train_time:85393ms step_avg:97.37ms
step:878/1770 train_time:85493ms step_avg:97.37ms
step:879/1770 train_time:85595ms step_avg:97.38ms
step:880/1770 train_time:85694ms step_avg:97.38ms
step:881/1770 train_time:85794ms step_avg:97.38ms
step:882/1770 train_time:85894ms step_avg:97.39ms
step:883/1770 train_time:85994ms step_avg:97.39ms
step:884/1770 train_time:86094ms step_avg:97.39ms
step:885/1770 train_time:86194ms step_avg:97.39ms
step:886/1770 train_time:86296ms step_avg:97.40ms
step:887/1770 train_time:86399ms step_avg:97.41ms
step:888/1770 train_time:86500ms step_avg:97.41ms
step:889/1770 train_time:86601ms step_avg:97.41ms
step:890/1770 train_time:86702ms step_avg:97.42ms
step:891/1770 train_time:86803ms step_avg:97.42ms
step:892/1770 train_time:86903ms step_avg:97.43ms
step:893/1770 train_time:87005ms step_avg:97.43ms
step:894/1770 train_time:87105ms step_avg:97.43ms
step:895/1770 train_time:87206ms step_avg:97.44ms
step:896/1770 train_time:87306ms step_avg:97.44ms
step:897/1770 train_time:87407ms step_avg:97.44ms
step:898/1770 train_time:87507ms step_avg:97.45ms
step:899/1770 train_time:87607ms step_avg:97.45ms
step:900/1770 train_time:87707ms step_avg:97.45ms
step:901/1770 train_time:87806ms step_avg:97.45ms
step:902/1770 train_time:87906ms step_avg:97.46ms
step:903/1770 train_time:88006ms step_avg:97.46ms
step:904/1770 train_time:88107ms step_avg:97.46ms
step:905/1770 train_time:88207ms step_avg:97.47ms
step:906/1770 train_time:88307ms step_avg:97.47ms
step:907/1770 train_time:88407ms step_avg:97.47ms
step:908/1770 train_time:88507ms step_avg:97.47ms
step:909/1770 train_time:88607ms step_avg:97.48ms
step:910/1770 train_time:88707ms step_avg:97.48ms
step:911/1770 train_time:88806ms step_avg:97.48ms
step:912/1770 train_time:88906ms step_avg:97.48ms
step:913/1770 train_time:89006ms step_avg:97.49ms
step:914/1770 train_time:89106ms step_avg:97.49ms
step:915/1770 train_time:89206ms step_avg:97.49ms
step:916/1770 train_time:89306ms step_avg:97.50ms
step:917/1770 train_time:89406ms step_avg:97.50ms
step:918/1770 train_time:89506ms step_avg:97.50ms
step:919/1770 train_time:89606ms step_avg:97.50ms
step:920/1770 train_time:89708ms step_avg:97.51ms
step:921/1770 train_time:89809ms step_avg:97.51ms
step:922/1770 train_time:89911ms step_avg:97.52ms
step:923/1770 train_time:90012ms step_avg:97.52ms
step:924/1770 train_time:90114ms step_avg:97.53ms
step:925/1770 train_time:90215ms step_avg:97.53ms
step:926/1770 train_time:90318ms step_avg:97.54ms
step:927/1770 train_time:90420ms step_avg:97.54ms
step:928/1770 train_time:90523ms step_avg:97.55ms
step:929/1770 train_time:90626ms step_avg:97.55ms
step:930/1770 train_time:90727ms step_avg:97.56ms
step:931/1770 train_time:90828ms step_avg:97.56ms
step:932/1770 train_time:90929ms step_avg:97.56ms
step:933/1770 train_time:91031ms step_avg:97.57ms
step:934/1770 train_time:91132ms step_avg:97.57ms
step:935/1770 train_time:91233ms step_avg:97.58ms
step:936/1770 train_time:91336ms step_avg:97.58ms
step:937/1770 train_time:91439ms step_avg:97.59ms
step:938/1770 train_time:91542ms step_avg:97.59ms
step:939/1770 train_time:91645ms step_avg:97.60ms
step:940/1770 train_time:91746ms step_avg:97.60ms
step:941/1770 train_time:91848ms step_avg:97.61ms
step:942/1770 train_time:91949ms step_avg:97.61ms
step:943/1770 train_time:92051ms step_avg:97.61ms
step:944/1770 train_time:92152ms step_avg:97.62ms
step:945/1770 train_time:92253ms step_avg:97.62ms
step:946/1770 train_time:92354ms step_avg:97.63ms
step:947/1770 train_time:92457ms step_avg:97.63ms
step:948/1770 train_time:92560ms step_avg:97.64ms
step:949/1770 train_time:92663ms step_avg:97.64ms
step:950/1770 train_time:92766ms step_avg:97.65ms
step:951/1770 train_time:92867ms step_avg:97.65ms
step:952/1770 train_time:92968ms step_avg:97.66ms
step:953/1770 train_time:93070ms step_avg:97.66ms
step:954/1770 train_time:93171ms step_avg:97.66ms
step:955/1770 train_time:93272ms step_avg:97.67ms
step:956/1770 train_time:93375ms step_avg:97.67ms
step:957/1770 train_time:93477ms step_avg:97.68ms
step:958/1770 train_time:93580ms step_avg:97.68ms
step:959/1770 train_time:93683ms step_avg:97.69ms
step:960/1770 train_time:93785ms step_avg:97.69ms
step:961/1770 train_time:93887ms step_avg:97.70ms
step:962/1770 train_time:93988ms step_avg:97.70ms
step:963/1770 train_time:94090ms step_avg:97.71ms
step:964/1770 train_time:94191ms step_avg:97.71ms
step:965/1770 train_time:94292ms step_avg:97.71ms
step:966/1770 train_time:94393ms step_avg:97.72ms
step:967/1770 train_time:94495ms step_avg:97.72ms
step:968/1770 train_time:94598ms step_avg:97.73ms
step:969/1770 train_time:94700ms step_avg:97.73ms
step:970/1770 train_time:94803ms step_avg:97.74ms
step:971/1770 train_time:94905ms step_avg:97.74ms
step:972/1770 train_time:95007ms step_avg:97.74ms
step:973/1770 train_time:95108ms step_avg:97.75ms
step:974/1770 train_time:95210ms step_avg:97.75ms
step:975/1770 train_time:95312ms step_avg:97.76ms
step:976/1770 train_time:95413ms step_avg:97.76ms
step:977/1770 train_time:95514ms step_avg:97.76ms
step:978/1770 train_time:95616ms step_avg:97.77ms
step:979/1770 train_time:95718ms step_avg:97.77ms
step:980/1770 train_time:95821ms step_avg:97.78ms
step:981/1770 train_time:95924ms step_avg:97.78ms
step:982/1770 train_time:96027ms step_avg:97.79ms
step:983/1770 train_time:96128ms step_avg:97.79ms
step:984/1770 train_time:96230ms step_avg:97.79ms
step:985/1770 train_time:96331ms step_avg:97.80ms
step:986/1770 train_time:96431ms step_avg:97.80ms
step:987/1770 train_time:96533ms step_avg:97.80ms
step:988/1770 train_time:96634ms step_avg:97.81ms
step:989/1770 train_time:96738ms step_avg:97.81ms
step:990/1770 train_time:96840ms step_avg:97.82ms
step:991/1770 train_time:96943ms step_avg:97.82ms
step:992/1770 train_time:97046ms step_avg:97.83ms
step:993/1770 train_time:97148ms step_avg:97.83ms
step:994/1770 train_time:97250ms step_avg:97.84ms
step:995/1770 train_time:97352ms step_avg:97.84ms
step:996/1770 train_time:97453ms step_avg:97.84ms
step:997/1770 train_time:97554ms step_avg:97.85ms
step:998/1770 train_time:97655ms step_avg:97.85ms
step:999/1770 train_time:97757ms step_avg:97.85ms
step:1000/1770 train_time:97861ms step_avg:97.86ms
step:1000/1770 val_loss:3.5118 train_time:97962ms step_avg:97.96ms
step:1001/1770 train_time:97982ms step_avg:97.88ms
step:1002/1770 train_time:98074ms step_avg:97.88ms
step:1003/1770 train_time:98177ms step_avg:97.88ms
step:1004/1770 train_time:98278ms step_avg:97.89ms
step:1005/1770 train_time:98380ms step_avg:97.89ms
step:1006/1770 train_time:98481ms step_avg:97.89ms
step:1007/1770 train_time:98582ms step_avg:97.90ms
step:1008/1770 train_time:98683ms step_avg:97.90ms
step:1009/1770 train_time:98784ms step_avg:97.90ms
step:1010/1770 train_time:98886ms step_avg:97.91ms
step:1011/1770 train_time:98992ms step_avg:97.92ms
step:1012/1770 train_time:99095ms step_avg:97.92ms
step:1013/1770 train_time:99197ms step_avg:97.92ms
step:1014/1770 train_time:99299ms step_avg:97.93ms
step:1015/1770 train_time:99400ms step_avg:97.93ms
step:1016/1770 train_time:99500ms step_avg:97.93ms
step:1017/1770 train_time:99602ms step_avg:97.94ms
step:1018/1770 train_time:99703ms step_avg:97.94ms
step:1019/1770 train_time:99805ms step_avg:97.94ms
step:1020/1770 train_time:99908ms step_avg:97.95ms
step:1021/1770 train_time:100012ms step_avg:97.95ms
step:1022/1770 train_time:100114ms step_avg:97.96ms
step:1023/1770 train_time:100216ms step_avg:97.96ms
step:1024/1770 train_time:100318ms step_avg:97.97ms
step:1025/1770 train_time:100419ms step_avg:97.97ms
step:1026/1770 train_time:100520ms step_avg:97.97ms
step:1027/1770 train_time:100622ms step_avg:97.98ms
step:1028/1770 train_time:100723ms step_avg:97.98ms
step:1029/1770 train_time:100824ms step_avg:97.98ms
step:1030/1770 train_time:100927ms step_avg:97.99ms
step:1031/1770 train_time:101030ms step_avg:97.99ms
step:1032/1770 train_time:101133ms step_avg:98.00ms
step:1033/1770 train_time:101235ms step_avg:98.00ms
step:1034/1770 train_time:101337ms step_avg:98.00ms
step:1035/1770 train_time:101438ms step_avg:98.01ms
step:1036/1770 train_time:101538ms step_avg:98.01ms
step:1037/1770 train_time:101639ms step_avg:98.01ms
step:1038/1770 train_time:101740ms step_avg:98.02ms
step:1039/1770 train_time:101842ms step_avg:98.02ms
step:1040/1770 train_time:101945ms step_avg:98.02ms
step:1041/1770 train_time:102046ms step_avg:98.03ms
step:1042/1770 train_time:102149ms step_avg:98.03ms
step:1043/1770 train_time:102253ms step_avg:98.04ms
step:1044/1770 train_time:102356ms step_avg:98.04ms
step:1045/1770 train_time:102457ms step_avg:98.04ms
step:1046/1770 train_time:102558ms step_avg:98.05ms
step:1047/1770 train_time:102659ms step_avg:98.05ms
step:1048/1770 train_time:102760ms step_avg:98.05ms
step:1049/1770 train_time:102862ms step_avg:98.06ms
step:1050/1770 train_time:102964ms step_avg:98.06ms
step:1051/1770 train_time:103067ms step_avg:98.07ms
step:1052/1770 train_time:103170ms step_avg:98.07ms
step:1053/1770 train_time:103274ms step_avg:98.08ms
step:1054/1770 train_time:103376ms step_avg:98.08ms
step:1055/1770 train_time:103478ms step_avg:98.08ms
step:1056/1770 train_time:103579ms step_avg:98.09ms
step:1057/1770 train_time:103680ms step_avg:98.09ms
step:1058/1770 train_time:103782ms step_avg:98.09ms
step:1059/1770 train_time:103884ms step_avg:98.10ms
step:1060/1770 train_time:103986ms step_avg:98.10ms
step:1061/1770 train_time:104090ms step_avg:98.11ms
step:1062/1770 train_time:104192ms step_avg:98.11ms
step:1063/1770 train_time:104295ms step_avg:98.11ms
step:1064/1770 train_time:104397ms step_avg:98.12ms
step:1065/1770 train_time:104499ms step_avg:98.12ms
step:1066/1770 train_time:104600ms step_avg:98.12ms
step:1067/1770 train_time:104702ms step_avg:98.13ms
step:1068/1770 train_time:104804ms step_avg:98.13ms
step:1069/1770 train_time:104906ms step_avg:98.13ms
step:1070/1770 train_time:105009ms step_avg:98.14ms
step:1071/1770 train_time:105112ms step_avg:98.14ms
step:1072/1770 train_time:105215ms step_avg:98.15ms
step:1073/1770 train_time:105317ms step_avg:98.15ms
step:1074/1770 train_time:105419ms step_avg:98.16ms
step:1075/1770 train_time:105520ms step_avg:98.16ms
step:1076/1770 train_time:105622ms step_avg:98.16ms
step:1077/1770 train_time:105725ms step_avg:98.17ms
step:1078/1770 train_time:105826ms step_avg:98.17ms
step:1079/1770 train_time:105929ms step_avg:98.17ms
step:1080/1770 train_time:106031ms step_avg:98.18ms
step:1081/1770 train_time:106134ms step_avg:98.18ms
step:1082/1770 train_time:106236ms step_avg:98.19ms
step:1083/1770 train_time:106338ms step_avg:98.19ms
step:1084/1770 train_time:106439ms step_avg:98.19ms
step:1085/1770 train_time:106541ms step_avg:98.19ms
step:1086/1770 train_time:106643ms step_avg:98.20ms
step:1087/1770 train_time:106745ms step_avg:98.20ms
step:1088/1770 train_time:106848ms step_avg:98.21ms
step:1089/1770 train_time:106951ms step_avg:98.21ms
step:1090/1770 train_time:107055ms step_avg:98.22ms
step:1091/1770 train_time:107157ms step_avg:98.22ms
step:1092/1770 train_time:107258ms step_avg:98.22ms
step:1093/1770 train_time:107360ms step_avg:98.22ms
step:1094/1770 train_time:107462ms step_avg:98.23ms
step:1095/1770 train_time:107564ms step_avg:98.23ms
step:1096/1770 train_time:107667ms step_avg:98.24ms
step:1097/1770 train_time:107769ms step_avg:98.24ms
step:1098/1770 train_time:107870ms step_avg:98.24ms
step:1099/1770 train_time:107973ms step_avg:98.25ms
step:1100/1770 train_time:108075ms step_avg:98.25ms
step:1101/1770 train_time:108176ms step_avg:98.25ms
step:1102/1770 train_time:108279ms step_avg:98.26ms
step:1103/1770 train_time:108380ms step_avg:98.26ms
step:1104/1770 train_time:108482ms step_avg:98.26ms
step:1105/1770 train_time:108583ms step_avg:98.27ms
step:1106/1770 train_time:108686ms step_avg:98.27ms
step:1107/1770 train_time:108788ms step_avg:98.27ms
step:1108/1770 train_time:108892ms step_avg:98.28ms
step:1109/1770 train_time:108994ms step_avg:98.28ms
step:1110/1770 train_time:109096ms step_avg:98.28ms
step:1111/1770 train_time:109198ms step_avg:98.29ms
step:1112/1770 train_time:109300ms step_avg:98.29ms
step:1113/1770 train_time:109402ms step_avg:98.29ms
step:1114/1770 train_time:109504ms step_avg:98.30ms
step:1115/1770 train_time:109606ms step_avg:98.30ms
step:1116/1770 train_time:109709ms step_avg:98.31ms
step:1117/1770 train_time:109811ms step_avg:98.31ms
step:1118/1770 train_time:109913ms step_avg:98.31ms
step:1119/1770 train_time:110016ms step_avg:98.32ms
step:1120/1770 train_time:110117ms step_avg:98.32ms
step:1121/1770 train_time:110219ms step_avg:98.32ms
step:1122/1770 train_time:110321ms step_avg:98.33ms
step:1123/1770 train_time:110423ms step_avg:98.33ms
step:1124/1770 train_time:110525ms step_avg:98.33ms
step:1125/1770 train_time:110628ms step_avg:98.34ms
step:1125/1770 val_loss:3.4711 train_time:110730ms step_avg:98.43ms
step:1126/1770 train_time:110748ms step_avg:98.36ms
step:1127/1770 train_time:110839ms step_avg:98.35ms
step:1128/1770 train_time:110942ms step_avg:98.35ms
step:1129/1770 train_time:111043ms step_avg:98.36ms
step:1130/1770 train_time:111145ms step_avg:98.36ms
step:1131/1770 train_time:111247ms step_avg:98.36ms
step:1132/1770 train_time:111349ms step_avg:98.36ms
step:1133/1770 train_time:111450ms step_avg:98.37ms
step:1134/1770 train_time:111552ms step_avg:98.37ms
step:1135/1770 train_time:111654ms step_avg:98.37ms
step:1136/1770 train_time:111759ms step_avg:98.38ms
step:1137/1770 train_time:111862ms step_avg:98.38ms
step:1138/1770 train_time:111965ms step_avg:98.39ms
step:1139/1770 train_time:112068ms step_avg:98.39ms
step:1140/1770 train_time:112171ms step_avg:98.40ms
step:1141/1770 train_time:112272ms step_avg:98.40ms
step:1142/1770 train_time:112374ms step_avg:98.40ms
step:1143/1770 train_time:112475ms step_avg:98.40ms
step:1144/1770 train_time:112577ms step_avg:98.41ms
step:1145/1770 train_time:112678ms step_avg:98.41ms
step:1146/1770 train_time:112780ms step_avg:98.41ms
step:1147/1770 train_time:112881ms step_avg:98.41ms
step:1148/1770 train_time:112983ms step_avg:98.42ms
step:1149/1770 train_time:113085ms step_avg:98.42ms
step:1150/1770 train_time:113188ms step_avg:98.42ms
step:1151/1770 train_time:113290ms step_avg:98.43ms
step:1152/1770 train_time:113393ms step_avg:98.43ms
step:1153/1770 train_time:113496ms step_avg:98.44ms
step:1154/1770 train_time:113598ms step_avg:98.44ms
step:1155/1770 train_time:113699ms step_avg:98.44ms
step:1156/1770 train_time:113800ms step_avg:98.44ms
step:1157/1770 train_time:113903ms step_avg:98.45ms
step:1158/1770 train_time:114005ms step_avg:98.45ms
step:1159/1770 train_time:114108ms step_avg:98.45ms
step:1160/1770 train_time:114210ms step_avg:98.46ms
step:1161/1770 train_time:114313ms step_avg:98.46ms
step:1162/1770 train_time:114416ms step_avg:98.46ms
step:1163/1770 train_time:114518ms step_avg:98.47ms
step:1164/1770 train_time:114620ms step_avg:98.47ms
step:1165/1770 train_time:114722ms step_avg:98.47ms
step:1166/1770 train_time:114823ms step_avg:98.48ms
step:1167/1770 train_time:114925ms step_avg:98.48ms
step:1168/1770 train_time:115027ms step_avg:98.48ms
step:1169/1770 train_time:115130ms step_avg:98.49ms
step:1170/1770 train_time:115232ms step_avg:98.49ms
step:1171/1770 train_time:115335ms step_avg:98.49ms
step:1172/1770 train_time:115436ms step_avg:98.50ms
step:1173/1770 train_time:115539ms step_avg:98.50ms
step:1174/1770 train_time:115640ms step_avg:98.50ms
step:1175/1770 train_time:115742ms step_avg:98.50ms
step:1176/1770 train_time:115844ms step_avg:98.51ms
step:1177/1770 train_time:115947ms step_avg:98.51ms
step:1178/1770 train_time:116049ms step_avg:98.51ms
step:1179/1770 train_time:116153ms step_avg:98.52ms
step:1180/1770 train_time:116255ms step_avg:98.52ms
step:1181/1770 train_time:116357ms step_avg:98.52ms
step:1182/1770 train_time:116459ms step_avg:98.53ms
step:1183/1770 train_time:116562ms step_avg:98.53ms
step:1184/1770 train_time:116668ms step_avg:98.54ms
step:1185/1770 train_time:116771ms step_avg:98.54ms
step:1186/1770 train_time:116873ms step_avg:98.54ms
step:1187/1770 train_time:116977ms step_avg:98.55ms
step:1188/1770 train_time:117080ms step_avg:98.55ms
step:1189/1770 train_time:117183ms step_avg:98.56ms
step:1190/1770 train_time:117286ms step_avg:98.56ms
step:1191/1770 train_time:117391ms step_avg:98.57ms
step:1192/1770 train_time:117495ms step_avg:98.57ms
step:1193/1770 train_time:117598ms step_avg:98.57ms
step:1194/1770 train_time:117701ms step_avg:98.58ms
step:1195/1770 train_time:117803ms step_avg:98.58ms
step:1196/1770 train_time:117909ms step_avg:98.59ms
step:1197/1770 train_time:118012ms step_avg:98.59ms
step:1198/1770 train_time:118115ms step_avg:98.59ms
step:1199/1770 train_time:118218ms step_avg:98.60ms
step:1200/1770 train_time:118322ms step_avg:98.60ms
step:1201/1770 train_time:118425ms step_avg:98.61ms
step:1202/1770 train_time:118529ms step_avg:98.61ms
step:1203/1770 train_time:118632ms step_avg:98.61ms
step:1204/1770 train_time:118737ms step_avg:98.62ms
step:1205/1770 train_time:118839ms step_avg:98.62ms
step:1206/1770 train_time:118942ms step_avg:98.63ms
step:1207/1770 train_time:119045ms step_avg:98.63ms
step:1208/1770 train_time:119150ms step_avg:98.63ms
step:1209/1770 train_time:119254ms step_avg:98.64ms
step:1210/1770 train_time:119357ms step_avg:98.64ms
step:1211/1770 train_time:119459ms step_avg:98.64ms
step:1212/1770 train_time:119564ms step_avg:98.65ms
step:1213/1770 train_time:119668ms step_avg:98.65ms
step:1214/1770 train_time:119771ms step_avg:98.66ms
step:1215/1770 train_time:119874ms step_avg:98.66ms
step:1216/1770 train_time:119978ms step_avg:98.67ms
step:1217/1770 train_time:120082ms step_avg:98.67ms
step:1218/1770 train_time:120185ms step_avg:98.67ms
step:1219/1770 train_time:120288ms step_avg:98.68ms
step:1220/1770 train_time:120391ms step_avg:98.68ms
step:1221/1770 train_time:120495ms step_avg:98.69ms
step:1222/1770 train_time:120600ms step_avg:98.69ms
step:1223/1770 train_time:120702ms step_avg:98.69ms
step:1224/1770 train_time:120806ms step_avg:98.70ms
step:1225/1770 train_time:120910ms step_avg:98.70ms
step:1226/1770 train_time:121014ms step_avg:98.71ms
step:1227/1770 train_time:121119ms step_avg:98.71ms
step:1228/1770 train_time:121223ms step_avg:98.72ms
step:1229/1770 train_time:121326ms step_avg:98.72ms
step:1230/1770 train_time:121430ms step_avg:98.72ms
step:1231/1770 train_time:121534ms step_avg:98.73ms
step:1232/1770 train_time:121637ms step_avg:98.73ms
step:1233/1770 train_time:121739ms step_avg:98.73ms
step:1234/1770 train_time:121842ms step_avg:98.74ms
step:1235/1770 train_time:121945ms step_avg:98.74ms
step:1236/1770 train_time:122049ms step_avg:98.75ms
step:1237/1770 train_time:122154ms step_avg:98.75ms
step:1238/1770 train_time:122257ms step_avg:98.75ms
step:1239/1770 train_time:122360ms step_avg:98.76ms
step:1240/1770 train_time:122463ms step_avg:98.76ms
step:1241/1770 train_time:122567ms step_avg:98.76ms
step:1242/1770 train_time:122671ms step_avg:98.77ms
step:1243/1770 train_time:122774ms step_avg:98.77ms
step:1244/1770 train_time:122877ms step_avg:98.78ms
step:1245/1770 train_time:122980ms step_avg:98.78ms
step:1246/1770 train_time:123083ms step_avg:98.78ms
step:1247/1770 train_time:123187ms step_avg:98.79ms
step:1248/1770 train_time:123292ms step_avg:98.79ms
step:1249/1770 train_time:123396ms step_avg:98.80ms
step:1250/1770 train_time:123499ms step_avg:98.80ms
step:1250/1770 val_loss:3.4225 train_time:123602ms step_avg:98.88ms
step:1251/1770 train_time:123619ms step_avg:98.82ms
step:1252/1770 train_time:123711ms step_avg:98.81ms
step:1253/1770 train_time:123814ms step_avg:98.81ms
step:1254/1770 train_time:123917ms step_avg:98.82ms
step:1255/1770 train_time:124021ms step_avg:98.82ms
step:1256/1770 train_time:124124ms step_avg:98.83ms
step:1257/1770 train_time:124227ms step_avg:98.83ms
step:1258/1770 train_time:124330ms step_avg:98.83ms
step:1259/1770 train_time:124433ms step_avg:98.83ms
step:1260/1770 train_time:124535ms step_avg:98.84ms
step:1261/1770 train_time:124641ms step_avg:98.84ms
step:1262/1770 train_time:124746ms step_avg:98.85ms
step:1263/1770 train_time:124850ms step_avg:98.85ms
step:1264/1770 train_time:124955ms step_avg:98.86ms
step:1265/1770 train_time:125057ms step_avg:98.86ms
step:1266/1770 train_time:125161ms step_avg:98.86ms
step:1267/1770 train_time:125264ms step_avg:98.87ms
step:1268/1770 train_time:125369ms step_avg:98.87ms
step:1269/1770 train_time:125472ms step_avg:98.87ms
step:1270/1770 train_time:125575ms step_avg:98.88ms
step:1271/1770 train_time:125680ms step_avg:98.88ms
step:1272/1770 train_time:125784ms step_avg:98.89ms
step:1273/1770 train_time:125890ms step_avg:98.89ms
step:1274/1770 train_time:125993ms step_avg:98.90ms
step:1275/1770 train_time:126095ms step_avg:98.90ms
step:1276/1770 train_time:126198ms step_avg:98.90ms
step:1277/1770 train_time:126301ms step_avg:98.90ms
step:1278/1770 train_time:126405ms step_avg:98.91ms
step:1279/1770 train_time:126508ms step_avg:98.91ms
step:1280/1770 train_time:126613ms step_avg:98.92ms
step:1281/1770 train_time:126715ms step_avg:98.92ms
step:1282/1770 train_time:126820ms step_avg:98.92ms
step:1283/1770 train_time:126924ms step_avg:98.93ms
step:1284/1770 train_time:127028ms step_avg:98.93ms
step:1285/1770 train_time:127131ms step_avg:98.93ms
step:1286/1770 train_time:127235ms step_avg:98.94ms
step:1287/1770 train_time:127340ms step_avg:98.94ms
step:1288/1770 train_time:127444ms step_avg:98.95ms
step:1289/1770 train_time:127547ms step_avg:98.95ms
step:1290/1770 train_time:127651ms step_avg:98.95ms
step:1291/1770 train_time:127754ms step_avg:98.96ms
step:1292/1770 train_time:127856ms step_avg:98.96ms
step:1293/1770 train_time:127961ms step_avg:98.96ms
step:1294/1770 train_time:128066ms step_avg:98.97ms
step:1295/1770 train_time:128169ms step_avg:98.97ms
step:1296/1770 train_time:128272ms step_avg:98.98ms
step:1297/1770 train_time:128374ms step_avg:98.98ms
step:1298/1770 train_time:128478ms step_avg:98.98ms
step:1299/1770 train_time:128581ms step_avg:98.98ms
step:1300/1770 train_time:128686ms step_avg:98.99ms
step:1301/1770 train_time:128790ms step_avg:98.99ms
step:1302/1770 train_time:128893ms step_avg:99.00ms
step:1303/1770 train_time:128996ms step_avg:99.00ms
step:1304/1770 train_time:129100ms step_avg:99.00ms
step:1305/1770 train_time:129204ms step_avg:99.01ms
step:1306/1770 train_time:129308ms step_avg:99.01ms
step:1307/1770 train_time:129412ms step_avg:99.01ms
step:1308/1770 train_time:129514ms step_avg:99.02ms
step:1309/1770 train_time:129617ms step_avg:99.02ms
step:1310/1770 train_time:129721ms step_avg:99.02ms
step:1311/1770 train_time:129825ms step_avg:99.03ms
step:1312/1770 train_time:129929ms step_avg:99.03ms
step:1313/1770 train_time:130032ms step_avg:99.03ms
step:1314/1770 train_time:130135ms step_avg:99.04ms
step:1315/1770 train_time:130238ms step_avg:99.04ms
step:1316/1770 train_time:130342ms step_avg:99.04ms
step:1317/1770 train_time:130446ms step_avg:99.05ms
step:1318/1770 train_time:130551ms step_avg:99.05ms
step:1319/1770 train_time:130654ms step_avg:99.06ms
step:1320/1770 train_time:130757ms step_avg:99.06ms
step:1321/1770 train_time:130861ms step_avg:99.06ms
step:1322/1770 train_time:130965ms step_avg:99.07ms
step:1323/1770 train_time:131070ms step_avg:99.07ms
step:1324/1770 train_time:131173ms step_avg:99.07ms
step:1325/1770 train_time:131277ms step_avg:99.08ms
step:1326/1770 train_time:131380ms step_avg:99.08ms
step:1327/1770 train_time:131486ms step_avg:99.09ms
step:1328/1770 train_time:131590ms step_avg:99.09ms
step:1329/1770 train_time:131693ms step_avg:99.09ms
step:1330/1770 train_time:131795ms step_avg:99.09ms
step:1331/1770 train_time:131898ms step_avg:99.10ms
step:1332/1770 train_time:132002ms step_avg:99.10ms
step:1333/1770 train_time:132106ms step_avg:99.10ms
step:1334/1770 train_time:132209ms step_avg:99.11ms
step:1335/1770 train_time:132312ms step_avg:99.11ms
step:1336/1770 train_time:132414ms step_avg:99.11ms
step:1337/1770 train_time:132517ms step_avg:99.12ms
step:1338/1770 train_time:132621ms step_avg:99.12ms
step:1339/1770 train_time:132724ms step_avg:99.12ms
step:1340/1770 train_time:132829ms step_avg:99.13ms
step:1341/1770 train_time:132931ms step_avg:99.13ms
step:1342/1770 train_time:133035ms step_avg:99.13ms
step:1343/1770 train_time:133138ms step_avg:99.13ms
step:1344/1770 train_time:133241ms step_avg:99.14ms
step:1345/1770 train_time:133345ms step_avg:99.14ms
step:1346/1770 train_time:133449ms step_avg:99.14ms
step:1347/1770 train_time:133552ms step_avg:99.15ms
step:1348/1770 train_time:133657ms step_avg:99.15ms
step:1349/1770 train_time:133761ms step_avg:99.16ms
step:1350/1770 train_time:133865ms step_avg:99.16ms
step:1351/1770 train_time:133968ms step_avg:99.16ms
step:1352/1770 train_time:134071ms step_avg:99.16ms
step:1353/1770 train_time:134175ms step_avg:99.17ms
step:1354/1770 train_time:134277ms step_avg:99.17ms
step:1355/1770 train_time:134381ms step_avg:99.17ms
step:1356/1770 train_time:134486ms step_avg:99.18ms
step:1357/1770 train_time:134590ms step_avg:99.18ms
step:1358/1770 train_time:134694ms step_avg:99.19ms
step:1359/1770 train_time:134798ms step_avg:99.19ms
step:1360/1770 train_time:134903ms step_avg:99.19ms
step:1361/1770 train_time:135008ms step_avg:99.20ms
step:1362/1770 train_time:135110ms step_avg:99.20ms
step:1363/1770 train_time:135214ms step_avg:99.20ms
step:1364/1770 train_time:135318ms step_avg:99.21ms
step:1365/1770 train_time:135421ms step_avg:99.21ms
step:1366/1770 train_time:135525ms step_avg:99.21ms
step:1367/1770 train_time:135630ms step_avg:99.22ms
step:1368/1770 train_time:135732ms step_avg:99.22ms
step:1369/1770 train_time:135836ms step_avg:99.22ms
step:1370/1770 train_time:135940ms step_avg:99.23ms
step:1371/1770 train_time:136044ms step_avg:99.23ms
step:1372/1770 train_time:136149ms step_avg:99.23ms
step:1373/1770 train_time:136254ms step_avg:99.24ms
step:1374/1770 train_time:136357ms step_avg:99.24ms
step:1375/1770 train_time:136461ms step_avg:99.24ms
step:1375/1770 val_loss:3.3793 train_time:136566ms step_avg:99.32ms
step:1376/1770 train_time:136583ms step_avg:99.26ms
step:1377/1770 train_time:136681ms step_avg:99.26ms
step:1378/1770 train_time:136785ms step_avg:99.26ms
step:1379/1770 train_time:136887ms step_avg:99.27ms
step:1380/1770 train_time:136990ms step_avg:99.27ms
step:1381/1770 train_time:137093ms step_avg:99.27ms
step:1382/1770 train_time:137196ms step_avg:99.27ms
step:1383/1770 train_time:137300ms step_avg:99.28ms
step:1384/1770 train_time:137402ms step_avg:99.28ms
step:1385/1770 train_time:137506ms step_avg:99.28ms
step:1386/1770 train_time:137610ms step_avg:99.29ms
step:1387/1770 train_time:137716ms step_avg:99.29ms
step:1388/1770 train_time:137820ms step_avg:99.29ms
step:1389/1770 train_time:137926ms step_avg:99.30ms
step:1390/1770 train_time:138029ms step_avg:99.30ms
step:1391/1770 train_time:138131ms step_avg:99.30ms
step:1392/1770 train_time:138233ms step_avg:99.31ms
step:1393/1770 train_time:138337ms step_avg:99.31ms
step:1394/1770 train_time:138439ms step_avg:99.31ms
step:1395/1770 train_time:138544ms step_avg:99.31ms
step:1396/1770 train_time:138649ms step_avg:99.32ms
step:1397/1770 train_time:138752ms step_avg:99.32ms
step:1398/1770 train_time:138855ms step_avg:99.32ms
step:1399/1770 train_time:138960ms step_avg:99.33ms
step:1400/1770 train_time:139064ms step_avg:99.33ms
step:1401/1770 train_time:139168ms step_avg:99.33ms
step:1402/1770 train_time:139271ms step_avg:99.34ms
step:1403/1770 train_time:139373ms step_avg:99.34ms
step:1404/1770 train_time:139477ms step_avg:99.34ms
step:1405/1770 train_time:139580ms step_avg:99.35ms
step:1406/1770 train_time:139684ms step_avg:99.35ms
step:1407/1770 train_time:139787ms step_avg:99.35ms
step:1408/1770 train_time:139890ms step_avg:99.35ms
step:1409/1770 train_time:139992ms step_avg:99.36ms
step:1410/1770 train_time:140097ms step_avg:99.36ms
step:1411/1770 train_time:140200ms step_avg:99.36ms
step:1412/1770 train_time:140304ms step_avg:99.37ms
step:1413/1770 train_time:140407ms step_avg:99.37ms
step:1414/1770 train_time:140511ms step_avg:99.37ms
step:1415/1770 train_time:140614ms step_avg:99.37ms
step:1416/1770 train_time:140720ms step_avg:99.38ms
step:1417/1770 train_time:140823ms step_avg:99.38ms
step:1418/1770 train_time:140927ms step_avg:99.38ms
step:1419/1770 train_time:141030ms step_avg:99.39ms
step:1420/1770 train_time:141132ms step_avg:99.39ms
step:1421/1770 train_time:141235ms step_avg:99.39ms
step:1422/1770 train_time:141339ms step_avg:99.39ms
step:1423/1770 train_time:141444ms step_avg:99.40ms
step:1424/1770 train_time:141547ms step_avg:99.40ms
step:1425/1770 train_time:141650ms step_avg:99.40ms
step:1426/1770 train_time:141753ms step_avg:99.41ms
step:1427/1770 train_time:141857ms step_avg:99.41ms
step:1428/1770 train_time:141961ms step_avg:99.41ms
step:1429/1770 train_time:142065ms step_avg:99.42ms
step:1430/1770 train_time:142169ms step_avg:99.42ms
step:1431/1770 train_time:142273ms step_avg:99.42ms
step:1432/1770 train_time:142376ms step_avg:99.42ms
step:1433/1770 train_time:142480ms step_avg:99.43ms
step:1434/1770 train_time:142584ms step_avg:99.43ms
step:1435/1770 train_time:142688ms step_avg:99.43ms
step:1436/1770 train_time:142792ms step_avg:99.44ms
step:1437/1770 train_time:142895ms step_avg:99.44ms
step:1438/1770 train_time:142999ms step_avg:99.44ms
step:1439/1770 train_time:143103ms step_avg:99.45ms
step:1440/1770 train_time:143207ms step_avg:99.45ms
step:1441/1770 train_time:143311ms step_avg:99.45ms
step:1442/1770 train_time:143414ms step_avg:99.45ms
step:1443/1770 train_time:143517ms step_avg:99.46ms
step:1444/1770 train_time:143622ms step_avg:99.46ms
step:1445/1770 train_time:143726ms step_avg:99.46ms
step:1446/1770 train_time:143830ms step_avg:99.47ms
step:1447/1770 train_time:143935ms step_avg:99.47ms
step:1448/1770 train_time:144040ms step_avg:99.47ms
step:1449/1770 train_time:144146ms step_avg:99.48ms
step:1450/1770 train_time:144250ms step_avg:99.48ms
step:1451/1770 train_time:144355ms step_avg:99.49ms
step:1452/1770 train_time:144460ms step_avg:99.49ms
step:1453/1770 train_time:144565ms step_avg:99.49ms
step:1454/1770 train_time:144670ms step_avg:99.50ms
step:1455/1770 train_time:144777ms step_avg:99.50ms
step:1456/1770 train_time:144884ms step_avg:99.51ms
step:1457/1770 train_time:144989ms step_avg:99.51ms
step:1458/1770 train_time:145093ms step_avg:99.52ms
step:1459/1770 train_time:145199ms step_avg:99.52ms
step:1460/1770 train_time:145304ms step_avg:99.52ms
step:1461/1770 train_time:145409ms step_avg:99.53ms
step:1462/1770 train_time:145513ms step_avg:99.53ms
step:1463/1770 train_time:145618ms step_avg:99.53ms
step:1464/1770 train_time:145725ms step_avg:99.54ms
step:1465/1770 train_time:145829ms step_avg:99.54ms
step:1466/1770 train_time:145934ms step_avg:99.55ms
step:1467/1770 train_time:146041ms step_avg:99.55ms
step:1468/1770 train_time:146145ms step_avg:99.55ms
step:1469/1770 train_time:146249ms step_avg:99.56ms
step:1470/1770 train_time:146352ms step_avg:99.56ms
step:1471/1770 train_time:146457ms step_avg:99.56ms
step:1472/1770 train_time:146563ms step_avg:99.57ms
step:1473/1770 train_time:146668ms step_avg:99.57ms
step:1474/1770 train_time:146772ms step_avg:99.57ms
step:1475/1770 train_time:146877ms step_avg:99.58ms
step:1476/1770 train_time:146983ms step_avg:99.58ms
step:1477/1770 train_time:147090ms step_avg:99.59ms
step:1478/1770 train_time:147195ms step_avg:99.59ms
step:1479/1770 train_time:147300ms step_avg:99.59ms
step:1480/1770 train_time:147404ms step_avg:99.60ms
step:1481/1770 train_time:147513ms step_avg:99.60ms
step:1482/1770 train_time:147616ms step_avg:99.61ms
step:1483/1770 train_time:147721ms step_avg:99.61ms
step:1484/1770 train_time:147826ms step_avg:99.61ms
step:1485/1770 train_time:147930ms step_avg:99.62ms
step:1486/1770 train_time:148034ms step_avg:99.62ms
step:1487/1770 train_time:148138ms step_avg:99.62ms
step:1488/1770 train_time:148242ms step_avg:99.63ms
step:1489/1770 train_time:148349ms step_avg:99.63ms
step:1490/1770 train_time:148453ms step_avg:99.63ms
step:1491/1770 train_time:148558ms step_avg:99.64ms
step:1492/1770 train_time:148664ms step_avg:99.64ms
step:1493/1770 train_time:148771ms step_avg:99.65ms
step:1494/1770 train_time:148879ms step_avg:99.65ms
step:1495/1770 train_time:148983ms step_avg:99.65ms
step:1496/1770 train_time:149087ms step_avg:99.66ms
step:1497/1770 train_time:149191ms step_avg:99.66ms
step:1498/1770 train_time:149296ms step_avg:99.66ms
step:1499/1770 train_time:149400ms step_avg:99.67ms
step:1500/1770 train_time:149505ms step_avg:99.67ms
step:1500/1770 val_loss:3.3424 train_time:149608ms step_avg:99.74ms
step:1501/1770 train_time:149626ms step_avg:99.68ms
step:1502/1770 train_time:149720ms step_avg:99.68ms
step:1503/1770 train_time:149823ms step_avg:99.68ms
step:1504/1770 train_time:149928ms step_avg:99.69ms
step:1505/1770 train_time:150033ms step_avg:99.69ms
step:1506/1770 train_time:150138ms step_avg:99.69ms
step:1507/1770 train_time:150242ms step_avg:99.70ms
step:1508/1770 train_time:150346ms step_avg:99.70ms
step:1509/1770 train_time:150451ms step_avg:99.70ms
step:1510/1770 train_time:150555ms step_avg:99.71ms
step:1511/1770 train_time:150661ms step_avg:99.71ms
step:1512/1770 train_time:150767ms step_avg:99.71ms
step:1513/1770 train_time:150872ms step_avg:99.72ms
step:1514/1770 train_time:150976ms step_avg:99.72ms
step:1515/1770 train_time:151081ms step_avg:99.72ms
step:1516/1770 train_time:151187ms step_avg:99.73ms
step:1517/1770 train_time:151291ms step_avg:99.73ms
step:1518/1770 train_time:151396ms step_avg:99.73ms
step:1519/1770 train_time:151499ms step_avg:99.74ms
step:1520/1770 train_time:151605ms step_avg:99.74ms
step:1521/1770 train_time:151711ms step_avg:99.74ms
step:1522/1770 train_time:151816ms step_avg:99.75ms
step:1523/1770 train_time:151922ms step_avg:99.75ms
step:1524/1770 train_time:152027ms step_avg:99.76ms
step:1525/1770 train_time:152131ms step_avg:99.76ms
step:1526/1770 train_time:152235ms step_avg:99.76ms
step:1527/1770 train_time:152339ms step_avg:99.76ms
step:1528/1770 train_time:152446ms step_avg:99.77ms
step:1529/1770 train_time:152551ms step_avg:99.77ms
step:1530/1770 train_time:152655ms step_avg:99.77ms
step:1531/1770 train_time:152759ms step_avg:99.78ms
step:1532/1770 train_time:152864ms step_avg:99.78ms
step:1533/1770 train_time:152970ms step_avg:99.78ms
step:1534/1770 train_time:153075ms step_avg:99.79ms
step:1535/1770 train_time:153178ms step_avg:99.79ms
step:1536/1770 train_time:153282ms step_avg:99.79ms
step:1537/1770 train_time:153387ms step_avg:99.80ms
step:1538/1770 train_time:153493ms step_avg:99.80ms
step:1539/1770 train_time:153597ms step_avg:99.80ms
step:1540/1770 train_time:153704ms step_avg:99.81ms
step:1541/1770 train_time:153811ms step_avg:99.81ms
step:1542/1770 train_time:153916ms step_avg:99.82ms
step:1543/1770 train_time:154020ms step_avg:99.82ms
step:1544/1770 train_time:154128ms step_avg:99.82ms
step:1545/1770 train_time:154232ms step_avg:99.83ms
step:1546/1770 train_time:154337ms step_avg:99.83ms
step:1547/1770 train_time:154441ms step_avg:99.83ms
step:1548/1770 train_time:154546ms step_avg:99.84ms
step:1549/1770 train_time:154651ms step_avg:99.84ms
step:1550/1770 train_time:154756ms step_avg:99.84ms
step:1551/1770 train_time:154860ms step_avg:99.85ms
step:1552/1770 train_time:154966ms step_avg:99.85ms
step:1553/1770 train_time:155070ms step_avg:99.85ms
step:1554/1770 train_time:155174ms step_avg:99.85ms
step:1555/1770 train_time:155279ms step_avg:99.86ms
step:1556/1770 train_time:155384ms step_avg:99.86ms
step:1557/1770 train_time:155489ms step_avg:99.86ms
step:1558/1770 train_time:155595ms step_avg:99.87ms
step:1559/1770 train_time:155700ms step_avg:99.87ms
step:1560/1770 train_time:155805ms step_avg:99.87ms
step:1561/1770 train_time:155912ms step_avg:99.88ms
step:1562/1770 train_time:156017ms step_avg:99.88ms
step:1563/1770 train_time:156122ms step_avg:99.89ms
step:1564/1770 train_time:156226ms step_avg:99.89ms
step:1565/1770 train_time:156331ms step_avg:99.89ms
step:1566/1770 train_time:156435ms step_avg:99.89ms
step:1567/1770 train_time:156540ms step_avg:99.90ms
step:1568/1770 train_time:156644ms step_avg:99.90ms
step:1569/1770 train_time:156752ms step_avg:99.91ms
step:1570/1770 train_time:156857ms step_avg:99.91ms
step:1571/1770 train_time:156961ms step_avg:99.91ms
step:1572/1770 train_time:157067ms step_avg:99.92ms
step:1573/1770 train_time:157173ms step_avg:99.92ms
step:1574/1770 train_time:157277ms step_avg:99.92ms
step:1575/1770 train_time:157381ms step_avg:99.92ms
step:1576/1770 train_time:157485ms step_avg:99.93ms
step:1577/1770 train_time:157591ms step_avg:99.93ms
step:1578/1770 train_time:157697ms step_avg:99.93ms
step:1579/1770 train_time:157802ms step_avg:99.94ms
step:1580/1770 train_time:157907ms step_avg:99.94ms
step:1581/1770 train_time:158015ms step_avg:99.95ms
step:1582/1770 train_time:158120ms step_avg:99.95ms
step:1583/1770 train_time:158225ms step_avg:99.95ms
step:1584/1770 train_time:158330ms step_avg:99.96ms
step:1585/1770 train_time:158435ms step_avg:99.96ms
step:1586/1770 train_time:158544ms step_avg:99.96ms
step:1587/1770 train_time:158648ms step_avg:99.97ms
step:1588/1770 train_time:158752ms step_avg:99.97ms
step:1589/1770 train_time:158858ms step_avg:99.97ms
step:1590/1770 train_time:158963ms step_avg:99.98ms
step:1591/1770 train_time:159068ms step_avg:99.98ms
step:1592/1770 train_time:159173ms step_avg:99.98ms
step:1593/1770 train_time:159278ms step_avg:99.99ms
step:1594/1770 train_time:159383ms step_avg:99.99ms
step:1595/1770 train_time:159488ms step_avg:99.99ms
step:1596/1770 train_time:159593ms step_avg:100.00ms
step:1597/1770 train_time:159697ms step_avg:100.00ms
step:1598/1770 train_time:159801ms step_avg:100.00ms
step:1599/1770 train_time:159907ms step_avg:100.00ms
step:1600/1770 train_time:160013ms step_avg:100.01ms
step:1601/1770 train_time:160118ms step_avg:100.01ms
step:1602/1770 train_time:160224ms step_avg:100.02ms
step:1603/1770 train_time:160330ms step_avg:100.02ms
step:1604/1770 train_time:160433ms step_avg:100.02ms
step:1605/1770 train_time:160538ms step_avg:100.02ms
step:1606/1770 train_time:160643ms step_avg:100.03ms
step:1607/1770 train_time:160751ms step_avg:100.03ms
step:1608/1770 train_time:160855ms step_avg:100.03ms
step:1609/1770 train_time:160960ms step_avg:100.04ms
step:1610/1770 train_time:161067ms step_avg:100.04ms
step:1611/1770 train_time:161173ms step_avg:100.05ms
step:1612/1770 train_time:161277ms step_avg:100.05ms
step:1613/1770 train_time:161382ms step_avg:100.05ms
step:1614/1770 train_time:161486ms step_avg:100.05ms
step:1615/1770 train_time:161592ms step_avg:100.06ms
step:1616/1770 train_time:161696ms step_avg:100.06ms
step:1617/1770 train_time:161802ms step_avg:100.06ms
step:1618/1770 train_time:161909ms step_avg:100.07ms
step:1619/1770 train_time:162014ms step_avg:100.07ms
step:1620/1770 train_time:162120ms step_avg:100.07ms
step:1621/1770 train_time:162227ms step_avg:100.08ms
step:1622/1770 train_time:162332ms step_avg:100.08ms
step:1623/1770 train_time:162439ms step_avg:100.09ms
step:1624/1770 train_time:162544ms step_avg:100.09ms
step:1625/1770 train_time:162648ms step_avg:100.09ms
step:1625/1770 val_loss:3.3082 train_time:162751ms step_avg:100.15ms
step:1626/1770 train_time:162769ms step_avg:100.10ms
step:1627/1770 train_time:162863ms step_avg:100.10ms
step:1628/1770 train_time:162967ms step_avg:100.10ms
step:1629/1770 train_time:163071ms step_avg:100.10ms
step:1630/1770 train_time:163175ms step_avg:100.11ms
step:1631/1770 train_time:163279ms step_avg:100.11ms
step:1632/1770 train_time:163385ms step_avg:100.11ms
step:1633/1770 train_time:163488ms step_avg:100.12ms
step:1634/1770 train_time:163593ms step_avg:100.12ms
step:1635/1770 train_time:163699ms step_avg:100.12ms
step:1636/1770 train_time:163807ms step_avg:100.13ms
step:1637/1770 train_time:163912ms step_avg:100.13ms
step:1638/1770 train_time:164017ms step_avg:100.13ms
step:1639/1770 train_time:164122ms step_avg:100.14ms
step:1640/1770 train_time:164228ms step_avg:100.14ms
step:1641/1770 train_time:164332ms step_avg:100.14ms
step:1642/1770 train_time:164435ms step_avg:100.14ms
step:1643/1770 train_time:164539ms step_avg:100.15ms
step:1644/1770 train_time:164646ms step_avg:100.15ms
step:1645/1770 train_time:164750ms step_avg:100.15ms
step:1646/1770 train_time:164857ms step_avg:100.16ms
step:1647/1770 train_time:164963ms step_avg:100.16ms
step:1648/1770 train_time:165067ms step_avg:100.16ms
step:1649/1770 train_time:165171ms step_avg:100.16ms
step:1650/1770 train_time:165276ms step_avg:100.17ms
step:1651/1770 train_time:165380ms step_avg:100.17ms
step:1652/1770 train_time:165485ms step_avg:100.17ms
step:1653/1770 train_time:165590ms step_avg:100.18ms
step:1654/1770 train_time:165698ms step_avg:100.18ms
step:1655/1770 train_time:165805ms step_avg:100.18ms
step:1656/1770 train_time:165910ms step_avg:100.19ms
step:1657/1770 train_time:166015ms step_avg:100.19ms
step:1658/1770 train_time:166120ms step_avg:100.19ms
step:1659/1770 train_time:166226ms step_avg:100.20ms
step:1660/1770 train_time:166331ms step_avg:100.20ms
step:1661/1770 train_time:166436ms step_avg:100.20ms
step:1662/1770 train_time:166541ms step_avg:100.21ms
step:1663/1770 train_time:166648ms step_avg:100.21ms
step:1664/1770 train_time:166752ms step_avg:100.21ms
step:1665/1770 train_time:166855ms step_avg:100.21ms
step:1666/1770 train_time:166960ms step_avg:100.22ms
step:1667/1770 train_time:167065ms step_avg:100.22ms
step:1668/1770 train_time:167170ms step_avg:100.22ms
step:1669/1770 train_time:167274ms step_avg:100.22ms
step:1670/1770 train_time:167377ms step_avg:100.23ms
step:1671/1770 train_time:167483ms step_avg:100.23ms
step:1672/1770 train_time:167590ms step_avg:100.23ms
step:1673/1770 train_time:167696ms step_avg:100.24ms
step:1674/1770 train_time:167800ms step_avg:100.24ms
step:1675/1770 train_time:167906ms step_avg:100.24ms
step:1676/1770 train_time:168012ms step_avg:100.25ms
step:1677/1770 train_time:168120ms step_avg:100.25ms
step:1678/1770 train_time:168223ms step_avg:100.25ms
step:1679/1770 train_time:168328ms step_avg:100.26ms
step:1680/1770 train_time:168433ms step_avg:100.26ms
step:1681/1770 train_time:168538ms step_avg:100.26ms
step:1682/1770 train_time:168646ms step_avg:100.26ms
step:1683/1770 train_time:168750ms step_avg:100.27ms
step:1684/1770 train_time:168854ms step_avg:100.27ms
step:1685/1770 train_time:168958ms step_avg:100.27ms
step:1686/1770 train_time:169064ms step_avg:100.28ms
step:1687/1770 train_time:169172ms step_avg:100.28ms
step:1688/1770 train_time:169277ms step_avg:100.28ms
step:1689/1770 train_time:169382ms step_avg:100.29ms
step:1690/1770 train_time:169488ms step_avg:100.29ms
step:1691/1770 train_time:169593ms step_avg:100.29ms
step:1692/1770 train_time:169697ms step_avg:100.29ms
step:1693/1770 train_time:169804ms step_avg:100.30ms
step:1694/1770 train_time:169908ms step_avg:100.30ms
step:1695/1770 train_time:170013ms step_avg:100.30ms
step:1696/1770 train_time:170120ms step_avg:100.31ms
step:1697/1770 train_time:170226ms step_avg:100.31ms
step:1698/1770 train_time:170331ms step_avg:100.31ms
step:1699/1770 train_time:170435ms step_avg:100.31ms
step:1700/1770 train_time:170539ms step_avg:100.32ms
step:1701/1770 train_time:170644ms step_avg:100.32ms
step:1702/1770 train_time:170748ms step_avg:100.32ms
step:1703/1770 train_time:170853ms step_avg:100.32ms
step:1704/1770 train_time:170957ms step_avg:100.33ms
step:1705/1770 train_time:171062ms step_avg:100.33ms
step:1706/1770 train_time:171167ms step_avg:100.33ms
step:1707/1770 train_time:171273ms step_avg:100.34ms
step:1708/1770 train_time:171378ms step_avg:100.34ms
step:1709/1770 train_time:171484ms step_avg:100.34ms
step:1710/1770 train_time:171593ms step_avg:100.35ms
step:1711/1770 train_time:171700ms step_avg:100.35ms
step:1712/1770 train_time:171806ms step_avg:100.35ms
step:1713/1770 train_time:171910ms step_avg:100.36ms
step:1714/1770 train_time:172015ms step_avg:100.36ms
step:1715/1770 train_time:172120ms step_avg:100.36ms
step:1716/1770 train_time:172226ms step_avg:100.36ms
step:1717/1770 train_time:172332ms step_avg:100.37ms
step:1718/1770 train_time:172438ms step_avg:100.37ms
step:1719/1770 train_time:172545ms step_avg:100.38ms
step:1720/1770 train_time:172651ms step_avg:100.38ms
step:1721/1770 train_time:172756ms step_avg:100.38ms
step:1722/1770 train_time:172864ms step_avg:100.39ms
step:1723/1770 train_time:172971ms step_avg:100.39ms
step:1724/1770 train_time:173078ms step_avg:100.39ms
step:1725/1770 train_time:173185ms step_avg:100.40ms
step:1726/1770 train_time:173291ms step_avg:100.40ms
step:1727/1770 train_time:173396ms step_avg:100.40ms
step:1728/1770 train_time:173503ms step_avg:100.41ms
step:1729/1770 train_time:173608ms step_avg:100.41ms
step:1730/1770 train_time:173715ms step_avg:100.41ms
step:1731/1770 train_time:173822ms step_avg:100.42ms
step:1732/1770 train_time:173928ms step_avg:100.42ms
step:1733/1770 train_time:174034ms step_avg:100.42ms
step:1734/1770 train_time:174138ms step_avg:100.43ms
step:1735/1770 train_time:174245ms step_avg:100.43ms
step:1736/1770 train_time:174351ms step_avg:100.43ms
step:1737/1770 train_time:174455ms step_avg:100.43ms
step:1738/1770 train_time:174561ms step_avg:100.44ms
step:1739/1770 train_time:174667ms step_avg:100.44ms
step:1740/1770 train_time:174772ms step_avg:100.44ms
step:1741/1770 train_time:174881ms step_avg:100.45ms
step:1742/1770 train_time:174988ms step_avg:100.45ms
step:1743/1770 train_time:175095ms step_avg:100.46ms
step:1744/1770 train_time:175201ms step_avg:100.46ms
step:1745/1770 train_time:175307ms step_avg:100.46ms
step:1746/1770 train_time:175414ms step_avg:100.47ms
step:1747/1770 train_time:175518ms step_avg:100.47ms
step:1748/1770 train_time:175625ms step_avg:100.47ms
step:1749/1770 train_time:175733ms step_avg:100.48ms
step:1750/1770 train_time:175838ms step_avg:100.48ms
step:1750/1770 val_loss:3.2811 train_time:175943ms step_avg:100.54ms
step:1751/1770 train_time:175961ms step_avg:100.49ms
step:1752/1770 train_time:176058ms step_avg:100.49ms
step:1753/1770 train_time:176164ms step_avg:100.49ms
step:1754/1770 train_time:176270ms step_avg:100.50ms
step:1755/1770 train_time:176375ms step_avg:100.50ms
step:1756/1770 train_time:176481ms step_avg:100.50ms
step:1757/1770 train_time:176586ms step_avg:100.50ms
step:1758/1770 train_time:176692ms step_avg:100.51ms
step:1759/1770 train_time:176797ms step_avg:100.51ms
step:1760/1770 train_time:176903ms step_avg:100.51ms
step:1761/1770 train_time:177012ms step_avg:100.52ms
step:1762/1770 train_time:177121ms step_avg:100.52ms
step:1763/1770 train_time:177227ms step_avg:100.53ms
step:1764/1770 train_time:177332ms step_avg:100.53ms
step:1765/1770 train_time:177437ms step_avg:100.53ms
step:1766/1770 train_time:177547ms step_avg:100.54ms
step:1767/1770 train_time:177651ms step_avg:100.54ms
step:1768/1770 train_time:177756ms step_avg:100.54ms
step:1769/1770 train_time:177861ms step_avg:100.54ms
step:1770/1770 train_time:177966ms step_avg:100.55ms
step:1770/1770 val_loss:3.2783 train_time:178073ms step_avg:100.61ms
peak memory allocated: 30724 MiB reserved: 46472 MiB
