import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
import copy
import glob
from dataclasses import dataclass
from functools import lru_cache, partial # Added partial for hook registration
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
#torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params: list[Tensor] = [*params]
        param_groups = []
        for size in {p.numel() for p in params}:
            b = torch.empty(world_size, size, dtype=torch.bfloat16, device="cuda")
            group = dict(params=[p for p in params if p.numel() == size],
                         update_buffer=b, update_buffer_views=[b[i] for i in range(world_size)])
            param_groups.append(group)
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            update_buffer: Tensor = group["update_buffer"]
            update_buffer_views: list[Tensor] = group["update_buffer_views"]
            # generate weight updates in distributed fashion
            params: list[Tensor] = group["params"]
            handle = None
            params_world = None
            def update_prev(): # optimized Muon implementation contributed by @YouJiacheng
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffer_views):
                    p_world.add_(g_world.view_as(p_world),
                                 alpha=-group["lr"] * max(1, p_world.size(-2) / p_world.size(-1))**0.5)
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if "momentum_buffer" not in state:
                        state["momentum_buffer"] = torch.zeros_like(g)
                    buf: Tensor = state["momentum_buffer"]
                    buf.lerp_(g, 1 - group["momentum"])
                    g = g.lerp_(buf, group["momentum"]) if group["nesterov"] else buf
                    g = zeropower_via_newtonschulz5(g, steps=group["ns_steps"]).flatten()
                else:
                    g = update_buffer_views[self.rank]
                if base_i > 0:
                    update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=0.12).transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, block_mask: BlockMask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, max_seq_len, i) for i in range(num_layers)])
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128),
                                    use_fp8=True, x_s=(model_dim**0.5)/448, w_s=24/448, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        self.skip_weights = nn.Parameter(torch.ones(num_layers//2))

    def create_blockmasks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        docs = (input_seq == 50256).cumsum(0)

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_blockmask: Tensor):
            num_blocks = dense_blockmask.sum(dim=-1, dtype=torch.int32)
            indices = dense_blockmask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
        causal_blockmask_any = block_idx[:, None] >= block_idx
        causal_blockmask_all = block_idx[:, None] > block_idx
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
        document_blockmask_any = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
        document_blockmask_all = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
        blockmask_any = causal_blockmask_any & document_blockmask_any
        blockmask_all = causal_blockmask_all & document_blockmask_all
        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(blockmask_any & ~blockmask_all)
        full_kv_num_blocks, full_kv_indices = dense_to_ordered(blockmask_all)
        def build_bm(window_size_blocks: Tensor) -> BlockMask:
            return BlockMask.from_kv_blocks(
                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(window_size_blocks - full_kv_num_blocks, 1)),
                partial_kv_indices,
                torch.clamp_max(full_kv_num_blocks, window_size_blocks - 1),
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        long_bm, short_bm = self.create_blockmasks(input_seq, sliding_window_num_blocks)
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, long_bm, short_bm, short_bm, short_bm, long_bm]
        assert len(block_masks) == len(self.blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977

        # U-net design by @brendanh0gan
        skip_connections = []
        n = len(self.skip_weights)
        for i in range(len(self.blocks)):
            if i >= n:
                x = x + self.skip_weights[i - n] * skip_connections.pop()
            x = self.blocks[i](x, ve[i], x0, block_masks[i])
            if i < n:
                skip_connections.append(x)

        x = norm(x)
        logits = self.lm_head(x).float()
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / (7.5 * x.size(-1)**0.5))
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq, reduction='sum' if self.training else 'mean')
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn't helpful.
        pos += batch_size
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_seq_len = 48*1024 # FlexAttention sequence length
    val_seq_len = 4*64*1024 # FlexAttention sequence length for validation
    # optimization
    num_iterations = 1770 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    # architecture
    vocab_size = 50257
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert world_size == 8 # this code is designed for 8xH100
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs("logs_bucket", exist_ok=True)
    logfile = f"logs_bucket/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

########################################
#    Construct model and optimizer     #
########################################

model: nn.Module = GPT(vocab_size=args.vocab_size, num_layers=12, num_heads=6, model_dim=768,
                       max_seq_len=max(args.train_seq_len, args.val_seq_len)).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
adam_params = [dict(params=head_params, lr=0.22), dict(params=embed_params, lr=0.6), dict(params=scalar_params, lr=0.04)]
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = torch.optim.Adam(adam_params, betas=(0.8, 0.95), eps=1e-10, fused=True)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then decay
def get_lr(step: int):
    x = step / args.num_iterations # progress in training
    assert 0 <= x < 1
    if x < 1 - args.cooldown_frac:
        return 1.0
    else:
        w = (1 - x) / args.cooldown_frac
        return w * 1.0 + (1 - w) * 0.1

# attention window size schedule: linearly increase
@lru_cache(1)
def get_window_size_blocks_helper(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)
def get_window_size_blocks(step: int):
    x = step / args.num_iterations # progress in training
    assert 0 <= x <= 1
    # Linearly increase the block-wise sliding window size over training 128 -> 1792
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * x, n=128)
    return get_window_size_blocks_helper(window_size)

model: nn.Module = torch.compile(model, dynamic=False)

########################################
#     Warmup kernel and Layerwise Bucketing   #
########################################

# Track the gradient readiness order during warmup
gradient_ready_order = []

def record_grad_order_hook(param):
    def hook(*_):
        if param.grad is not None and all(param is not p for p in gradient_ready_order):
            gradient_ready_order.append(param)
    return hook

# Register temp hooks to track when each parameter's gradient is ready
tmp_handles = []
for p in model.parameters():
    if p.requires_grad:
        handle = p.register_post_accumulate_grad_hook(record_grad_order_hook(p))
        tmp_handles.append(handle)

warmup_steps = 10
initial_state = dict(
    model=copy.deepcopy(model.state_dict()),
    optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]
)

for step in range(warmup_steps):
    # Create a local generator with fixed, step-dependent seed
    g = torch.Generator(device="cuda")
    g.manual_seed(1337 + step)

    # Use this generator to sample inputs
    inputs = targets = torch.randint(
        low=0,
        high=args.vocab_size,
        size=(args.train_seq_len,),
        device="cuda",
        dtype=torch.long,
        generator=g,  # <- Local generator ensures deterministic and isolated RNG
    )
    model(inputs.to(torch.int32), targets, get_window_size_blocks(0)).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)

# Remove hooks after warmup
for h in tmp_handles:
    h.remove()

# Restore model/optimizer state to ensure a clean start
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del initial_state

print0(f"Collected {len(gradient_ready_order)} parameters in gradient-ready order")

########################################
#      Overlap Communication Setup     #
########################################

# Layer-aware bucketing using gradient-ready order
def create_buckets_layerwise(ordered_params, bucket_size_mb=25):
    """Group parameters into buckets of ~bucket_size_mb MB, in gradient ready order"""
    buckets = []
    current_bucket = []
    current_size = 0

    for param in ordered_params:
        if not param.requires_grad:
            continue
        param_size_mb = param.numel() * param.element_size() / (1024 * 1024)

        if current_size + param_size_mb > bucket_size_mb and current_bucket:
            buckets.append(current_bucket)
            current_bucket = [param]
            current_size = param_size_mb
        else:
            current_bucket.append(param)
            current_size += param_size_mb

    if current_bucket:
        buckets.append(current_bucket)

    return buckets

# Create buckets for all parameters
all_params = [p for p in model.parameters() if p.requires_grad]
param_buckets = create_buckets_layerwise(gradient_ready_order)
print0(f"Created {len(param_buckets)} gradient buckets")
for i, bucket in enumerate(param_buckets):
    total_size = sum(p.numel() * p.element_size() for p in bucket) / (1024 * 1024)
    print0(f"Bucket {i}: {len(bucket)} params, {total_size:.1f} MB")

# Bucket state tracking
bucket_ready_count = [0] * len(param_buckets)
bucket_handles = [None] * len(param_buckets)
param_to_bucket = {}

# Map each parameter to its bucket index
for bucket_idx, bucket in enumerate(param_buckets):
    for param in bucket:
        param_to_bucket[param] = bucket_idx

def _gradient_hook(param: Tensor):
    """Called when a parameter's gradient is ready"""
    if param.grad is None:
        return

    bucket_idx = param_to_bucket[param]
    bucket_ready_count[bucket_idx] += 1

    # Check if all parameters in this bucket are ready
    if bucket_ready_count[bucket_idx] == len(param_buckets[bucket_idx]):
        # All-reduce this bucket
        bucket_grads = [p.grad for p in param_buckets[bucket_idx]]

        # For multi-tensor operations, we can reduce them together
        if len(bucket_grads) == 1:
            handle = dist.all_reduce(bucket_grads[0], op=dist.ReduceOp.AVG, async_op=True)
        else:
            # Use multi-tensor all-reduce for efficiency
            handle = dist.all_reduce_coalesced(bucket_grads, op=dist.ReduceOp.AVG, async_op=True)

        bucket_handles[bucket_idx] = handle

# Register hooks for all parameters
print0("Registering bucketed gradient hooks...")
for param in all_params:
    param.register_post_accumulate_grad_hook(_gradient_hook)

def wait_for_gradients():
    """Wait for all gradient reductions to complete and reset bucket state"""
    for handle in bucket_handles:
        if handle is not None:
            handle.wait()

    # Reset state for next iteration
    for i in range(len(bucket_ready_count)):
        bucket_ready_count[i] = 0
        bucket_handles[i] = None

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, world_size * args.train_seq_len, rank, world_size)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_batch_size = world_size * args.val_seq_len
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        val_loader = distributed_data_generator(args.val_files, val_batch_size, rank, world_size)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets = next(val_loader)
                val_loss += model(inputs, targets, get_window_size_blocks(step))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs_bucket/{run_id}", exist_ok=True)
            torch.save(log, f"logs_bucket/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    inputs, targets = next(train_loader)
    model(inputs, targets, get_window_size_blocks(step)).backward()
    #for param in model.parameters():
    #    dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
    wait_for_gradients() # does the same thing as commented two lines above, but faster

    # set optimization hyperparameters
    for opt in optimizers:
        for group in opt.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)
    for group in optimizer2.param_groups:
        frac = min(step / 300, 1) # momentum warmup for muon
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers
    for opt in optimizers:
        opt.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0]
Running PyTorch 2.8.0.dev20250612+cu126 compiled for CUDA 12.6
Fri Jun 13 06:27:16 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   37C    P0            116W /  700W |    5824MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   29C    P0            111W /  700W |    1514MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   28C    P0            112W /  700W |    1514MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   36C    P0            120W /  700W |    1514MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   37C    P0            117W /  700W |    1514MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   29C    P0            111W /  700W |    1514MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   36C    P0            118W /  700W |    1514MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   29C    P0            113W /  700W |    1514MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
Collected 75 parameters in gradient-ready order
Created 21 gradient buckets
Bucket 0: 1 params, 73.6 MB
Bucket 1: 1 params, 73.6 MB
Bucket 2: 4 params, 15.8 MB
Bucket 3: 1 params, 147.4 MB
Bucket 4: 4 params, 20.3 MB
Bucket 5: 4 params, 24.8 MB
Bucket 6: 4 params, 18.0 MB
Bucket 7: 5 params, 18.0 MB
Bucket 8: 4 params, 20.3 MB
Bucket 9: 5 params, 24.8 MB
Bucket 10: 4 params, 20.3 MB
Bucket 11: 4 params, 24.8 MB
Bucket 12: 5 params, 18.0 MB
Bucket 13: 5 params, 20.3 MB
Bucket 14: 5 params, 18.0 MB
Bucket 15: 4 params, 20.3 MB
Bucket 16: 5 params, 24.8 MB
Bucket 17: 4 params, 24.8 MB
Bucket 18: 4 params, 2.3 MB
Bucket 19: 1 params, 73.6 MB
Bucket 20: 1 params, 73.6 MB
Registering bucketed gradient hooks...
step:0/1770 val_loss:10.8258 train_time:0ms step_avg:0.02ms
step:1/1770 train_time:71ms step_avg:71.17ms
step:2/1770 train_time:146ms step_avg:73.16ms
step:3/1770 train_time:235ms step_avg:78.26ms
step:4/1770 train_time:328ms step_avg:81.99ms
step:5/1770 train_time:422ms step_avg:84.41ms
step:6/1770 train_time:517ms step_avg:86.11ms
step:7/1770 train_time:611ms step_avg:87.26ms
step:8/1770 train_time:704ms step_avg:88.05ms
step:9/1770 train_time:799ms step_avg:88.75ms
step:10/1770 train_time:893ms step_avg:89.35ms
step:11/1770 train_time:987ms step_avg:89.76ms
step:12/1770 train_time:1082ms step_avg:90.20ms
step:13/1770 train_time:1178ms step_avg:90.61ms
step:14/1770 train_time:1274ms step_avg:90.97ms
step:15/1770 train_time:1369ms step_avg:91.27ms
step:16/1770 train_time:1463ms step_avg:91.46ms
step:17/1770 train_time:1558ms step_avg:91.64ms
step:18/1770 train_time:1653ms step_avg:91.81ms
step:19/1770 train_time:1747ms step_avg:91.96ms
step:20/1770 train_time:1841ms step_avg:92.05ms
step:21/1770 train_time:1936ms step_avg:92.18ms
step:22/1770 train_time:2030ms step_avg:92.29ms
step:23/1770 train_time:2125ms step_avg:92.41ms
step:24/1770 train_time:2220ms step_avg:92.50ms
step:25/1770 train_time:2315ms step_avg:92.58ms
step:26/1770 train_time:2409ms step_avg:92.66ms
step:27/1770 train_time:2503ms step_avg:92.72ms
step:28/1770 train_time:2599ms step_avg:92.81ms
step:29/1770 train_time:2694ms step_avg:92.90ms
step:30/1770 train_time:2789ms step_avg:92.96ms
step:31/1770 train_time:2883ms step_avg:92.99ms
step:32/1770 train_time:2977ms step_avg:93.02ms
step:33/1770 train_time:3072ms step_avg:93.09ms
step:34/1770 train_time:3167ms step_avg:93.14ms
step:35/1770 train_time:3261ms step_avg:93.17ms
step:36/1770 train_time:3356ms step_avg:93.23ms
step:37/1770 train_time:3452ms step_avg:93.29ms
step:38/1770 train_time:3546ms step_avg:93.33ms
step:39/1770 train_time:3641ms step_avg:93.36ms
step:40/1770 train_time:3736ms step_avg:93.40ms
step:41/1770 train_time:3831ms step_avg:93.43ms
step:42/1770 train_time:3925ms step_avg:93.45ms
step:43/1770 train_time:4020ms step_avg:93.50ms
step:44/1770 train_time:4116ms step_avg:93.56ms
step:45/1770 train_time:4212ms step_avg:93.60ms
step:46/1770 train_time:4306ms step_avg:93.61ms
step:47/1770 train_time:4401ms step_avg:93.65ms
step:48/1770 train_time:4497ms step_avg:93.69ms
step:49/1770 train_time:4591ms step_avg:93.70ms
step:50/1770 train_time:4686ms step_avg:93.71ms
step:51/1770 train_time:4780ms step_avg:93.72ms
step:52/1770 train_time:4876ms step_avg:93.76ms
step:53/1770 train_time:4971ms step_avg:93.79ms
step:54/1770 train_time:5065ms step_avg:93.79ms
step:55/1770 train_time:5160ms step_avg:93.82ms
step:56/1770 train_time:5255ms step_avg:93.84ms
step:57/1770 train_time:5350ms step_avg:93.86ms
step:58/1770 train_time:5445ms step_avg:93.87ms
step:59/1770 train_time:5539ms step_avg:93.88ms
step:60/1770 train_time:5634ms step_avg:93.91ms
step:61/1770 train_time:5729ms step_avg:93.92ms
step:62/1770 train_time:5823ms step_avg:93.92ms
step:63/1770 train_time:5918ms step_avg:93.94ms
step:64/1770 train_time:6014ms step_avg:93.97ms
step:65/1770 train_time:6110ms step_avg:93.99ms
step:66/1770 train_time:6204ms step_avg:94.00ms
step:67/1770 train_time:6298ms step_avg:94.01ms
step:68/1770 train_time:6393ms step_avg:94.02ms
step:69/1770 train_time:6488ms step_avg:94.03ms
step:70/1770 train_time:6582ms step_avg:94.03ms
step:71/1770 train_time:6676ms step_avg:94.03ms
step:72/1770 train_time:6771ms step_avg:94.04ms
step:73/1770 train_time:6865ms step_avg:94.04ms
step:74/1770 train_time:6959ms step_avg:94.04ms
step:75/1770 train_time:7054ms step_avg:94.06ms
step:76/1770 train_time:7150ms step_avg:94.07ms
step:77/1770 train_time:7243ms step_avg:94.07ms
step:78/1770 train_time:7338ms step_avg:94.08ms
step:79/1770 train_time:7433ms step_avg:94.09ms
step:80/1770 train_time:7527ms step_avg:94.09ms
step:81/1770 train_time:7621ms step_avg:94.08ms
step:82/1770 train_time:7716ms step_avg:94.10ms
step:83/1770 train_time:7811ms step_avg:94.11ms
step:84/1770 train_time:7906ms step_avg:94.12ms
step:85/1770 train_time:8000ms step_avg:94.11ms
step:86/1770 train_time:8095ms step_avg:94.13ms
step:87/1770 train_time:8190ms step_avg:94.14ms
step:88/1770 train_time:8284ms step_avg:94.14ms
step:89/1770 train_time:8378ms step_avg:94.14ms
step:90/1770 train_time:8473ms step_avg:94.15ms
step:91/1770 train_time:8569ms step_avg:94.17ms
step:92/1770 train_time:8663ms step_avg:94.16ms
step:93/1770 train_time:8758ms step_avg:94.17ms
step:94/1770 train_time:8852ms step_avg:94.17ms
step:95/1770 train_time:8947ms step_avg:94.18ms
step:96/1770 train_time:9041ms step_avg:94.18ms
step:97/1770 train_time:9136ms step_avg:94.18ms
step:98/1770 train_time:9231ms step_avg:94.20ms
step:99/1770 train_time:9325ms step_avg:94.20ms
step:100/1770 train_time:9421ms step_avg:94.21ms
step:101/1770 train_time:9517ms step_avg:94.22ms
step:102/1770 train_time:9612ms step_avg:94.23ms
step:103/1770 train_time:9707ms step_avg:94.24ms
step:104/1770 train_time:9801ms step_avg:94.24ms
step:105/1770 train_time:9896ms step_avg:94.24ms
step:106/1770 train_time:9990ms step_avg:94.25ms
step:107/1770 train_time:10085ms step_avg:94.25ms
step:108/1770 train_time:10179ms step_avg:94.25ms
step:109/1770 train_time:10274ms step_avg:94.25ms
step:110/1770 train_time:10369ms step_avg:94.26ms
step:111/1770 train_time:10463ms step_avg:94.26ms
step:112/1770 train_time:10558ms step_avg:94.27ms
step:113/1770 train_time:10654ms step_avg:94.28ms
step:114/1770 train_time:10749ms step_avg:94.29ms
step:115/1770 train_time:10842ms step_avg:94.28ms
step:116/1770 train_time:10937ms step_avg:94.29ms
step:117/1770 train_time:11032ms step_avg:94.29ms
step:118/1770 train_time:11126ms step_avg:94.29ms
step:119/1770 train_time:11220ms step_avg:94.29ms
step:120/1770 train_time:11315ms step_avg:94.29ms
step:121/1770 train_time:11409ms step_avg:94.29ms
step:122/1770 train_time:11503ms step_avg:94.29ms
step:123/1770 train_time:11598ms step_avg:94.29ms
step:124/1770 train_time:11692ms step_avg:94.29ms
step:125/1770 train_time:11787ms step_avg:94.29ms
step:125/1770 val_loss:4.6424 train_time:11881ms step_avg:95.05ms
step:126/1770 train_time:11898ms step_avg:94.43ms
step:127/1770 train_time:11981ms step_avg:94.34ms
step:128/1770 train_time:12081ms step_avg:94.38ms
step:129/1770 train_time:12176ms step_avg:94.39ms
step:130/1770 train_time:12270ms step_avg:94.38ms
step:131/1770 train_time:12364ms step_avg:94.38ms
step:132/1770 train_time:12459ms step_avg:94.39ms
step:133/1770 train_time:12553ms step_avg:94.38ms
step:134/1770 train_time:12647ms step_avg:94.38ms
step:135/1770 train_time:12743ms step_avg:94.39ms
step:136/1770 train_time:12837ms step_avg:94.39ms
step:137/1770 train_time:12933ms step_avg:94.40ms
step:138/1770 train_time:13029ms step_avg:94.41ms
step:139/1770 train_time:13125ms step_avg:94.43ms
step:140/1770 train_time:13223ms step_avg:94.45ms
step:141/1770 train_time:13319ms step_avg:94.46ms
step:142/1770 train_time:13413ms step_avg:94.46ms
step:143/1770 train_time:13507ms step_avg:94.46ms
step:144/1770 train_time:13602ms step_avg:94.46ms
step:145/1770 train_time:13698ms step_avg:94.47ms
step:146/1770 train_time:13792ms step_avg:94.47ms
step:147/1770 train_time:13888ms step_avg:94.47ms
step:148/1770 train_time:13984ms step_avg:94.49ms
step:149/1770 train_time:14080ms step_avg:94.50ms
step:150/1770 train_time:14175ms step_avg:94.50ms
step:151/1770 train_time:14270ms step_avg:94.50ms
step:152/1770 train_time:14366ms step_avg:94.51ms
step:153/1770 train_time:14462ms step_avg:94.52ms
step:154/1770 train_time:14557ms step_avg:94.52ms
step:155/1770 train_time:14651ms step_avg:94.52ms
step:156/1770 train_time:14746ms step_avg:94.53ms
step:157/1770 train_time:14842ms step_avg:94.54ms
step:158/1770 train_time:14938ms step_avg:94.54ms
step:159/1770 train_time:15034ms step_avg:94.55ms
step:160/1770 train_time:15128ms step_avg:94.55ms
step:161/1770 train_time:15224ms step_avg:94.56ms
step:162/1770 train_time:15320ms step_avg:94.57ms
step:163/1770 train_time:15416ms step_avg:94.58ms
step:164/1770 train_time:15511ms step_avg:94.58ms
step:165/1770 train_time:15606ms step_avg:94.58ms
step:166/1770 train_time:15701ms step_avg:94.59ms
step:167/1770 train_time:15796ms step_avg:94.59ms
step:168/1770 train_time:15891ms step_avg:94.59ms
step:169/1770 train_time:15987ms step_avg:94.60ms
step:170/1770 train_time:16083ms step_avg:94.61ms
step:171/1770 train_time:16178ms step_avg:94.61ms
step:172/1770 train_time:16274ms step_avg:94.61ms
step:173/1770 train_time:16369ms step_avg:94.62ms
step:174/1770 train_time:16464ms step_avg:94.62ms
step:175/1770 train_time:16559ms step_avg:94.62ms
step:176/1770 train_time:16654ms step_avg:94.63ms
step:177/1770 train_time:16749ms step_avg:94.63ms
step:178/1770 train_time:16844ms step_avg:94.63ms
step:179/1770 train_time:16939ms step_avg:94.63ms
step:180/1770 train_time:17034ms step_avg:94.64ms
step:181/1770 train_time:17129ms step_avg:94.64ms
step:182/1770 train_time:17225ms step_avg:94.64ms
step:183/1770 train_time:17320ms step_avg:94.65ms
step:184/1770 train_time:17416ms step_avg:94.65ms
step:185/1770 train_time:17510ms step_avg:94.65ms
step:186/1770 train_time:17605ms step_avg:94.65ms
step:187/1770 train_time:17701ms step_avg:94.66ms
step:188/1770 train_time:17796ms step_avg:94.66ms
step:189/1770 train_time:17891ms step_avg:94.66ms
step:190/1770 train_time:17986ms step_avg:94.66ms
step:191/1770 train_time:18081ms step_avg:94.67ms
step:192/1770 train_time:18177ms step_avg:94.67ms
step:193/1770 train_time:18272ms step_avg:94.67ms
step:194/1770 train_time:18367ms step_avg:94.67ms
step:195/1770 train_time:18462ms step_avg:94.68ms
step:196/1770 train_time:18557ms step_avg:94.68ms
step:197/1770 train_time:18652ms step_avg:94.68ms
step:198/1770 train_time:18747ms step_avg:94.68ms
step:199/1770 train_time:18843ms step_avg:94.69ms
step:200/1770 train_time:18938ms step_avg:94.69ms
step:201/1770 train_time:19033ms step_avg:94.69ms
step:202/1770 train_time:19129ms step_avg:94.70ms
step:203/1770 train_time:19224ms step_avg:94.70ms
step:204/1770 train_time:19320ms step_avg:94.70ms
step:205/1770 train_time:19414ms step_avg:94.70ms
step:206/1770 train_time:19509ms step_avg:94.70ms
step:207/1770 train_time:19604ms step_avg:94.71ms
step:208/1770 train_time:19700ms step_avg:94.71ms
step:209/1770 train_time:19795ms step_avg:94.71ms
step:210/1770 train_time:19890ms step_avg:94.71ms
step:211/1770 train_time:19985ms step_avg:94.71ms
step:212/1770 train_time:20081ms step_avg:94.72ms
step:213/1770 train_time:20176ms step_avg:94.72ms
step:214/1770 train_time:20271ms step_avg:94.72ms
step:215/1770 train_time:20366ms step_avg:94.73ms
step:216/1770 train_time:20462ms step_avg:94.73ms
step:217/1770 train_time:20558ms step_avg:94.74ms
step:218/1770 train_time:20653ms step_avg:94.74ms
step:219/1770 train_time:20748ms step_avg:94.74ms
step:220/1770 train_time:20843ms step_avg:94.74ms
step:221/1770 train_time:20938ms step_avg:94.74ms
step:222/1770 train_time:21033ms step_avg:94.74ms
step:223/1770 train_time:21128ms step_avg:94.74ms
step:224/1770 train_time:21223ms step_avg:94.75ms
step:225/1770 train_time:21319ms step_avg:94.75ms
step:226/1770 train_time:21414ms step_avg:94.75ms
step:227/1770 train_time:21509ms step_avg:94.75ms
step:228/1770 train_time:21605ms step_avg:94.76ms
step:229/1770 train_time:21701ms step_avg:94.76ms
step:230/1770 train_time:21796ms step_avg:94.76ms
step:231/1770 train_time:21890ms step_avg:94.76ms
step:232/1770 train_time:21985ms step_avg:94.76ms
step:233/1770 train_time:22081ms step_avg:94.77ms
step:234/1770 train_time:22175ms step_avg:94.77ms
step:235/1770 train_time:22271ms step_avg:94.77ms
step:236/1770 train_time:22366ms step_avg:94.77ms
step:237/1770 train_time:22461ms step_avg:94.77ms
step:238/1770 train_time:22557ms step_avg:94.78ms
step:239/1770 train_time:22652ms step_avg:94.78ms
step:240/1770 train_time:22747ms step_avg:94.78ms
step:241/1770 train_time:22843ms step_avg:94.78ms
step:242/1770 train_time:22938ms step_avg:94.79ms
step:243/1770 train_time:23033ms step_avg:94.78ms
step:244/1770 train_time:23128ms step_avg:94.79ms
step:245/1770 train_time:23224ms step_avg:94.79ms
step:246/1770 train_time:23319ms step_avg:94.79ms
step:247/1770 train_time:23414ms step_avg:94.79ms
step:248/1770 train_time:23509ms step_avg:94.79ms
step:249/1770 train_time:23604ms step_avg:94.80ms
step:250/1770 train_time:23700ms step_avg:94.80ms
step:250/1770 val_loss:4.1038 train_time:23795ms step_avg:95.18ms
step:251/1770 train_time:23812ms step_avg:94.87ms
step:252/1770 train_time:23897ms step_avg:94.83ms
step:253/1770 train_time:23994ms step_avg:94.84ms
step:254/1770 train_time:24089ms step_avg:94.84ms
step:255/1770 train_time:24185ms step_avg:94.84ms
step:256/1770 train_time:24279ms step_avg:94.84ms
step:257/1770 train_time:24373ms step_avg:94.84ms
step:258/1770 train_time:24468ms step_avg:94.84ms
step:259/1770 train_time:24564ms step_avg:94.84ms
step:260/1770 train_time:24658ms step_avg:94.84ms
step:261/1770 train_time:24752ms step_avg:94.84ms
step:262/1770 train_time:24849ms step_avg:94.84ms
step:263/1770 train_time:24946ms step_avg:94.85ms
step:264/1770 train_time:25042ms step_avg:94.86ms
step:265/1770 train_time:25137ms step_avg:94.86ms
step:266/1770 train_time:25233ms step_avg:94.86ms
step:267/1770 train_time:25328ms step_avg:94.86ms
step:268/1770 train_time:25423ms step_avg:94.86ms
step:269/1770 train_time:25518ms step_avg:94.86ms
step:270/1770 train_time:25613ms step_avg:94.86ms
step:271/1770 train_time:25709ms step_avg:94.87ms
step:272/1770 train_time:25805ms step_avg:94.87ms
step:273/1770 train_time:25900ms step_avg:94.87ms
step:274/1770 train_time:25996ms step_avg:94.88ms
step:275/1770 train_time:26093ms step_avg:94.88ms
step:276/1770 train_time:26190ms step_avg:94.89ms
step:277/1770 train_time:26286ms step_avg:94.89ms
step:278/1770 train_time:26381ms step_avg:94.90ms
step:279/1770 train_time:26476ms step_avg:94.90ms
step:280/1770 train_time:26571ms step_avg:94.90ms
step:281/1770 train_time:26666ms step_avg:94.90ms
step:282/1770 train_time:26761ms step_avg:94.90ms
step:283/1770 train_time:26857ms step_avg:94.90ms
step:284/1770 train_time:26952ms step_avg:94.90ms
step:285/1770 train_time:27048ms step_avg:94.91ms
step:286/1770 train_time:27144ms step_avg:94.91ms
step:287/1770 train_time:27239ms step_avg:94.91ms
step:288/1770 train_time:27334ms step_avg:94.91ms
step:289/1770 train_time:27430ms step_avg:94.91ms
step:290/1770 train_time:27525ms step_avg:94.91ms
step:291/1770 train_time:27620ms step_avg:94.91ms
step:292/1770 train_time:27715ms step_avg:94.92ms
step:293/1770 train_time:27812ms step_avg:94.92ms
step:294/1770 train_time:27908ms step_avg:94.92ms
step:295/1770 train_time:28004ms step_avg:94.93ms
step:296/1770 train_time:28099ms step_avg:94.93ms
step:297/1770 train_time:28195ms step_avg:94.93ms
step:298/1770 train_time:28292ms step_avg:94.94ms
step:299/1770 train_time:28387ms step_avg:94.94ms
step:300/1770 train_time:28483ms step_avg:94.94ms
step:301/1770 train_time:28578ms step_avg:94.94ms
step:302/1770 train_time:28673ms step_avg:94.95ms
step:303/1770 train_time:28769ms step_avg:94.95ms
step:304/1770 train_time:28865ms step_avg:94.95ms
step:305/1770 train_time:28960ms step_avg:94.95ms
step:306/1770 train_time:29056ms step_avg:94.95ms
step:307/1770 train_time:29152ms step_avg:94.96ms
step:308/1770 train_time:29248ms step_avg:94.96ms
step:309/1770 train_time:29343ms step_avg:94.96ms
step:310/1770 train_time:29438ms step_avg:94.96ms
step:311/1770 train_time:29534ms step_avg:94.97ms
step:312/1770 train_time:29630ms step_avg:94.97ms
step:313/1770 train_time:29726ms step_avg:94.97ms
step:314/1770 train_time:29821ms step_avg:94.97ms
step:315/1770 train_time:29917ms step_avg:94.97ms
step:316/1770 train_time:30012ms step_avg:94.98ms
step:317/1770 train_time:30109ms step_avg:94.98ms
step:318/1770 train_time:30205ms step_avg:94.98ms
step:319/1770 train_time:30300ms step_avg:94.98ms
step:320/1770 train_time:30396ms step_avg:94.99ms
step:321/1770 train_time:30492ms step_avg:94.99ms
step:322/1770 train_time:30588ms step_avg:94.99ms
step:323/1770 train_time:30684ms step_avg:95.00ms
step:324/1770 train_time:30779ms step_avg:95.00ms
step:325/1770 train_time:30875ms step_avg:95.00ms
step:326/1770 train_time:30971ms step_avg:95.00ms
step:327/1770 train_time:31067ms step_avg:95.01ms
step:328/1770 train_time:31163ms step_avg:95.01ms
step:329/1770 train_time:31259ms step_avg:95.01ms
step:330/1770 train_time:31354ms step_avg:95.01ms
step:331/1770 train_time:31449ms step_avg:95.01ms
step:332/1770 train_time:31545ms step_avg:95.02ms
step:333/1770 train_time:31641ms step_avg:95.02ms
step:334/1770 train_time:31736ms step_avg:95.02ms
step:335/1770 train_time:31831ms step_avg:95.02ms
step:336/1770 train_time:31927ms step_avg:95.02ms
step:337/1770 train_time:32023ms step_avg:95.02ms
step:338/1770 train_time:32119ms step_avg:95.03ms
step:339/1770 train_time:32215ms step_avg:95.03ms
step:340/1770 train_time:32311ms step_avg:95.03ms
step:341/1770 train_time:32407ms step_avg:95.04ms
step:342/1770 train_time:32503ms step_avg:95.04ms
step:343/1770 train_time:32598ms step_avg:95.04ms
step:344/1770 train_time:32694ms step_avg:95.04ms
step:345/1770 train_time:32790ms step_avg:95.04ms
step:346/1770 train_time:32885ms step_avg:95.04ms
step:347/1770 train_time:32981ms step_avg:95.05ms
step:348/1770 train_time:33076ms step_avg:95.05ms
step:349/1770 train_time:33172ms step_avg:95.05ms
step:350/1770 train_time:33268ms step_avg:95.05ms
step:351/1770 train_time:33364ms step_avg:95.05ms
step:352/1770 train_time:33459ms step_avg:95.05ms
step:353/1770 train_time:33555ms step_avg:95.06ms
step:354/1770 train_time:33651ms step_avg:95.06ms
step:355/1770 train_time:33747ms step_avg:95.06ms
step:356/1770 train_time:33843ms step_avg:95.06ms
step:357/1770 train_time:33938ms step_avg:95.06ms
step:358/1770 train_time:34034ms step_avg:95.07ms
step:359/1770 train_time:34129ms step_avg:95.07ms
step:360/1770 train_time:34225ms step_avg:95.07ms
step:361/1770 train_time:34321ms step_avg:95.07ms
step:362/1770 train_time:34417ms step_avg:95.07ms
step:363/1770 train_time:34512ms step_avg:95.08ms
step:364/1770 train_time:34609ms step_avg:95.08ms
step:365/1770 train_time:34705ms step_avg:95.08ms
step:366/1770 train_time:34801ms step_avg:95.08ms
step:367/1770 train_time:34896ms step_avg:95.09ms
step:368/1770 train_time:34992ms step_avg:95.09ms
step:369/1770 train_time:35088ms step_avg:95.09ms
step:370/1770 train_time:35184ms step_avg:95.09ms
step:371/1770 train_time:35280ms step_avg:95.09ms
step:372/1770 train_time:35376ms step_avg:95.10ms
step:373/1770 train_time:35472ms step_avg:95.10ms
step:374/1770 train_time:35567ms step_avg:95.10ms
step:375/1770 train_time:35663ms step_avg:95.10ms
step:375/1770 val_loss:3.8937 train_time:35759ms step_avg:95.36ms
step:376/1770 train_time:35778ms step_avg:95.15ms
step:377/1770 train_time:35863ms step_avg:95.13ms
step:378/1770 train_time:35964ms step_avg:95.14ms
step:379/1770 train_time:36059ms step_avg:95.14ms
step:380/1770 train_time:36155ms step_avg:95.14ms
step:381/1770 train_time:36250ms step_avg:95.15ms
step:382/1770 train_time:36346ms step_avg:95.15ms
step:383/1770 train_time:36440ms step_avg:95.14ms
step:384/1770 train_time:36536ms step_avg:95.14ms
step:385/1770 train_time:36630ms step_avg:95.14ms
step:386/1770 train_time:36725ms step_avg:95.14ms
step:387/1770 train_time:36822ms step_avg:95.15ms
step:388/1770 train_time:36919ms step_avg:95.15ms
step:389/1770 train_time:37016ms step_avg:95.16ms
step:390/1770 train_time:37111ms step_avg:95.16ms
step:391/1770 train_time:37207ms step_avg:95.16ms
step:392/1770 train_time:37303ms step_avg:95.16ms
step:393/1770 train_time:37399ms step_avg:95.16ms
step:394/1770 train_time:37494ms step_avg:95.16ms
step:395/1770 train_time:37589ms step_avg:95.16ms
step:396/1770 train_time:37687ms step_avg:95.17ms
step:397/1770 train_time:37785ms step_avg:95.18ms
step:398/1770 train_time:37883ms step_avg:95.18ms
step:399/1770 train_time:37982ms step_avg:95.19ms
step:400/1770 train_time:38081ms step_avg:95.20ms
step:401/1770 train_time:38180ms step_avg:95.21ms
step:402/1770 train_time:38278ms step_avg:95.22ms
step:403/1770 train_time:38376ms step_avg:95.23ms
step:404/1770 train_time:38474ms step_avg:95.23ms
step:405/1770 train_time:38572ms step_avg:95.24ms
step:406/1770 train_time:38669ms step_avg:95.24ms
step:407/1770 train_time:38766ms step_avg:95.25ms
step:408/1770 train_time:38864ms step_avg:95.26ms
step:409/1770 train_time:38963ms step_avg:95.26ms
step:410/1770 train_time:39061ms step_avg:95.27ms
step:411/1770 train_time:39159ms step_avg:95.28ms
step:412/1770 train_time:39258ms step_avg:95.29ms
step:413/1770 train_time:39356ms step_avg:95.29ms
step:414/1770 train_time:39453ms step_avg:95.30ms
step:415/1770 train_time:39551ms step_avg:95.30ms
step:416/1770 train_time:39649ms step_avg:95.31ms
step:417/1770 train_time:39746ms step_avg:95.31ms
step:418/1770 train_time:39845ms step_avg:95.32ms
step:419/1770 train_time:39943ms step_avg:95.33ms
step:420/1770 train_time:40041ms step_avg:95.34ms
step:421/1770 train_time:40138ms step_avg:95.34ms
step:422/1770 train_time:40237ms step_avg:95.35ms
step:423/1770 train_time:40335ms step_avg:95.35ms
step:424/1770 train_time:40432ms step_avg:95.36ms
step:425/1770 train_time:40530ms step_avg:95.37ms
step:426/1770 train_time:40628ms step_avg:95.37ms
step:427/1770 train_time:40726ms step_avg:95.38ms
step:428/1770 train_time:40825ms step_avg:95.38ms
step:429/1770 train_time:40923ms step_avg:95.39ms
step:430/1770 train_time:41022ms step_avg:95.40ms
step:431/1770 train_time:41121ms step_avg:95.41ms
step:432/1770 train_time:41218ms step_avg:95.41ms
step:433/1770 train_time:41316ms step_avg:95.42ms
step:434/1770 train_time:41414ms step_avg:95.42ms
step:435/1770 train_time:41512ms step_avg:95.43ms
step:436/1770 train_time:41609ms step_avg:95.43ms
step:437/1770 train_time:41706ms step_avg:95.44ms
step:438/1770 train_time:41804ms step_avg:95.44ms
step:439/1770 train_time:41902ms step_avg:95.45ms
step:440/1770 train_time:42000ms step_avg:95.46ms
step:441/1770 train_time:42100ms step_avg:95.46ms
step:442/1770 train_time:42198ms step_avg:95.47ms
step:443/1770 train_time:42296ms step_avg:95.48ms
step:444/1770 train_time:42394ms step_avg:95.48ms
step:445/1770 train_time:42493ms step_avg:95.49ms
step:446/1770 train_time:42591ms step_avg:95.50ms
step:447/1770 train_time:42689ms step_avg:95.50ms
step:448/1770 train_time:42787ms step_avg:95.51ms
step:449/1770 train_time:42885ms step_avg:95.51ms
step:450/1770 train_time:42982ms step_avg:95.52ms
step:451/1770 train_time:43080ms step_avg:95.52ms
step:452/1770 train_time:43178ms step_avg:95.53ms
step:453/1770 train_time:43276ms step_avg:95.53ms
step:454/1770 train_time:43374ms step_avg:95.54ms
step:455/1770 train_time:43473ms step_avg:95.54ms
step:456/1770 train_time:43571ms step_avg:95.55ms
step:457/1770 train_time:43668ms step_avg:95.55ms
step:458/1770 train_time:43766ms step_avg:95.56ms
step:459/1770 train_time:43863ms step_avg:95.56ms
step:460/1770 train_time:43961ms step_avg:95.57ms
step:461/1770 train_time:44059ms step_avg:95.57ms
step:462/1770 train_time:44157ms step_avg:95.58ms
step:463/1770 train_time:44255ms step_avg:95.58ms
step:464/1770 train_time:44353ms step_avg:95.59ms
step:465/1770 train_time:44451ms step_avg:95.59ms
step:466/1770 train_time:44550ms step_avg:95.60ms
step:467/1770 train_time:44648ms step_avg:95.61ms
step:468/1770 train_time:44746ms step_avg:95.61ms
step:469/1770 train_time:44843ms step_avg:95.61ms
step:470/1770 train_time:44942ms step_avg:95.62ms
step:471/1770 train_time:45039ms step_avg:95.62ms
step:472/1770 train_time:45137ms step_avg:95.63ms
step:473/1770 train_time:45235ms step_avg:95.63ms
step:474/1770 train_time:45332ms step_avg:95.64ms
step:475/1770 train_time:45429ms step_avg:95.64ms
step:476/1770 train_time:45527ms step_avg:95.64ms
step:477/1770 train_time:45625ms step_avg:95.65ms
step:478/1770 train_time:45724ms step_avg:95.66ms
step:479/1770 train_time:45821ms step_avg:95.66ms
step:480/1770 train_time:45921ms step_avg:95.67ms
step:481/1770 train_time:46019ms step_avg:95.67ms
step:482/1770 train_time:46116ms step_avg:95.68ms
step:483/1770 train_time:46214ms step_avg:95.68ms
step:484/1770 train_time:46311ms step_avg:95.68ms
step:485/1770 train_time:46409ms step_avg:95.69ms
step:486/1770 train_time:46506ms step_avg:95.69ms
step:487/1770 train_time:46604ms step_avg:95.70ms
step:488/1770 train_time:46703ms step_avg:95.70ms
step:489/1770 train_time:46800ms step_avg:95.71ms
step:490/1770 train_time:46898ms step_avg:95.71ms
step:491/1770 train_time:46996ms step_avg:95.71ms
step:492/1770 train_time:47094ms step_avg:95.72ms
step:493/1770 train_time:47191ms step_avg:95.72ms
step:494/1770 train_time:47289ms step_avg:95.73ms
step:495/1770 train_time:47388ms step_avg:95.73ms
step:496/1770 train_time:47485ms step_avg:95.74ms
step:497/1770 train_time:47583ms step_avg:95.74ms
step:498/1770 train_time:47681ms step_avg:95.74ms
step:499/1770 train_time:47780ms step_avg:95.75ms
step:500/1770 train_time:47878ms step_avg:95.76ms
step:500/1770 val_loss:3.7464 train_time:47975ms step_avg:95.95ms
step:501/1770 train_time:47995ms step_avg:95.80ms
step:502/1770 train_time:48079ms step_avg:95.77ms
step:503/1770 train_time:48178ms step_avg:95.78ms
step:504/1770 train_time:48276ms step_avg:95.79ms
step:505/1770 train_time:48373ms step_avg:95.79ms
step:506/1770 train_time:48471ms step_avg:95.79ms
step:507/1770 train_time:48568ms step_avg:95.80ms
step:508/1770 train_time:48666ms step_avg:95.80ms
step:509/1770 train_time:48763ms step_avg:95.80ms
step:510/1770 train_time:48860ms step_avg:95.80ms
step:511/1770 train_time:48958ms step_avg:95.81ms
step:512/1770 train_time:49057ms step_avg:95.81ms
step:513/1770 train_time:49156ms step_avg:95.82ms
step:514/1770 train_time:49254ms step_avg:95.82ms
step:515/1770 train_time:49352ms step_avg:95.83ms
step:516/1770 train_time:49450ms step_avg:95.83ms
step:517/1770 train_time:49548ms step_avg:95.84ms
step:518/1770 train_time:49646ms step_avg:95.84ms
step:519/1770 train_time:49744ms step_avg:95.85ms
step:520/1770 train_time:49842ms step_avg:95.85ms
step:521/1770 train_time:49940ms step_avg:95.85ms
step:522/1770 train_time:50038ms step_avg:95.86ms
step:523/1770 train_time:50136ms step_avg:95.86ms
step:524/1770 train_time:50235ms step_avg:95.87ms
step:525/1770 train_time:50334ms step_avg:95.87ms
step:526/1770 train_time:50432ms step_avg:95.88ms
step:527/1770 train_time:50530ms step_avg:95.88ms
step:528/1770 train_time:50628ms step_avg:95.89ms
step:529/1770 train_time:50726ms step_avg:95.89ms
step:530/1770 train_time:50825ms step_avg:95.90ms
step:531/1770 train_time:50924ms step_avg:95.90ms
step:532/1770 train_time:51022ms step_avg:95.91ms
step:533/1770 train_time:51120ms step_avg:95.91ms
step:534/1770 train_time:51220ms step_avg:95.92ms
step:535/1770 train_time:51319ms step_avg:95.92ms
step:536/1770 train_time:51417ms step_avg:95.93ms
step:537/1770 train_time:51515ms step_avg:95.93ms
step:538/1770 train_time:51613ms step_avg:95.93ms
step:539/1770 train_time:51711ms step_avg:95.94ms
step:540/1770 train_time:51810ms step_avg:95.94ms
step:541/1770 train_time:51908ms step_avg:95.95ms
step:542/1770 train_time:52007ms step_avg:95.95ms
step:543/1770 train_time:52106ms step_avg:95.96ms
step:544/1770 train_time:52206ms step_avg:95.97ms
step:545/1770 train_time:52305ms step_avg:95.97ms
step:546/1770 train_time:52404ms step_avg:95.98ms
step:547/1770 train_time:52504ms step_avg:95.98ms
step:548/1770 train_time:52602ms step_avg:95.99ms
step:549/1770 train_time:52701ms step_avg:95.99ms
step:550/1770 train_time:52799ms step_avg:96.00ms
step:551/1770 train_time:52898ms step_avg:96.00ms
step:552/1770 train_time:52995ms step_avg:96.01ms
step:553/1770 train_time:53093ms step_avg:96.01ms
step:554/1770 train_time:53191ms step_avg:96.01ms
step:555/1770 train_time:53291ms step_avg:96.02ms
step:556/1770 train_time:53390ms step_avg:96.03ms
step:557/1770 train_time:53489ms step_avg:96.03ms
step:558/1770 train_time:53589ms step_avg:96.04ms
step:559/1770 train_time:53688ms step_avg:96.04ms
step:560/1770 train_time:53787ms step_avg:96.05ms
step:561/1770 train_time:53887ms step_avg:96.06ms
step:562/1770 train_time:53987ms step_avg:96.06ms
step:563/1770 train_time:54086ms step_avg:96.07ms
step:564/1770 train_time:54185ms step_avg:96.07ms
step:565/1770 train_time:54283ms step_avg:96.08ms
step:566/1770 train_time:54382ms step_avg:96.08ms
step:567/1770 train_time:54480ms step_avg:96.08ms
step:568/1770 train_time:54578ms step_avg:96.09ms
step:569/1770 train_time:54676ms step_avg:96.09ms
step:570/1770 train_time:54775ms step_avg:96.10ms
step:571/1770 train_time:54873ms step_avg:96.10ms
step:572/1770 train_time:54972ms step_avg:96.10ms
step:573/1770 train_time:55070ms step_avg:96.11ms
step:574/1770 train_time:55169ms step_avg:96.11ms
step:575/1770 train_time:55268ms step_avg:96.12ms
step:576/1770 train_time:55366ms step_avg:96.12ms
step:577/1770 train_time:55464ms step_avg:96.12ms
step:578/1770 train_time:55562ms step_avg:96.13ms
step:579/1770 train_time:55661ms step_avg:96.13ms
step:580/1770 train_time:55760ms step_avg:96.14ms
step:581/1770 train_time:55858ms step_avg:96.14ms
step:582/1770 train_time:55956ms step_avg:96.15ms
step:583/1770 train_time:56055ms step_avg:96.15ms
step:584/1770 train_time:56153ms step_avg:96.15ms
step:585/1770 train_time:56251ms step_avg:96.15ms
step:586/1770 train_time:56350ms step_avg:96.16ms
step:587/1770 train_time:56448ms step_avg:96.16ms
step:588/1770 train_time:56547ms step_avg:96.17ms
step:589/1770 train_time:56646ms step_avg:96.17ms
step:590/1770 train_time:56746ms step_avg:96.18ms
step:591/1770 train_time:56845ms step_avg:96.19ms
step:592/1770 train_time:56944ms step_avg:96.19ms
step:593/1770 train_time:57044ms step_avg:96.20ms
step:594/1770 train_time:57143ms step_avg:96.20ms
step:595/1770 train_time:57242ms step_avg:96.20ms
step:596/1770 train_time:57340ms step_avg:96.21ms
step:597/1770 train_time:57438ms step_avg:96.21ms
step:598/1770 train_time:57536ms step_avg:96.21ms
step:599/1770 train_time:57633ms step_avg:96.22ms
step:600/1770 train_time:57731ms step_avg:96.22ms
step:601/1770 train_time:57830ms step_avg:96.22ms
step:602/1770 train_time:57929ms step_avg:96.23ms
step:603/1770 train_time:58028ms step_avg:96.23ms
step:604/1770 train_time:58127ms step_avg:96.24ms
step:605/1770 train_time:58226ms step_avg:96.24ms
step:606/1770 train_time:58326ms step_avg:96.25ms
step:607/1770 train_time:58426ms step_avg:96.25ms
step:608/1770 train_time:58526ms step_avg:96.26ms
step:609/1770 train_time:58625ms step_avg:96.26ms
step:610/1770 train_time:58724ms step_avg:96.27ms
step:611/1770 train_time:58823ms step_avg:96.27ms
step:612/1770 train_time:58921ms step_avg:96.28ms
step:613/1770 train_time:59019ms step_avg:96.28ms
step:614/1770 train_time:59117ms step_avg:96.28ms
step:615/1770 train_time:59215ms step_avg:96.28ms
step:616/1770 train_time:59313ms step_avg:96.29ms
step:617/1770 train_time:59412ms step_avg:96.29ms
step:618/1770 train_time:59511ms step_avg:96.30ms
step:619/1770 train_time:59611ms step_avg:96.30ms
step:620/1770 train_time:59709ms step_avg:96.30ms
step:621/1770 train_time:59807ms step_avg:96.31ms
step:622/1770 train_time:59906ms step_avg:96.31ms
step:623/1770 train_time:60005ms step_avg:96.32ms
step:624/1770 train_time:60103ms step_avg:96.32ms
step:625/1770 train_time:60202ms step_avg:96.32ms
step:625/1770 val_loss:3.6620 train_time:60300ms step_avg:96.48ms
step:626/1770 train_time:60318ms step_avg:96.35ms
step:627/1770 train_time:60406ms step_avg:96.34ms
step:628/1770 train_time:60507ms step_avg:96.35ms
step:629/1770 train_time:60605ms step_avg:96.35ms
step:630/1770 train_time:60703ms step_avg:96.35ms
step:631/1770 train_time:60801ms step_avg:96.36ms
step:632/1770 train_time:60898ms step_avg:96.36ms
step:633/1770 train_time:60997ms step_avg:96.36ms
step:634/1770 train_time:61095ms step_avg:96.36ms
step:635/1770 train_time:61192ms step_avg:96.37ms
step:636/1770 train_time:61291ms step_avg:96.37ms
step:637/1770 train_time:61390ms step_avg:96.37ms
step:638/1770 train_time:61489ms step_avg:96.38ms
step:639/1770 train_time:61588ms step_avg:96.38ms
step:640/1770 train_time:61687ms step_avg:96.39ms
step:641/1770 train_time:61784ms step_avg:96.39ms
step:642/1770 train_time:61881ms step_avg:96.39ms
step:643/1770 train_time:61979ms step_avg:96.39ms
step:644/1770 train_time:62077ms step_avg:96.39ms
step:645/1770 train_time:62175ms step_avg:96.40ms
step:646/1770 train_time:62274ms step_avg:96.40ms
step:647/1770 train_time:62373ms step_avg:96.40ms
step:648/1770 train_time:62473ms step_avg:96.41ms
step:649/1770 train_time:62572ms step_avg:96.41ms
step:650/1770 train_time:62671ms step_avg:96.42ms
step:651/1770 train_time:62768ms step_avg:96.42ms
step:652/1770 train_time:62866ms step_avg:96.42ms
step:653/1770 train_time:62964ms step_avg:96.42ms
step:654/1770 train_time:63062ms step_avg:96.43ms
step:655/1770 train_time:63161ms step_avg:96.43ms
step:656/1770 train_time:63259ms step_avg:96.43ms
step:657/1770 train_time:63357ms step_avg:96.43ms
step:658/1770 train_time:63458ms step_avg:96.44ms
step:659/1770 train_time:63560ms step_avg:96.45ms
step:660/1770 train_time:63661ms step_avg:96.46ms
step:661/1770 train_time:63762ms step_avg:96.46ms
step:662/1770 train_time:63864ms step_avg:96.47ms
step:663/1770 train_time:63964ms step_avg:96.48ms
step:664/1770 train_time:64064ms step_avg:96.48ms
step:665/1770 train_time:64163ms step_avg:96.49ms
step:666/1770 train_time:64263ms step_avg:96.49ms
step:667/1770 train_time:64363ms step_avg:96.50ms
step:668/1770 train_time:64464ms step_avg:96.50ms
step:669/1770 train_time:64565ms step_avg:96.51ms
step:670/1770 train_time:64665ms step_avg:96.52ms
step:671/1770 train_time:64765ms step_avg:96.52ms
step:672/1770 train_time:64865ms step_avg:96.52ms
step:673/1770 train_time:64964ms step_avg:96.53ms
step:674/1770 train_time:65064ms step_avg:96.53ms
step:675/1770 train_time:65164ms step_avg:96.54ms
step:676/1770 train_time:65264ms step_avg:96.54ms
step:677/1770 train_time:65364ms step_avg:96.55ms
step:678/1770 train_time:65465ms step_avg:96.56ms
step:679/1770 train_time:65565ms step_avg:96.56ms
step:680/1770 train_time:65665ms step_avg:96.57ms
step:681/1770 train_time:65765ms step_avg:96.57ms
step:682/1770 train_time:65865ms step_avg:96.58ms
step:683/1770 train_time:65964ms step_avg:96.58ms
step:684/1770 train_time:66064ms step_avg:96.58ms
step:685/1770 train_time:66164ms step_avg:96.59ms
step:686/1770 train_time:66264ms step_avg:96.59ms
step:687/1770 train_time:66364ms step_avg:96.60ms
step:688/1770 train_time:66464ms step_avg:96.60ms
step:689/1770 train_time:66564ms step_avg:96.61ms
step:690/1770 train_time:66664ms step_avg:96.61ms
step:691/1770 train_time:66764ms step_avg:96.62ms
step:692/1770 train_time:66864ms step_avg:96.62ms
step:693/1770 train_time:66964ms step_avg:96.63ms
step:694/1770 train_time:67064ms step_avg:96.63ms
step:695/1770 train_time:67163ms step_avg:96.64ms
step:696/1770 train_time:67263ms step_avg:96.64ms
step:697/1770 train_time:67363ms step_avg:96.65ms
step:698/1770 train_time:67463ms step_avg:96.65ms
step:699/1770 train_time:67564ms step_avg:96.66ms
step:700/1770 train_time:67664ms step_avg:96.66ms
step:701/1770 train_time:67765ms step_avg:96.67ms
step:702/1770 train_time:67865ms step_avg:96.67ms
step:703/1770 train_time:67965ms step_avg:96.68ms
step:704/1770 train_time:68065ms step_avg:96.68ms
step:705/1770 train_time:68165ms step_avg:96.69ms
step:706/1770 train_time:68265ms step_avg:96.69ms
step:707/1770 train_time:68365ms step_avg:96.70ms
step:708/1770 train_time:68465ms step_avg:96.70ms
step:709/1770 train_time:68565ms step_avg:96.71ms
step:710/1770 train_time:68667ms step_avg:96.71ms
step:711/1770 train_time:68766ms step_avg:96.72ms
step:712/1770 train_time:68866ms step_avg:96.72ms
step:713/1770 train_time:68966ms step_avg:96.73ms
step:714/1770 train_time:69065ms step_avg:96.73ms
step:715/1770 train_time:69165ms step_avg:96.73ms
step:716/1770 train_time:69264ms step_avg:96.74ms
step:717/1770 train_time:69364ms step_avg:96.74ms
step:718/1770 train_time:69464ms step_avg:96.75ms
step:719/1770 train_time:69565ms step_avg:96.75ms
step:720/1770 train_time:69666ms step_avg:96.76ms
step:721/1770 train_time:69766ms step_avg:96.76ms
step:722/1770 train_time:69866ms step_avg:96.77ms
step:723/1770 train_time:69966ms step_avg:96.77ms
step:724/1770 train_time:70066ms step_avg:96.78ms
step:725/1770 train_time:70166ms step_avg:96.78ms
step:726/1770 train_time:70265ms step_avg:96.78ms
step:727/1770 train_time:70364ms step_avg:96.79ms
step:728/1770 train_time:70464ms step_avg:96.79ms
step:729/1770 train_time:70565ms step_avg:96.80ms
step:730/1770 train_time:70666ms step_avg:96.80ms
step:731/1770 train_time:70766ms step_avg:96.81ms
step:732/1770 train_time:70866ms step_avg:96.81ms
step:733/1770 train_time:70966ms step_avg:96.82ms
step:734/1770 train_time:71065ms step_avg:96.82ms
step:735/1770 train_time:71166ms step_avg:96.82ms
step:736/1770 train_time:71265ms step_avg:96.83ms
step:737/1770 train_time:71365ms step_avg:96.83ms
step:738/1770 train_time:71464ms step_avg:96.84ms
step:739/1770 train_time:71565ms step_avg:96.84ms
step:740/1770 train_time:71665ms step_avg:96.84ms
step:741/1770 train_time:71765ms step_avg:96.85ms
step:742/1770 train_time:71865ms step_avg:96.85ms
step:743/1770 train_time:71965ms step_avg:96.86ms
step:744/1770 train_time:72065ms step_avg:96.86ms
step:745/1770 train_time:72165ms step_avg:96.87ms
step:746/1770 train_time:72264ms step_avg:96.87ms
step:747/1770 train_time:72364ms step_avg:96.87ms
step:748/1770 train_time:72465ms step_avg:96.88ms
step:749/1770 train_time:72565ms step_avg:96.88ms
step:750/1770 train_time:72664ms step_avg:96.89ms
step:750/1770 val_loss:3.5977 train_time:72763ms step_avg:97.02ms
step:751/1770 train_time:72781ms step_avg:96.91ms
step:752/1770 train_time:72868ms step_avg:96.90ms
step:753/1770 train_time:72969ms step_avg:96.90ms
step:754/1770 train_time:73069ms step_avg:96.91ms
step:755/1770 train_time:73168ms step_avg:96.91ms
step:756/1770 train_time:73268ms step_avg:96.91ms
step:757/1770 train_time:73367ms step_avg:96.92ms
step:758/1770 train_time:73466ms step_avg:96.92ms
step:759/1770 train_time:73565ms step_avg:96.92ms
step:760/1770 train_time:73664ms step_avg:96.93ms
step:761/1770 train_time:73764ms step_avg:96.93ms
step:762/1770 train_time:73865ms step_avg:96.94ms
step:763/1770 train_time:73965ms step_avg:96.94ms
step:764/1770 train_time:74065ms step_avg:96.94ms
step:765/1770 train_time:74166ms step_avg:96.95ms
step:766/1770 train_time:74265ms step_avg:96.95ms
step:767/1770 train_time:74365ms step_avg:96.96ms
step:768/1770 train_time:74464ms step_avg:96.96ms
step:769/1770 train_time:74564ms step_avg:96.96ms
step:770/1770 train_time:74663ms step_avg:96.97ms
step:771/1770 train_time:74763ms step_avg:96.97ms
step:772/1770 train_time:74863ms step_avg:96.97ms
step:773/1770 train_time:74963ms step_avg:96.98ms
step:774/1770 train_time:75062ms step_avg:96.98ms
step:775/1770 train_time:75164ms step_avg:96.99ms
step:776/1770 train_time:75265ms step_avg:96.99ms
step:777/1770 train_time:75364ms step_avg:96.99ms
step:778/1770 train_time:75464ms step_avg:97.00ms
step:779/1770 train_time:75563ms step_avg:97.00ms
step:780/1770 train_time:75662ms step_avg:97.00ms
step:781/1770 train_time:75762ms step_avg:97.01ms
step:782/1770 train_time:75862ms step_avg:97.01ms
step:783/1770 train_time:75963ms step_avg:97.01ms
step:784/1770 train_time:76062ms step_avg:97.02ms
step:785/1770 train_time:76163ms step_avg:97.02ms
step:786/1770 train_time:76262ms step_avg:97.03ms
step:787/1770 train_time:76362ms step_avg:97.03ms
step:788/1770 train_time:76463ms step_avg:97.03ms
step:789/1770 train_time:76563ms step_avg:97.04ms
step:790/1770 train_time:76663ms step_avg:97.04ms
step:791/1770 train_time:76763ms step_avg:97.05ms
step:792/1770 train_time:76863ms step_avg:97.05ms
step:793/1770 train_time:76963ms step_avg:97.05ms
step:794/1770 train_time:77064ms step_avg:97.06ms
step:795/1770 train_time:77164ms step_avg:97.06ms
step:796/1770 train_time:77264ms step_avg:97.07ms
step:797/1770 train_time:77365ms step_avg:97.07ms
step:798/1770 train_time:77464ms step_avg:97.07ms
step:799/1770 train_time:77564ms step_avg:97.08ms
step:800/1770 train_time:77664ms step_avg:97.08ms
step:801/1770 train_time:77764ms step_avg:97.08ms
step:802/1770 train_time:77864ms step_avg:97.09ms
step:803/1770 train_time:77964ms step_avg:97.09ms
step:804/1770 train_time:78064ms step_avg:97.09ms
step:805/1770 train_time:78164ms step_avg:97.10ms
step:806/1770 train_time:78264ms step_avg:97.10ms
step:807/1770 train_time:78364ms step_avg:97.10ms
step:808/1770 train_time:78464ms step_avg:97.11ms
step:809/1770 train_time:78563ms step_avg:97.11ms
step:810/1770 train_time:78664ms step_avg:97.12ms
step:811/1770 train_time:78763ms step_avg:97.12ms
step:812/1770 train_time:78863ms step_avg:97.12ms
step:813/1770 train_time:78963ms step_avg:97.13ms
step:814/1770 train_time:79063ms step_avg:97.13ms
step:815/1770 train_time:79163ms step_avg:97.13ms
step:816/1770 train_time:79262ms step_avg:97.14ms
step:817/1770 train_time:79363ms step_avg:97.14ms
step:818/1770 train_time:79463ms step_avg:97.14ms
step:819/1770 train_time:79563ms step_avg:97.15ms
step:820/1770 train_time:79663ms step_avg:97.15ms
step:821/1770 train_time:79763ms step_avg:97.15ms
step:822/1770 train_time:79862ms step_avg:97.16ms
step:823/1770 train_time:79963ms step_avg:97.16ms
step:824/1770 train_time:80064ms step_avg:97.16ms
step:825/1770 train_time:80164ms step_avg:97.17ms
step:826/1770 train_time:80264ms step_avg:97.17ms
step:827/1770 train_time:80364ms step_avg:97.18ms
step:828/1770 train_time:80464ms step_avg:97.18ms
step:829/1770 train_time:80563ms step_avg:97.18ms
step:830/1770 train_time:80662ms step_avg:97.18ms
step:831/1770 train_time:80763ms step_avg:97.19ms
step:832/1770 train_time:80863ms step_avg:97.19ms
step:833/1770 train_time:80963ms step_avg:97.19ms
step:834/1770 train_time:81064ms step_avg:97.20ms
step:835/1770 train_time:81163ms step_avg:97.20ms
step:836/1770 train_time:81264ms step_avg:97.21ms
step:837/1770 train_time:81364ms step_avg:97.21ms
step:838/1770 train_time:81463ms step_avg:97.21ms
step:839/1770 train_time:81563ms step_avg:97.21ms
step:840/1770 train_time:81663ms step_avg:97.22ms
step:841/1770 train_time:81764ms step_avg:97.22ms
step:842/1770 train_time:81863ms step_avg:97.22ms
step:843/1770 train_time:81963ms step_avg:97.23ms
step:844/1770 train_time:82064ms step_avg:97.23ms
step:845/1770 train_time:82164ms step_avg:97.24ms
step:846/1770 train_time:82264ms step_avg:97.24ms
step:847/1770 train_time:82364ms step_avg:97.24ms
step:848/1770 train_time:82463ms step_avg:97.24ms
step:849/1770 train_time:82563ms step_avg:97.25ms
step:850/1770 train_time:82663ms step_avg:97.25ms
step:851/1770 train_time:82763ms step_avg:97.25ms
step:852/1770 train_time:82863ms step_avg:97.26ms
step:853/1770 train_time:82963ms step_avg:97.26ms
step:854/1770 train_time:83063ms step_avg:97.26ms
step:855/1770 train_time:83163ms step_avg:97.27ms
step:856/1770 train_time:83263ms step_avg:97.27ms
step:857/1770 train_time:83363ms step_avg:97.27ms
step:858/1770 train_time:83464ms step_avg:97.28ms
step:859/1770 train_time:83564ms step_avg:97.28ms
step:860/1770 train_time:83664ms step_avg:97.28ms
step:861/1770 train_time:83764ms step_avg:97.29ms
step:862/1770 train_time:83863ms step_avg:97.29ms
step:863/1770 train_time:83963ms step_avg:97.29ms
step:864/1770 train_time:84063ms step_avg:97.30ms
step:865/1770 train_time:84162ms step_avg:97.30ms
step:866/1770 train_time:84263ms step_avg:97.30ms
step:867/1770 train_time:84363ms step_avg:97.30ms
step:868/1770 train_time:84463ms step_avg:97.31ms
step:869/1770 train_time:84563ms step_avg:97.31ms
step:870/1770 train_time:84663ms step_avg:97.31ms
step:871/1770 train_time:84763ms step_avg:97.32ms
step:872/1770 train_time:84863ms step_avg:97.32ms
step:873/1770 train_time:84962ms step_avg:97.32ms
step:874/1770 train_time:85062ms step_avg:97.33ms
step:875/1770 train_time:85162ms step_avg:97.33ms
step:875/1770 val_loss:3.5482 train_time:85262ms step_avg:97.44ms
step:876/1770 train_time:85281ms step_avg:97.35ms
step:877/1770 train_time:85369ms step_avg:97.34ms
step:878/1770 train_time:85472ms step_avg:97.35ms
step:879/1770 train_time:85573ms step_avg:97.35ms
step:880/1770 train_time:85674ms step_avg:97.36ms
step:881/1770 train_time:85774ms step_avg:97.36ms
step:882/1770 train_time:85875ms step_avg:97.36ms
step:883/1770 train_time:85975ms step_avg:97.37ms
step:884/1770 train_time:86074ms step_avg:97.37ms
step:885/1770 train_time:86175ms step_avg:97.37ms
step:886/1770 train_time:86278ms step_avg:97.38ms
step:887/1770 train_time:86380ms step_avg:97.38ms
step:888/1770 train_time:86481ms step_avg:97.39ms
step:889/1770 train_time:86582ms step_avg:97.39ms
step:890/1770 train_time:86682ms step_avg:97.40ms
step:891/1770 train_time:86782ms step_avg:97.40ms
step:892/1770 train_time:86881ms step_avg:97.40ms
step:893/1770 train_time:86981ms step_avg:97.40ms
step:894/1770 train_time:87080ms step_avg:97.41ms
step:895/1770 train_time:87180ms step_avg:97.41ms
step:896/1770 train_time:87280ms step_avg:97.41ms
step:897/1770 train_time:87380ms step_avg:97.41ms
step:898/1770 train_time:87480ms step_avg:97.42ms
step:899/1770 train_time:87581ms step_avg:97.42ms
step:900/1770 train_time:87681ms step_avg:97.42ms
step:901/1770 train_time:87781ms step_avg:97.43ms
step:902/1770 train_time:87881ms step_avg:97.43ms
step:903/1770 train_time:87980ms step_avg:97.43ms
step:904/1770 train_time:88080ms step_avg:97.43ms
step:905/1770 train_time:88179ms step_avg:97.44ms
step:906/1770 train_time:88279ms step_avg:97.44ms
step:907/1770 train_time:88378ms step_avg:97.44ms
step:908/1770 train_time:88479ms step_avg:97.44ms
step:909/1770 train_time:88579ms step_avg:97.45ms
step:910/1770 train_time:88679ms step_avg:97.45ms
step:911/1770 train_time:88779ms step_avg:97.45ms
step:912/1770 train_time:88879ms step_avg:97.45ms
step:913/1770 train_time:88978ms step_avg:97.46ms
step:914/1770 train_time:89078ms step_avg:97.46ms
step:915/1770 train_time:89178ms step_avg:97.46ms
step:916/1770 train_time:89278ms step_avg:97.46ms
step:917/1770 train_time:89378ms step_avg:97.47ms
step:918/1770 train_time:89478ms step_avg:97.47ms
step:919/1770 train_time:89578ms step_avg:97.47ms
step:920/1770 train_time:89680ms step_avg:97.48ms
step:921/1770 train_time:89782ms step_avg:97.48ms
step:922/1770 train_time:89883ms step_avg:97.49ms
step:923/1770 train_time:89984ms step_avg:97.49ms
step:924/1770 train_time:90086ms step_avg:97.50ms
step:925/1770 train_time:90186ms step_avg:97.50ms
step:926/1770 train_time:90288ms step_avg:97.50ms
step:927/1770 train_time:90390ms step_avg:97.51ms
step:928/1770 train_time:90493ms step_avg:97.51ms
step:929/1770 train_time:90595ms step_avg:97.52ms
step:930/1770 train_time:90698ms step_avg:97.52ms
step:931/1770 train_time:90800ms step_avg:97.53ms
step:932/1770 train_time:90902ms step_avg:97.53ms
step:933/1770 train_time:91003ms step_avg:97.54ms
step:934/1770 train_time:91105ms step_avg:97.54ms
step:935/1770 train_time:91206ms step_avg:97.55ms
step:936/1770 train_time:91307ms step_avg:97.55ms
step:937/1770 train_time:91409ms step_avg:97.55ms
step:938/1770 train_time:91514ms step_avg:97.56ms
step:939/1770 train_time:91616ms step_avg:97.57ms
step:940/1770 train_time:91718ms step_avg:97.57ms
step:941/1770 train_time:91821ms step_avg:97.58ms
step:942/1770 train_time:91922ms step_avg:97.58ms
step:943/1770 train_time:92023ms step_avg:97.59ms
step:944/1770 train_time:92124ms step_avg:97.59ms
step:945/1770 train_time:92225ms step_avg:97.59ms
step:946/1770 train_time:92327ms step_avg:97.60ms
step:947/1770 train_time:92428ms step_avg:97.60ms
step:948/1770 train_time:92531ms step_avg:97.61ms
step:949/1770 train_time:92635ms step_avg:97.61ms
step:950/1770 train_time:92739ms step_avg:97.62ms
step:951/1770 train_time:92841ms step_avg:97.62ms
step:952/1770 train_time:92942ms step_avg:97.63ms
step:953/1770 train_time:93043ms step_avg:97.63ms
step:954/1770 train_time:93144ms step_avg:97.64ms
step:955/1770 train_time:93245ms step_avg:97.64ms
step:956/1770 train_time:93346ms step_avg:97.64ms
step:957/1770 train_time:93447ms step_avg:97.65ms
step:958/1770 train_time:93549ms step_avg:97.65ms
step:959/1770 train_time:93651ms step_avg:97.66ms
step:960/1770 train_time:93754ms step_avg:97.66ms
step:961/1770 train_time:93857ms step_avg:97.67ms
step:962/1770 train_time:93959ms step_avg:97.67ms
step:963/1770 train_time:94061ms step_avg:97.67ms
step:964/1770 train_time:94162ms step_avg:97.68ms
step:965/1770 train_time:94263ms step_avg:97.68ms
step:966/1770 train_time:94364ms step_avg:97.69ms
step:967/1770 train_time:94466ms step_avg:97.69ms
step:968/1770 train_time:94568ms step_avg:97.69ms
step:969/1770 train_time:94670ms step_avg:97.70ms
step:970/1770 train_time:94772ms step_avg:97.70ms
step:971/1770 train_time:94874ms step_avg:97.71ms
step:972/1770 train_time:94978ms step_avg:97.71ms
step:973/1770 train_time:95079ms step_avg:97.72ms
step:974/1770 train_time:95181ms step_avg:97.72ms
step:975/1770 train_time:95283ms step_avg:97.73ms
step:976/1770 train_time:95384ms step_avg:97.73ms
step:977/1770 train_time:95485ms step_avg:97.73ms
step:978/1770 train_time:95586ms step_avg:97.74ms
step:979/1770 train_time:95689ms step_avg:97.74ms
step:980/1770 train_time:95792ms step_avg:97.75ms
step:981/1770 train_time:95895ms step_avg:97.75ms
step:982/1770 train_time:95997ms step_avg:97.76ms
step:983/1770 train_time:96100ms step_avg:97.76ms
step:984/1770 train_time:96202ms step_avg:97.77ms
step:985/1770 train_time:96303ms step_avg:97.77ms
step:986/1770 train_time:96405ms step_avg:97.77ms
step:987/1770 train_time:96506ms step_avg:97.78ms
step:988/1770 train_time:96607ms step_avg:97.78ms
step:989/1770 train_time:96709ms step_avg:97.78ms
step:990/1770 train_time:96810ms step_avg:97.79ms
step:991/1770 train_time:96914ms step_avg:97.79ms
step:992/1770 train_time:97017ms step_avg:97.80ms
step:993/1770 train_time:97120ms step_avg:97.80ms
step:994/1770 train_time:97222ms step_avg:97.81ms
step:995/1770 train_time:97323ms step_avg:97.81ms
step:996/1770 train_time:97424ms step_avg:97.82ms
step:997/1770 train_time:97526ms step_avg:97.82ms
step:998/1770 train_time:97627ms step_avg:97.82ms
step:999/1770 train_time:97728ms step_avg:97.83ms
step:1000/1770 train_time:97831ms step_avg:97.83ms
step:1000/1770 val_loss:3.5107 train_time:97932ms step_avg:97.93ms
step:1001/1770 train_time:97950ms step_avg:97.85ms
step:1002/1770 train_time:98044ms step_avg:97.85ms
step:1003/1770 train_time:98147ms step_avg:97.85ms
step:1004/1770 train_time:98249ms step_avg:97.86ms
step:1005/1770 train_time:98350ms step_avg:97.86ms
step:1006/1770 train_time:98450ms step_avg:97.86ms
step:1007/1770 train_time:98551ms step_avg:97.87ms
step:1008/1770 train_time:98652ms step_avg:97.87ms
step:1009/1770 train_time:98753ms step_avg:97.87ms
step:1010/1770 train_time:98855ms step_avg:97.88ms
step:1011/1770 train_time:98959ms step_avg:97.88ms
step:1012/1770 train_time:99063ms step_avg:97.89ms
step:1013/1770 train_time:99165ms step_avg:97.89ms
step:1014/1770 train_time:99267ms step_avg:97.90ms
step:1015/1770 train_time:99368ms step_avg:97.90ms
step:1016/1770 train_time:99469ms step_avg:97.90ms
step:1017/1770 train_time:99570ms step_avg:97.91ms
step:1018/1770 train_time:99671ms step_avg:97.91ms
step:1019/1770 train_time:99772ms step_avg:97.91ms
step:1020/1770 train_time:99874ms step_avg:97.92ms
step:1021/1770 train_time:99977ms step_avg:97.92ms
step:1022/1770 train_time:100081ms step_avg:97.93ms
step:1023/1770 train_time:100183ms step_avg:97.93ms
step:1024/1770 train_time:100286ms step_avg:97.94ms
step:1025/1770 train_time:100387ms step_avg:97.94ms
step:1026/1770 train_time:100489ms step_avg:97.94ms
step:1027/1770 train_time:100590ms step_avg:97.95ms
step:1028/1770 train_time:100691ms step_avg:97.95ms
step:1029/1770 train_time:100793ms step_avg:97.95ms
step:1030/1770 train_time:100894ms step_avg:97.96ms
step:1031/1770 train_time:100997ms step_avg:97.96ms
step:1032/1770 train_time:101100ms step_avg:97.96ms
step:1033/1770 train_time:101203ms step_avg:97.97ms
step:1034/1770 train_time:101305ms step_avg:97.97ms
step:1035/1770 train_time:101407ms step_avg:97.98ms
step:1036/1770 train_time:101507ms step_avg:97.98ms
step:1037/1770 train_time:101608ms step_avg:97.98ms
step:1038/1770 train_time:101709ms step_avg:97.99ms
step:1039/1770 train_time:101810ms step_avg:97.99ms
step:1040/1770 train_time:101911ms step_avg:97.99ms
step:1041/1770 train_time:102014ms step_avg:98.00ms
step:1042/1770 train_time:102117ms step_avg:98.00ms
step:1043/1770 train_time:102220ms step_avg:98.01ms
step:1044/1770 train_time:102323ms step_avg:98.01ms
step:1045/1770 train_time:102424ms step_avg:98.01ms
step:1046/1770 train_time:102525ms step_avg:98.02ms
step:1047/1770 train_time:102626ms step_avg:98.02ms
step:1048/1770 train_time:102727ms step_avg:98.02ms
step:1049/1770 train_time:102828ms step_avg:98.02ms
step:1050/1770 train_time:102930ms step_avg:98.03ms
step:1051/1770 train_time:103034ms step_avg:98.03ms
step:1052/1770 train_time:103137ms step_avg:98.04ms
step:1053/1770 train_time:103240ms step_avg:98.04ms
step:1054/1770 train_time:103342ms step_avg:98.05ms
step:1055/1770 train_time:103444ms step_avg:98.05ms
step:1056/1770 train_time:103546ms step_avg:98.05ms
step:1057/1770 train_time:103647ms step_avg:98.06ms
step:1058/1770 train_time:103748ms step_avg:98.06ms
step:1059/1770 train_time:103850ms step_avg:98.06ms
step:1060/1770 train_time:103951ms step_avg:98.07ms
step:1061/1770 train_time:104054ms step_avg:98.07ms
step:1062/1770 train_time:104157ms step_avg:98.08ms
step:1063/1770 train_time:104261ms step_avg:98.08ms
step:1064/1770 train_time:104363ms step_avg:98.09ms
step:1065/1770 train_time:104464ms step_avg:98.09ms
step:1066/1770 train_time:104566ms step_avg:98.09ms
step:1067/1770 train_time:104668ms step_avg:98.10ms
step:1068/1770 train_time:104770ms step_avg:98.10ms
step:1069/1770 train_time:104871ms step_avg:98.10ms
step:1070/1770 train_time:104973ms step_avg:98.11ms
step:1071/1770 train_time:105075ms step_avg:98.11ms
step:1072/1770 train_time:105177ms step_avg:98.11ms
step:1073/1770 train_time:105281ms step_avg:98.12ms
step:1074/1770 train_time:105384ms step_avg:98.12ms
step:1075/1770 train_time:105486ms step_avg:98.13ms
step:1076/1770 train_time:105588ms step_avg:98.13ms
step:1077/1770 train_time:105690ms step_avg:98.13ms
step:1078/1770 train_time:105792ms step_avg:98.14ms
step:1079/1770 train_time:105893ms step_avg:98.14ms
step:1080/1770 train_time:105995ms step_avg:98.14ms
step:1081/1770 train_time:106096ms step_avg:98.15ms
step:1082/1770 train_time:106199ms step_avg:98.15ms
step:1083/1770 train_time:106304ms step_avg:98.16ms
step:1084/1770 train_time:106406ms step_avg:98.16ms
step:1085/1770 train_time:106508ms step_avg:98.16ms
step:1086/1770 train_time:106609ms step_avg:98.17ms
step:1087/1770 train_time:106711ms step_avg:98.17ms
step:1088/1770 train_time:106812ms step_avg:98.17ms
step:1089/1770 train_time:106914ms step_avg:98.18ms
step:1090/1770 train_time:107018ms step_avg:98.18ms
step:1091/1770 train_time:107119ms step_avg:98.18ms
step:1092/1770 train_time:107222ms step_avg:98.19ms
step:1093/1770 train_time:107324ms step_avg:98.19ms
step:1094/1770 train_time:107426ms step_avg:98.20ms
step:1095/1770 train_time:107529ms step_avg:98.20ms
step:1096/1770 train_time:107631ms step_avg:98.20ms
step:1097/1770 train_time:107732ms step_avg:98.21ms
step:1098/1770 train_time:107833ms step_avg:98.21ms
step:1099/1770 train_time:107935ms step_avg:98.21ms
step:1100/1770 train_time:108038ms step_avg:98.22ms
step:1101/1770 train_time:108140ms step_avg:98.22ms
step:1102/1770 train_time:108243ms step_avg:98.22ms
step:1103/1770 train_time:108346ms step_avg:98.23ms
step:1104/1770 train_time:108448ms step_avg:98.23ms
step:1105/1770 train_time:108549ms step_avg:98.23ms
step:1106/1770 train_time:108651ms step_avg:98.24ms
step:1107/1770 train_time:108751ms step_avg:98.24ms
step:1108/1770 train_time:108853ms step_avg:98.24ms
step:1109/1770 train_time:108956ms step_avg:98.25ms
step:1110/1770 train_time:109059ms step_avg:98.25ms
step:1111/1770 train_time:109162ms step_avg:98.26ms
step:1112/1770 train_time:109264ms step_avg:98.26ms
step:1113/1770 train_time:109365ms step_avg:98.26ms
step:1114/1770 train_time:109467ms step_avg:98.26ms
step:1115/1770 train_time:109569ms step_avg:98.27ms
step:1116/1770 train_time:109670ms step_avg:98.27ms
step:1117/1770 train_time:109771ms step_avg:98.27ms
step:1118/1770 train_time:109873ms step_avg:98.28ms
step:1119/1770 train_time:109976ms step_avg:98.28ms
step:1120/1770 train_time:110079ms step_avg:98.28ms
step:1121/1770 train_time:110182ms step_avg:98.29ms
step:1122/1770 train_time:110284ms step_avg:98.29ms
step:1123/1770 train_time:110386ms step_avg:98.30ms
step:1124/1770 train_time:110489ms step_avg:98.30ms
step:1125/1770 train_time:110591ms step_avg:98.30ms
step:1125/1770 val_loss:3.4703 train_time:110692ms step_avg:98.39ms
step:1126/1770 train_time:110709ms step_avg:98.32ms
step:1127/1770 train_time:110800ms step_avg:98.31ms
step:1128/1770 train_time:110904ms step_avg:98.32ms
step:1129/1770 train_time:111005ms step_avg:98.32ms
step:1130/1770 train_time:111106ms step_avg:98.32ms
step:1131/1770 train_time:111208ms step_avg:98.33ms
step:1132/1770 train_time:111309ms step_avg:98.33ms
step:1133/1770 train_time:111410ms step_avg:98.33ms
step:1134/1770 train_time:111511ms step_avg:98.33ms
step:1135/1770 train_time:111612ms step_avg:98.34ms
step:1136/1770 train_time:111715ms step_avg:98.34ms
step:1137/1770 train_time:111819ms step_avg:98.35ms
step:1138/1770 train_time:111922ms step_avg:98.35ms
step:1139/1770 train_time:112024ms step_avg:98.35ms
step:1140/1770 train_time:112127ms step_avg:98.36ms
step:1141/1770 train_time:112228ms step_avg:98.36ms
step:1142/1770 train_time:112329ms step_avg:98.36ms
step:1143/1770 train_time:112429ms step_avg:98.36ms
step:1144/1770 train_time:112531ms step_avg:98.37ms
step:1145/1770 train_time:112632ms step_avg:98.37ms
step:1146/1770 train_time:112733ms step_avg:98.37ms
step:1147/1770 train_time:112836ms step_avg:98.37ms
step:1148/1770 train_time:112939ms step_avg:98.38ms
step:1149/1770 train_time:113041ms step_avg:98.38ms
step:1150/1770 train_time:113144ms step_avg:98.39ms
step:1151/1770 train_time:113246ms step_avg:98.39ms
step:1152/1770 train_time:113348ms step_avg:98.39ms
step:1153/1770 train_time:113449ms step_avg:98.39ms
step:1154/1770 train_time:113551ms step_avg:98.40ms
step:1155/1770 train_time:113652ms step_avg:98.40ms
step:1156/1770 train_time:113753ms step_avg:98.40ms
step:1157/1770 train_time:113856ms step_avg:98.41ms
step:1158/1770 train_time:113959ms step_avg:98.41ms
step:1159/1770 train_time:114062ms step_avg:98.41ms
step:1160/1770 train_time:114165ms step_avg:98.42ms
step:1161/1770 train_time:114268ms step_avg:98.42ms
step:1162/1770 train_time:114370ms step_avg:98.43ms
step:1163/1770 train_time:114472ms step_avg:98.43ms
step:1164/1770 train_time:114574ms step_avg:98.43ms
step:1165/1770 train_time:114675ms step_avg:98.43ms
step:1166/1770 train_time:114776ms step_avg:98.44ms
step:1167/1770 train_time:114878ms step_avg:98.44ms
step:1168/1770 train_time:114981ms step_avg:98.44ms
step:1169/1770 train_time:115084ms step_avg:98.45ms
step:1170/1770 train_time:115187ms step_avg:98.45ms
step:1171/1770 train_time:115289ms step_avg:98.45ms
step:1172/1770 train_time:115391ms step_avg:98.46ms
step:1173/1770 train_time:115493ms step_avg:98.46ms
step:1174/1770 train_time:115594ms step_avg:98.46ms
step:1175/1770 train_time:115696ms step_avg:98.46ms
step:1176/1770 train_time:115797ms step_avg:98.47ms
step:1177/1770 train_time:115899ms step_avg:98.47ms
step:1178/1770 train_time:116002ms step_avg:98.47ms
step:1179/1770 train_time:116105ms step_avg:98.48ms
step:1180/1770 train_time:116208ms step_avg:98.48ms
step:1181/1770 train_time:116311ms step_avg:98.49ms
step:1182/1770 train_time:116413ms step_avg:98.49ms
step:1183/1770 train_time:116516ms step_avg:98.49ms
step:1184/1770 train_time:116621ms step_avg:98.50ms
step:1185/1770 train_time:116724ms step_avg:98.50ms
step:1186/1770 train_time:116826ms step_avg:98.50ms
step:1187/1770 train_time:116931ms step_avg:98.51ms
step:1188/1770 train_time:117033ms step_avg:98.51ms
step:1189/1770 train_time:117136ms step_avg:98.52ms
step:1190/1770 train_time:117240ms step_avg:98.52ms
step:1191/1770 train_time:117344ms step_avg:98.53ms
step:1192/1770 train_time:117448ms step_avg:98.53ms
step:1193/1770 train_time:117550ms step_avg:98.53ms
step:1194/1770 train_time:117652ms step_avg:98.54ms
step:1195/1770 train_time:117755ms step_avg:98.54ms
step:1196/1770 train_time:117860ms step_avg:98.54ms
step:1197/1770 train_time:117963ms step_avg:98.55ms
step:1198/1770 train_time:118066ms step_avg:98.55ms
step:1199/1770 train_time:118169ms step_avg:98.56ms
step:1200/1770 train_time:118272ms step_avg:98.56ms
step:1201/1770 train_time:118376ms step_avg:98.56ms
step:1202/1770 train_time:118478ms step_avg:98.57ms
step:1203/1770 train_time:118583ms step_avg:98.57ms
step:1204/1770 train_time:118688ms step_avg:98.58ms
step:1205/1770 train_time:118790ms step_avg:98.58ms
step:1206/1770 train_time:118894ms step_avg:98.59ms
step:1207/1770 train_time:118997ms step_avg:98.59ms
step:1208/1770 train_time:119100ms step_avg:98.59ms
step:1209/1770 train_time:119204ms step_avg:98.60ms
step:1210/1770 train_time:119308ms step_avg:98.60ms
step:1211/1770 train_time:119411ms step_avg:98.60ms
step:1212/1770 train_time:119515ms step_avg:98.61ms
step:1213/1770 train_time:119619ms step_avg:98.61ms
step:1214/1770 train_time:119723ms step_avg:98.62ms
step:1215/1770 train_time:119826ms step_avg:98.62ms
step:1216/1770 train_time:119931ms step_avg:98.63ms
step:1217/1770 train_time:120033ms step_avg:98.63ms
step:1218/1770 train_time:120137ms step_avg:98.63ms
step:1219/1770 train_time:120241ms step_avg:98.64ms
step:1220/1770 train_time:120345ms step_avg:98.64ms
step:1221/1770 train_time:120448ms step_avg:98.65ms
step:1222/1770 train_time:120553ms step_avg:98.65ms
step:1223/1770 train_time:120655ms step_avg:98.66ms
step:1224/1770 train_time:120758ms step_avg:98.66ms
step:1225/1770 train_time:120864ms step_avg:98.66ms
step:1226/1770 train_time:120967ms step_avg:98.67ms
step:1227/1770 train_time:121071ms step_avg:98.67ms
step:1228/1770 train_time:121174ms step_avg:98.68ms
step:1229/1770 train_time:121277ms step_avg:98.68ms
step:1230/1770 train_time:121381ms step_avg:98.68ms
step:1231/1770 train_time:121486ms step_avg:98.69ms
step:1232/1770 train_time:121588ms step_avg:98.69ms
step:1233/1770 train_time:121691ms step_avg:98.69ms
step:1234/1770 train_time:121794ms step_avg:98.70ms
step:1235/1770 train_time:121897ms step_avg:98.70ms
step:1236/1770 train_time:122001ms step_avg:98.71ms
step:1237/1770 train_time:122105ms step_avg:98.71ms
step:1238/1770 train_time:122208ms step_avg:98.71ms
step:1239/1770 train_time:122311ms step_avg:98.72ms
step:1240/1770 train_time:122414ms step_avg:98.72ms
step:1241/1770 train_time:122518ms step_avg:98.73ms
step:1242/1770 train_time:122622ms step_avg:98.73ms
step:1243/1770 train_time:122727ms step_avg:98.73ms
step:1244/1770 train_time:122829ms step_avg:98.74ms
step:1245/1770 train_time:122932ms step_avg:98.74ms
step:1246/1770 train_time:123035ms step_avg:98.74ms
step:1247/1770 train_time:123138ms step_avg:98.75ms
step:1248/1770 train_time:123244ms step_avg:98.75ms
step:1249/1770 train_time:123348ms step_avg:98.76ms
step:1250/1770 train_time:123451ms step_avg:98.76ms
step:1250/1770 val_loss:3.4232 train_time:123555ms step_avg:98.84ms
step:1251/1770 train_time:123572ms step_avg:98.78ms
step:1252/1770 train_time:123664ms step_avg:98.77ms
step:1253/1770 train_time:123768ms step_avg:98.78ms
step:1254/1770 train_time:123873ms step_avg:98.78ms
step:1255/1770 train_time:123976ms step_avg:98.79ms
step:1256/1770 train_time:124078ms step_avg:98.79ms
step:1257/1770 train_time:124180ms step_avg:98.79ms
step:1258/1770 train_time:124282ms step_avg:98.79ms
step:1259/1770 train_time:124384ms step_avg:98.80ms
step:1260/1770 train_time:124487ms step_avg:98.80ms
step:1261/1770 train_time:124593ms step_avg:98.81ms
step:1262/1770 train_time:124697ms step_avg:98.81ms
step:1263/1770 train_time:124801ms step_avg:98.81ms
step:1264/1770 train_time:124905ms step_avg:98.82ms
step:1265/1770 train_time:125008ms step_avg:98.82ms
step:1266/1770 train_time:125113ms step_avg:98.83ms
step:1267/1770 train_time:125217ms step_avg:98.83ms
step:1268/1770 train_time:125320ms step_avg:98.83ms
step:1269/1770 train_time:125422ms step_avg:98.84ms
step:1270/1770 train_time:125525ms step_avg:98.84ms
step:1271/1770 train_time:125630ms step_avg:98.84ms
step:1272/1770 train_time:125733ms step_avg:98.85ms
step:1273/1770 train_time:125838ms step_avg:98.85ms
step:1274/1770 train_time:125942ms step_avg:98.86ms
step:1275/1770 train_time:126045ms step_avg:98.86ms
step:1276/1770 train_time:126149ms step_avg:98.86ms
step:1277/1770 train_time:126253ms step_avg:98.87ms
step:1278/1770 train_time:126357ms step_avg:98.87ms
step:1279/1770 train_time:126459ms step_avg:98.87ms
step:1280/1770 train_time:126563ms step_avg:98.88ms
step:1281/1770 train_time:126666ms step_avg:98.88ms
step:1282/1770 train_time:126771ms step_avg:98.89ms
step:1283/1770 train_time:126875ms step_avg:98.89ms
step:1284/1770 train_time:126978ms step_avg:98.89ms
step:1285/1770 train_time:127081ms step_avg:98.90ms
step:1286/1770 train_time:127185ms step_avg:98.90ms
step:1287/1770 train_time:127290ms step_avg:98.90ms
step:1288/1770 train_time:127394ms step_avg:98.91ms
step:1289/1770 train_time:127497ms step_avg:98.91ms
step:1290/1770 train_time:127599ms step_avg:98.91ms
step:1291/1770 train_time:127702ms step_avg:98.92ms
step:1292/1770 train_time:127805ms step_avg:98.92ms
step:1293/1770 train_time:127909ms step_avg:98.92ms
step:1294/1770 train_time:128013ms step_avg:98.93ms
step:1295/1770 train_time:128117ms step_avg:98.93ms
step:1296/1770 train_time:128221ms step_avg:98.94ms
step:1297/1770 train_time:128324ms step_avg:98.94ms
step:1298/1770 train_time:128428ms step_avg:98.94ms
step:1299/1770 train_time:128533ms step_avg:98.95ms
step:1300/1770 train_time:128637ms step_avg:98.95ms
step:1301/1770 train_time:128740ms step_avg:98.95ms
step:1302/1770 train_time:128843ms step_avg:98.96ms
step:1303/1770 train_time:128946ms step_avg:98.96ms
step:1304/1770 train_time:129050ms step_avg:98.96ms
step:1305/1770 train_time:129154ms step_avg:98.97ms
step:1306/1770 train_time:129258ms step_avg:98.97ms
step:1307/1770 train_time:129361ms step_avg:98.98ms
step:1308/1770 train_time:129465ms step_avg:98.98ms
step:1309/1770 train_time:129569ms step_avg:98.98ms
step:1310/1770 train_time:129674ms step_avg:98.99ms
step:1311/1770 train_time:129778ms step_avg:98.99ms
step:1312/1770 train_time:129880ms step_avg:98.99ms
step:1313/1770 train_time:129982ms step_avg:99.00ms
step:1314/1770 train_time:130085ms step_avg:99.00ms
step:1315/1770 train_time:130190ms step_avg:99.00ms
step:1316/1770 train_time:130294ms step_avg:99.01ms
step:1317/1770 train_time:130397ms step_avg:99.01ms
step:1318/1770 train_time:130502ms step_avg:99.01ms
step:1319/1770 train_time:130606ms step_avg:99.02ms
step:1320/1770 train_time:130711ms step_avg:99.02ms
step:1321/1770 train_time:130815ms step_avg:99.03ms
step:1322/1770 train_time:130918ms step_avg:99.03ms
step:1323/1770 train_time:131021ms step_avg:99.03ms
step:1324/1770 train_time:131126ms step_avg:99.04ms
step:1325/1770 train_time:131230ms step_avg:99.04ms
step:1326/1770 train_time:131333ms step_avg:99.04ms
step:1327/1770 train_time:131438ms step_avg:99.05ms
step:1328/1770 train_time:131541ms step_avg:99.05ms
step:1329/1770 train_time:131645ms step_avg:99.06ms
step:1330/1770 train_time:131749ms step_avg:99.06ms
step:1331/1770 train_time:131852ms step_avg:99.06ms
step:1332/1770 train_time:131956ms step_avg:99.07ms
step:1333/1770 train_time:132059ms step_avg:99.07ms
step:1334/1770 train_time:132161ms step_avg:99.07ms
step:1335/1770 train_time:132264ms step_avg:99.07ms
step:1336/1770 train_time:132368ms step_avg:99.08ms
step:1337/1770 train_time:132472ms step_avg:99.08ms
step:1338/1770 train_time:132576ms step_avg:99.09ms
step:1339/1770 train_time:132679ms step_avg:99.09ms
step:1340/1770 train_time:132784ms step_avg:99.09ms
step:1341/1770 train_time:132888ms step_avg:99.10ms
step:1342/1770 train_time:132993ms step_avg:99.10ms
step:1343/1770 train_time:133096ms step_avg:99.10ms
step:1344/1770 train_time:133199ms step_avg:99.11ms
step:1345/1770 train_time:133301ms step_avg:99.11ms
step:1346/1770 train_time:133405ms step_avg:99.11ms
step:1347/1770 train_time:133509ms step_avg:99.12ms
step:1348/1770 train_time:133615ms step_avg:99.12ms
step:1349/1770 train_time:133718ms step_avg:99.12ms
step:1350/1770 train_time:133821ms step_avg:99.13ms
step:1351/1770 train_time:133925ms step_avg:99.13ms
step:1352/1770 train_time:134029ms step_avg:99.13ms
step:1353/1770 train_time:134134ms step_avg:99.14ms
step:1354/1770 train_time:134237ms step_avg:99.14ms
step:1355/1770 train_time:134340ms step_avg:99.14ms
step:1356/1770 train_time:134442ms step_avg:99.15ms
step:1357/1770 train_time:134546ms step_avg:99.15ms
step:1358/1770 train_time:134649ms step_avg:99.15ms
step:1359/1770 train_time:134753ms step_avg:99.16ms
step:1360/1770 train_time:134857ms step_avg:99.16ms
step:1361/1770 train_time:134960ms step_avg:99.16ms
step:1362/1770 train_time:135065ms step_avg:99.17ms
step:1363/1770 train_time:135171ms step_avg:99.17ms
step:1364/1770 train_time:135274ms step_avg:99.17ms
step:1365/1770 train_time:135377ms step_avg:99.18ms
step:1366/1770 train_time:135479ms step_avg:99.18ms
step:1367/1770 train_time:135584ms step_avg:99.18ms
step:1368/1770 train_time:135686ms step_avg:99.19ms
step:1369/1770 train_time:135792ms step_avg:99.19ms
step:1370/1770 train_time:135894ms step_avg:99.19ms
step:1371/1770 train_time:135998ms step_avg:99.20ms
step:1372/1770 train_time:136101ms step_avg:99.20ms
step:1373/1770 train_time:136206ms step_avg:99.20ms
step:1374/1770 train_time:136311ms step_avg:99.21ms
step:1375/1770 train_time:136415ms step_avg:99.21ms
step:1375/1770 val_loss:3.3805 train_time:136518ms step_avg:99.29ms
step:1376/1770 train_time:136535ms step_avg:99.23ms
step:1377/1770 train_time:136630ms step_avg:99.22ms
step:1378/1770 train_time:136732ms step_avg:99.22ms
step:1379/1770 train_time:136835ms step_avg:99.23ms
step:1380/1770 train_time:136938ms step_avg:99.23ms
step:1381/1770 train_time:137042ms step_avg:99.23ms
step:1382/1770 train_time:137145ms step_avg:99.24ms
step:1383/1770 train_time:137248ms step_avg:99.24ms
step:1384/1770 train_time:137350ms step_avg:99.24ms
step:1385/1770 train_time:137455ms step_avg:99.25ms
step:1386/1770 train_time:137562ms step_avg:99.25ms
step:1387/1770 train_time:137666ms step_avg:99.25ms
step:1388/1770 train_time:137769ms step_avg:99.26ms
step:1389/1770 train_time:137872ms step_avg:99.26ms
step:1390/1770 train_time:137975ms step_avg:99.26ms
step:1391/1770 train_time:138079ms step_avg:99.27ms
step:1392/1770 train_time:138182ms step_avg:99.27ms
step:1393/1770 train_time:138285ms step_avg:99.27ms
step:1394/1770 train_time:138388ms step_avg:99.27ms
step:1395/1770 train_time:138492ms step_avg:99.28ms
step:1396/1770 train_time:138597ms step_avg:99.28ms
step:1397/1770 train_time:138702ms step_avg:99.29ms
step:1398/1770 train_time:138806ms step_avg:99.29ms
step:1399/1770 train_time:138908ms step_avg:99.29ms
step:1400/1770 train_time:139011ms step_avg:99.29ms
step:1401/1770 train_time:139115ms step_avg:99.30ms
step:1402/1770 train_time:139219ms step_avg:99.30ms
step:1403/1770 train_time:139323ms step_avg:99.30ms
step:1404/1770 train_time:139427ms step_avg:99.31ms
step:1405/1770 train_time:139529ms step_avg:99.31ms
step:1406/1770 train_time:139632ms step_avg:99.31ms
step:1407/1770 train_time:139736ms step_avg:99.32ms
step:1408/1770 train_time:139840ms step_avg:99.32ms
step:1409/1770 train_time:139944ms step_avg:99.32ms
step:1410/1770 train_time:140047ms step_avg:99.32ms
step:1411/1770 train_time:140149ms step_avg:99.33ms
step:1412/1770 train_time:140253ms step_avg:99.33ms
step:1413/1770 train_time:140356ms step_avg:99.33ms
step:1414/1770 train_time:140461ms step_avg:99.34ms
step:1415/1770 train_time:140564ms step_avg:99.34ms
step:1416/1770 train_time:140668ms step_avg:99.34ms
step:1417/1770 train_time:140770ms step_avg:99.34ms
step:1418/1770 train_time:140874ms step_avg:99.35ms
step:1419/1770 train_time:140978ms step_avg:99.35ms
step:1420/1770 train_time:141081ms step_avg:99.35ms
step:1421/1770 train_time:141185ms step_avg:99.36ms
step:1422/1770 train_time:141287ms step_avg:99.36ms
step:1423/1770 train_time:141391ms step_avg:99.36ms
step:1424/1770 train_time:141495ms step_avg:99.36ms
step:1425/1770 train_time:141600ms step_avg:99.37ms
step:1426/1770 train_time:141704ms step_avg:99.37ms
step:1427/1770 train_time:141807ms step_avg:99.37ms
step:1428/1770 train_time:141911ms step_avg:99.38ms
step:1429/1770 train_time:142015ms step_avg:99.38ms
step:1430/1770 train_time:142118ms step_avg:99.38ms
step:1431/1770 train_time:142224ms step_avg:99.39ms
step:1432/1770 train_time:142326ms step_avg:99.39ms
step:1433/1770 train_time:142429ms step_avg:99.39ms
step:1434/1770 train_time:142532ms step_avg:99.39ms
step:1435/1770 train_time:142637ms step_avg:99.40ms
step:1436/1770 train_time:142742ms step_avg:99.40ms
step:1437/1770 train_time:142846ms step_avg:99.41ms
step:1438/1770 train_time:142948ms step_avg:99.41ms
step:1439/1770 train_time:143052ms step_avg:99.41ms
step:1440/1770 train_time:143155ms step_avg:99.41ms
step:1441/1770 train_time:143260ms step_avg:99.42ms
step:1442/1770 train_time:143363ms step_avg:99.42ms
step:1443/1770 train_time:143465ms step_avg:99.42ms
step:1444/1770 train_time:143569ms step_avg:99.42ms
step:1445/1770 train_time:143674ms step_avg:99.43ms
step:1446/1770 train_time:143780ms step_avg:99.43ms
step:1447/1770 train_time:143884ms step_avg:99.44ms
step:1448/1770 train_time:143989ms step_avg:99.44ms
step:1449/1770 train_time:144094ms step_avg:99.44ms
step:1450/1770 train_time:144198ms step_avg:99.45ms
step:1451/1770 train_time:144302ms step_avg:99.45ms
step:1452/1770 train_time:144407ms step_avg:99.45ms
step:1453/1770 train_time:144511ms step_avg:99.46ms
step:1454/1770 train_time:144615ms step_avg:99.46ms
step:1455/1770 train_time:144722ms step_avg:99.46ms
step:1456/1770 train_time:144827ms step_avg:99.47ms
step:1457/1770 train_time:144933ms step_avg:99.47ms
step:1458/1770 train_time:145038ms step_avg:99.48ms
step:1459/1770 train_time:145143ms step_avg:99.48ms
step:1460/1770 train_time:145248ms step_avg:99.48ms
step:1461/1770 train_time:145353ms step_avg:99.49ms
step:1462/1770 train_time:145459ms step_avg:99.49ms
step:1463/1770 train_time:145563ms step_avg:99.50ms
step:1464/1770 train_time:145670ms step_avg:99.50ms
step:1465/1770 train_time:145774ms step_avg:99.50ms
step:1466/1770 train_time:145880ms step_avg:99.51ms
step:1467/1770 train_time:145986ms step_avg:99.51ms
step:1468/1770 train_time:146090ms step_avg:99.52ms
step:1469/1770 train_time:146195ms step_avg:99.52ms
step:1470/1770 train_time:146300ms step_avg:99.52ms
step:1471/1770 train_time:146404ms step_avg:99.53ms
step:1472/1770 train_time:146509ms step_avg:99.53ms
step:1473/1770 train_time:146614ms step_avg:99.53ms
step:1474/1770 train_time:146718ms step_avg:99.54ms
step:1475/1770 train_time:146822ms step_avg:99.54ms
step:1476/1770 train_time:146926ms step_avg:99.54ms
step:1477/1770 train_time:147032ms step_avg:99.55ms
step:1478/1770 train_time:147137ms step_avg:99.55ms
step:1479/1770 train_time:147241ms step_avg:99.55ms
step:1480/1770 train_time:147346ms step_avg:99.56ms
step:1481/1770 train_time:147453ms step_avg:99.56ms
step:1482/1770 train_time:147557ms step_avg:99.57ms
step:1483/1770 train_time:147662ms step_avg:99.57ms
step:1484/1770 train_time:147766ms step_avg:99.57ms
step:1485/1770 train_time:147869ms step_avg:99.58ms
step:1486/1770 train_time:147973ms step_avg:99.58ms
step:1487/1770 train_time:148078ms step_avg:99.58ms
step:1488/1770 train_time:148183ms step_avg:99.59ms
step:1489/1770 train_time:148289ms step_avg:99.59ms
step:1490/1770 train_time:148395ms step_avg:99.59ms
step:1491/1770 train_time:148499ms step_avg:99.60ms
step:1492/1770 train_time:148604ms step_avg:99.60ms
step:1493/1770 train_time:148711ms step_avg:99.61ms
step:1494/1770 train_time:148820ms step_avg:99.61ms
step:1495/1770 train_time:148924ms step_avg:99.61ms
step:1496/1770 train_time:149028ms step_avg:99.62ms
step:1497/1770 train_time:149133ms step_avg:99.62ms
step:1498/1770 train_time:149238ms step_avg:99.62ms
step:1499/1770 train_time:149343ms step_avg:99.63ms
step:1500/1770 train_time:149446ms step_avg:99.63ms
step:1500/1770 val_loss:3.3428 train_time:149548ms step_avg:99.70ms
step:1501/1770 train_time:149566ms step_avg:99.64ms
step:1502/1770 train_time:149658ms step_avg:99.64ms
step:1503/1770 train_time:149761ms step_avg:99.64ms
step:1504/1770 train_time:149865ms step_avg:99.64ms
step:1505/1770 train_time:149971ms step_avg:99.65ms
step:1506/1770 train_time:150076ms step_avg:99.65ms
step:1507/1770 train_time:150180ms step_avg:99.65ms
step:1508/1770 train_time:150284ms step_avg:99.66ms
step:1509/1770 train_time:150388ms step_avg:99.66ms
step:1510/1770 train_time:150493ms step_avg:99.66ms
step:1511/1770 train_time:150600ms step_avg:99.67ms
step:1512/1770 train_time:150704ms step_avg:99.67ms
step:1513/1770 train_time:150809ms step_avg:99.68ms
step:1514/1770 train_time:150915ms step_avg:99.68ms
step:1515/1770 train_time:151020ms step_avg:99.68ms
step:1516/1770 train_time:151124ms step_avg:99.69ms
step:1517/1770 train_time:151228ms step_avg:99.69ms
step:1518/1770 train_time:151333ms step_avg:99.69ms
step:1519/1770 train_time:151438ms step_avg:99.70ms
step:1520/1770 train_time:151543ms step_avg:99.70ms
step:1521/1770 train_time:151647ms step_avg:99.70ms
step:1522/1770 train_time:151752ms step_avg:99.71ms
step:1523/1770 train_time:151858ms step_avg:99.71ms
step:1524/1770 train_time:151964ms step_avg:99.71ms
step:1525/1770 train_time:152068ms step_avg:99.72ms
step:1526/1770 train_time:152173ms step_avg:99.72ms
step:1527/1770 train_time:152278ms step_avg:99.72ms
step:1528/1770 train_time:152383ms step_avg:99.73ms
step:1529/1770 train_time:152488ms step_avg:99.73ms
step:1530/1770 train_time:152593ms step_avg:99.73ms
step:1531/1770 train_time:152697ms step_avg:99.74ms
step:1532/1770 train_time:152802ms step_avg:99.74ms
step:1533/1770 train_time:152908ms step_avg:99.74ms
step:1534/1770 train_time:153013ms step_avg:99.75ms
step:1535/1770 train_time:153117ms step_avg:99.75ms
step:1536/1770 train_time:153221ms step_avg:99.75ms
step:1537/1770 train_time:153326ms step_avg:99.76ms
step:1538/1770 train_time:153432ms step_avg:99.76ms
step:1539/1770 train_time:153537ms step_avg:99.76ms
step:1540/1770 train_time:153644ms step_avg:99.77ms
step:1541/1770 train_time:153750ms step_avg:99.77ms
step:1542/1770 train_time:153855ms step_avg:99.78ms
step:1543/1770 train_time:153958ms step_avg:99.78ms
step:1544/1770 train_time:154066ms step_avg:99.78ms
step:1545/1770 train_time:154171ms step_avg:99.79ms
step:1546/1770 train_time:154276ms step_avg:99.79ms
step:1547/1770 train_time:154380ms step_avg:99.79ms
step:1548/1770 train_time:154484ms step_avg:99.80ms
step:1549/1770 train_time:154589ms step_avg:99.80ms
step:1550/1770 train_time:154695ms step_avg:99.80ms
step:1551/1770 train_time:154799ms step_avg:99.81ms
step:1552/1770 train_time:154905ms step_avg:99.81ms
step:1553/1770 train_time:155008ms step_avg:99.81ms
step:1554/1770 train_time:155113ms step_avg:99.82ms
step:1555/1770 train_time:155218ms step_avg:99.82ms
step:1556/1770 train_time:155322ms step_avg:99.82ms
step:1557/1770 train_time:155426ms step_avg:99.82ms
step:1558/1770 train_time:155531ms step_avg:99.83ms
step:1559/1770 train_time:155635ms step_avg:99.83ms
step:1560/1770 train_time:155740ms step_avg:99.83ms
step:1561/1770 train_time:155846ms step_avg:99.84ms
step:1562/1770 train_time:155951ms step_avg:99.84ms
step:1563/1770 train_time:156057ms step_avg:99.84ms
step:1564/1770 train_time:156160ms step_avg:99.85ms
step:1565/1770 train_time:156264ms step_avg:99.85ms
step:1566/1770 train_time:156368ms step_avg:99.85ms
step:1567/1770 train_time:156474ms step_avg:99.86ms
step:1568/1770 train_time:156579ms step_avg:99.86ms
step:1569/1770 train_time:156687ms step_avg:99.86ms
step:1570/1770 train_time:156792ms step_avg:99.87ms
step:1571/1770 train_time:156898ms step_avg:99.87ms
step:1572/1770 train_time:157002ms step_avg:99.87ms
step:1573/1770 train_time:157109ms step_avg:99.88ms
step:1574/1770 train_time:157214ms step_avg:99.88ms
step:1575/1770 train_time:157318ms step_avg:99.88ms
step:1576/1770 train_time:157423ms step_avg:99.89ms
step:1577/1770 train_time:157529ms step_avg:99.89ms
step:1578/1770 train_time:157635ms step_avg:99.90ms
step:1579/1770 train_time:157739ms step_avg:99.90ms
step:1580/1770 train_time:157844ms step_avg:99.90ms
step:1581/1770 train_time:157951ms step_avg:99.91ms
step:1582/1770 train_time:158058ms step_avg:99.91ms
step:1583/1770 train_time:158162ms step_avg:99.91ms
step:1584/1770 train_time:158267ms step_avg:99.92ms
step:1585/1770 train_time:158373ms step_avg:99.92ms
step:1586/1770 train_time:158481ms step_avg:99.93ms
step:1587/1770 train_time:158587ms step_avg:99.93ms
step:1588/1770 train_time:158693ms step_avg:99.93ms
step:1589/1770 train_time:158799ms step_avg:99.94ms
step:1590/1770 train_time:158902ms step_avg:99.94ms
step:1591/1770 train_time:159006ms step_avg:99.94ms
step:1592/1770 train_time:159113ms step_avg:99.95ms
step:1593/1770 train_time:159218ms step_avg:99.95ms
step:1594/1770 train_time:159322ms step_avg:99.95ms
step:1595/1770 train_time:159426ms step_avg:99.95ms
step:1596/1770 train_time:159533ms step_avg:99.96ms
step:1597/1770 train_time:159637ms step_avg:99.96ms
step:1598/1770 train_time:159742ms step_avg:99.96ms
step:1599/1770 train_time:159847ms step_avg:99.97ms
step:1600/1770 train_time:159954ms step_avg:99.97ms
step:1601/1770 train_time:160059ms step_avg:99.97ms
step:1602/1770 train_time:160165ms step_avg:99.98ms
step:1603/1770 train_time:160270ms step_avg:99.98ms
step:1604/1770 train_time:160374ms step_avg:99.98ms
step:1605/1770 train_time:160478ms step_avg:99.99ms
step:1606/1770 train_time:160583ms step_avg:99.99ms
step:1607/1770 train_time:160690ms step_avg:99.99ms
step:1608/1770 train_time:160795ms step_avg:100.00ms
step:1609/1770 train_time:160899ms step_avg:100.00ms
step:1610/1770 train_time:161004ms step_avg:100.00ms
step:1611/1770 train_time:161111ms step_avg:100.01ms
step:1612/1770 train_time:161216ms step_avg:100.01ms
step:1613/1770 train_time:161321ms step_avg:100.01ms
step:1614/1770 train_time:161425ms step_avg:100.02ms
step:1615/1770 train_time:161529ms step_avg:100.02ms
step:1616/1770 train_time:161634ms step_avg:100.02ms
step:1617/1770 train_time:161740ms step_avg:100.02ms
step:1618/1770 train_time:161847ms step_avg:100.03ms
step:1619/1770 train_time:161952ms step_avg:100.03ms
step:1620/1770 train_time:162058ms step_avg:100.04ms
step:1621/1770 train_time:162164ms step_avg:100.04ms
step:1622/1770 train_time:162270ms step_avg:100.04ms
step:1623/1770 train_time:162378ms step_avg:100.05ms
step:1624/1770 train_time:162482ms step_avg:100.05ms
step:1625/1770 train_time:162586ms step_avg:100.05ms
step:1625/1770 val_loss:3.3084 train_time:162691ms step_avg:100.12ms
step:1626/1770 train_time:162710ms step_avg:100.07ms
step:1627/1770 train_time:162802ms step_avg:100.06ms
step:1628/1770 train_time:162906ms step_avg:100.06ms
step:1629/1770 train_time:163010ms step_avg:100.07ms
step:1630/1770 train_time:163114ms step_avg:100.07ms
step:1631/1770 train_time:163218ms step_avg:100.07ms
step:1632/1770 train_time:163322ms step_avg:100.08ms
step:1633/1770 train_time:163426ms step_avg:100.08ms
step:1634/1770 train_time:163532ms step_avg:100.08ms
step:1635/1770 train_time:163637ms step_avg:100.08ms
step:1636/1770 train_time:163744ms step_avg:100.09ms
step:1637/1770 train_time:163850ms step_avg:100.09ms
step:1638/1770 train_time:163955ms step_avg:100.09ms
step:1639/1770 train_time:164060ms step_avg:100.10ms
step:1640/1770 train_time:164164ms step_avg:100.10ms
step:1641/1770 train_time:164270ms step_avg:100.10ms
step:1642/1770 train_time:164373ms step_avg:100.11ms
step:1643/1770 train_time:164478ms step_avg:100.11ms
step:1644/1770 train_time:164583ms step_avg:100.11ms
step:1645/1770 train_time:164688ms step_avg:100.11ms
step:1646/1770 train_time:164797ms step_avg:100.12ms
step:1647/1770 train_time:164902ms step_avg:100.12ms
step:1648/1770 train_time:165006ms step_avg:100.13ms
step:1649/1770 train_time:165111ms step_avg:100.13ms
step:1650/1770 train_time:165216ms step_avg:100.13ms
step:1651/1770 train_time:165319ms step_avg:100.13ms
step:1652/1770 train_time:165423ms step_avg:100.14ms
step:1653/1770 train_time:165528ms step_avg:100.14ms
step:1654/1770 train_time:165636ms step_avg:100.14ms
step:1655/1770 train_time:165744ms step_avg:100.15ms
step:1656/1770 train_time:165849ms step_avg:100.15ms
step:1657/1770 train_time:165954ms step_avg:100.15ms
step:1658/1770 train_time:166059ms step_avg:100.16ms
step:1659/1770 train_time:166166ms step_avg:100.16ms
step:1660/1770 train_time:166270ms step_avg:100.16ms
step:1661/1770 train_time:166375ms step_avg:100.17ms
step:1662/1770 train_time:166480ms step_avg:100.17ms
step:1663/1770 train_time:166585ms step_avg:100.17ms
step:1664/1770 train_time:166689ms step_avg:100.17ms
step:1665/1770 train_time:166794ms step_avg:100.18ms
step:1666/1770 train_time:166899ms step_avg:100.18ms
step:1667/1770 train_time:167004ms step_avg:100.18ms
step:1668/1770 train_time:167109ms step_avg:100.19ms
step:1669/1770 train_time:167213ms step_avg:100.19ms
step:1670/1770 train_time:167316ms step_avg:100.19ms
step:1671/1770 train_time:167421ms step_avg:100.19ms
step:1672/1770 train_time:167527ms step_avg:100.20ms
step:1673/1770 train_time:167633ms step_avg:100.20ms
step:1674/1770 train_time:167738ms step_avg:100.20ms
step:1675/1770 train_time:167843ms step_avg:100.20ms
step:1676/1770 train_time:167950ms step_avg:100.21ms
step:1677/1770 train_time:168058ms step_avg:100.21ms
step:1678/1770 train_time:168162ms step_avg:100.22ms
step:1679/1770 train_time:168267ms step_avg:100.22ms
step:1680/1770 train_time:168372ms step_avg:100.22ms
step:1681/1770 train_time:168476ms step_avg:100.22ms
step:1682/1770 train_time:168582ms step_avg:100.23ms
step:1683/1770 train_time:168686ms step_avg:100.23ms
step:1684/1770 train_time:168791ms step_avg:100.23ms
step:1685/1770 train_time:168896ms step_avg:100.23ms
step:1686/1770 train_time:169002ms step_avg:100.24ms
step:1687/1770 train_time:169109ms step_avg:100.24ms
step:1688/1770 train_time:169215ms step_avg:100.25ms
step:1689/1770 train_time:169319ms step_avg:100.25ms
step:1690/1770 train_time:169424ms step_avg:100.25ms
step:1691/1770 train_time:169529ms step_avg:100.25ms
step:1692/1770 train_time:169633ms step_avg:100.26ms
step:1693/1770 train_time:169739ms step_avg:100.26ms
step:1694/1770 train_time:169843ms step_avg:100.26ms
step:1695/1770 train_time:169950ms step_avg:100.27ms
step:1696/1770 train_time:170057ms step_avg:100.27ms
step:1697/1770 train_time:170163ms step_avg:100.27ms
step:1698/1770 train_time:170269ms step_avg:100.28ms
step:1699/1770 train_time:170373ms step_avg:100.28ms
step:1700/1770 train_time:170477ms step_avg:100.28ms
step:1701/1770 train_time:170582ms step_avg:100.28ms
step:1702/1770 train_time:170686ms step_avg:100.29ms
step:1703/1770 train_time:170791ms step_avg:100.29ms
step:1704/1770 train_time:170896ms step_avg:100.29ms
step:1705/1770 train_time:171000ms step_avg:100.29ms
step:1706/1770 train_time:171104ms step_avg:100.30ms
step:1707/1770 train_time:171211ms step_avg:100.30ms
step:1708/1770 train_time:171316ms step_avg:100.30ms
step:1709/1770 train_time:171422ms step_avg:100.31ms
step:1710/1770 train_time:171531ms step_avg:100.31ms
step:1711/1770 train_time:171638ms step_avg:100.31ms
step:1712/1770 train_time:171743ms step_avg:100.32ms
step:1713/1770 train_time:171848ms step_avg:100.32ms
step:1714/1770 train_time:171954ms step_avg:100.32ms
step:1715/1770 train_time:172059ms step_avg:100.33ms
step:1716/1770 train_time:172165ms step_avg:100.33ms
step:1717/1770 train_time:172271ms step_avg:100.33ms
step:1718/1770 train_time:172377ms step_avg:100.34ms
step:1719/1770 train_time:172483ms step_avg:100.34ms
step:1720/1770 train_time:172590ms step_avg:100.34ms
step:1721/1770 train_time:172695ms step_avg:100.35ms
step:1722/1770 train_time:172802ms step_avg:100.35ms
step:1723/1770 train_time:172908ms step_avg:100.35ms
step:1724/1770 train_time:173016ms step_avg:100.36ms
step:1725/1770 train_time:173123ms step_avg:100.36ms
step:1726/1770 train_time:173230ms step_avg:100.36ms
step:1727/1770 train_time:173335ms step_avg:100.37ms
step:1728/1770 train_time:173441ms step_avg:100.37ms
step:1729/1770 train_time:173547ms step_avg:100.37ms
step:1730/1770 train_time:173654ms step_avg:100.38ms
step:1731/1770 train_time:173760ms step_avg:100.38ms
step:1732/1770 train_time:173866ms step_avg:100.38ms
step:1733/1770 train_time:173972ms step_avg:100.39ms
step:1734/1770 train_time:174077ms step_avg:100.39ms
step:1735/1770 train_time:174183ms step_avg:100.39ms
step:1736/1770 train_time:174288ms step_avg:100.40ms
step:1737/1770 train_time:174394ms step_avg:100.40ms
step:1738/1770 train_time:174499ms step_avg:100.40ms
step:1739/1770 train_time:174605ms step_avg:100.41ms
step:1740/1770 train_time:174711ms step_avg:100.41ms
step:1741/1770 train_time:174820ms step_avg:100.41ms
step:1742/1770 train_time:174928ms step_avg:100.42ms
step:1743/1770 train_time:175034ms step_avg:100.42ms
step:1744/1770 train_time:175140ms step_avg:100.42ms
step:1745/1770 train_time:175245ms step_avg:100.43ms
step:1746/1770 train_time:175352ms step_avg:100.43ms
step:1747/1770 train_time:175456ms step_avg:100.43ms
step:1748/1770 train_time:175564ms step_avg:100.44ms
step:1749/1770 train_time:175673ms step_avg:100.44ms
step:1750/1770 train_time:175778ms step_avg:100.44ms
step:1750/1770 val_loss:3.2819 train_time:175883ms step_avg:100.50ms
step:1751/1770 train_time:175900ms step_avg:100.46ms
step:1752/1770 train_time:175998ms step_avg:100.46ms
step:1753/1770 train_time:176104ms step_avg:100.46ms
step:1754/1770 train_time:176210ms step_avg:100.46ms
step:1755/1770 train_time:176314ms step_avg:100.46ms
step:1756/1770 train_time:176420ms step_avg:100.47ms
step:1757/1770 train_time:176526ms step_avg:100.47ms
step:1758/1770 train_time:176632ms step_avg:100.47ms
step:1759/1770 train_time:176737ms step_avg:100.48ms
step:1760/1770 train_time:176843ms step_avg:100.48ms
step:1761/1770 train_time:176951ms step_avg:100.48ms
step:1762/1770 train_time:177060ms step_avg:100.49ms
step:1763/1770 train_time:177167ms step_avg:100.49ms
step:1764/1770 train_time:177272ms step_avg:100.49ms
step:1765/1770 train_time:177377ms step_avg:100.50ms
step:1766/1770 train_time:177486ms step_avg:100.50ms
step:1767/1770 train_time:177590ms step_avg:100.50ms
step:1768/1770 train_time:177696ms step_avg:100.51ms
step:1769/1770 train_time:177801ms step_avg:100.51ms
step:1770/1770 train_time:177906ms step_avg:100.51ms
step:1770/1770 val_loss:3.2789 train_time:178012ms step_avg:100.57ms
peak memory allocated: 30724 MiB reserved: 46472 MiB
