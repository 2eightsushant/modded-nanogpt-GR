import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
import copy
import glob
from dataclasses import dataclass
from functools import lru_cache, partial # Added partial for hook registration
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
#torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params: list[Tensor] = [*params]
        param_groups = []
        for size in {p.numel() for p in params}:
            b = torch.empty(world_size, size, dtype=torch.bfloat16, device="cuda")
            group = dict(params=[p for p in params if p.numel() == size],
                         update_buffer=b, update_buffer_views=[b[i] for i in range(world_size)])
            param_groups.append(group)
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            update_buffer: Tensor = group["update_buffer"]
            update_buffer_views: list[Tensor] = group["update_buffer_views"]
            # generate weight updates in distributed fashion
            params: list[Tensor] = group["params"]
            handle = None
            params_world = None
            def update_prev(): # optimized Muon implementation contributed by @YouJiacheng
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffer_views):
                    p_world.add_(g_world.view_as(p_world),
                                 alpha=-group["lr"] * max(1, p_world.size(-2) / p_world.size(-1))**0.5)
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if "momentum_buffer" not in state:
                        state["momentum_buffer"] = torch.zeros_like(g)
                    buf: Tensor = state["momentum_buffer"]
                    buf.lerp_(g, 1 - group["momentum"])
                    g = g.lerp_(buf, group["momentum"]) if group["nesterov"] else buf
                    g = zeropower_via_newtonschulz5(g, steps=group["ns_steps"]).flatten()
                else:
                    g = update_buffer_views[self.rank]
                if base_i > 0:
                    update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=0.12).transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, block_mask: BlockMask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, max_seq_len, i) for i in range(num_layers)])
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128),
                                    use_fp8=True, x_s=(model_dim**0.5)/448, w_s=24/448, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        self.skip_weights = nn.Parameter(torch.ones(num_layers//2))

    def create_blockmasks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        docs = (input_seq == 50256).cumsum(0)

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_blockmask: Tensor):
            num_blocks = dense_blockmask.sum(dim=-1, dtype=torch.int32)
            indices = dense_blockmask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
        causal_blockmask_any = block_idx[:, None] >= block_idx
        causal_blockmask_all = block_idx[:, None] > block_idx
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
        document_blockmask_any = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
        document_blockmask_all = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
        blockmask_any = causal_blockmask_any & document_blockmask_any
        blockmask_all = causal_blockmask_all & document_blockmask_all
        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(blockmask_any & ~blockmask_all)
        full_kv_num_blocks, full_kv_indices = dense_to_ordered(blockmask_all)
        def build_bm(window_size_blocks: Tensor) -> BlockMask:
            return BlockMask.from_kv_blocks(
                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(window_size_blocks - full_kv_num_blocks, 1)),
                partial_kv_indices,
                torch.clamp_max(full_kv_num_blocks, window_size_blocks - 1),
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        long_bm, short_bm = self.create_blockmasks(input_seq, sliding_window_num_blocks)
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, long_bm, short_bm, short_bm, short_bm, long_bm]
        assert len(block_masks) == len(self.blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977

        # U-net design by @brendanh0gan
        skip_connections = []
        n = len(self.skip_weights)
        for i in range(len(self.blocks)):
            if i >= n:
                x = x + self.skip_weights[i - n] * skip_connections.pop()
            x = self.blocks[i](x, ve[i], x0, block_masks[i])
            if i < n:
                skip_connections.append(x)

        x = norm(x)
        logits = self.lm_head(x).float()
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / (7.5 * x.size(-1)**0.5))
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq, reduction='sum' if self.training else 'mean')
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn't helpful.
        pos += batch_size
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_seq_len = 48*1024 # FlexAttention sequence length
    val_seq_len = 4*64*1024 # FlexAttention sequence length for validation
    # optimization
    num_iterations = 1770 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    # architecture
    vocab_size = 50257
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert world_size == 8 # this code is designed for 8xH100
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs("logs_bucket", exist_ok=True)
    logfile = f"logs_bucket/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

########################################
#    Construct model and optimizer     #
########################################

model: nn.Module = GPT(vocab_size=args.vocab_size, num_layers=12, num_heads=6, model_dim=768,
                       max_seq_len=max(args.train_seq_len, args.val_seq_len)).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
adam_params = [dict(params=head_params, lr=0.22), dict(params=embed_params, lr=0.6), dict(params=scalar_params, lr=0.04)]
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = torch.optim.Adam(adam_params, betas=(0.8, 0.95), eps=1e-10, fused=True)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then decay
def get_lr(step: int):
    x = step / args.num_iterations # progress in training
    assert 0 <= x < 1
    if x < 1 - args.cooldown_frac:
        return 1.0
    else:
        w = (1 - x) / args.cooldown_frac
        return w * 1.0 + (1 - w) * 0.1

# attention window size schedule: linearly increase
@lru_cache(1)
def get_window_size_blocks_helper(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)
def get_window_size_blocks(step: int):
    x = step / args.num_iterations # progress in training
    assert 0 <= x <= 1
    # Linearly increase the block-wise sliding window size over training 128 -> 1792
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * x, n=128)
    return get_window_size_blocks_helper(window_size)

model: nn.Module = torch.compile(model, dynamic=False)

########################################
#     Warmup kernel and Layerwise Bucketing   #
########################################

# Track the gradient readiness order during warmup
gradient_ready_order = []

def record_grad_order_hook(param):
    def hook(*_):
        if param.grad is not None and all(param is not p for p in gradient_ready_order):
            gradient_ready_order.append(param)
    return hook

# Register temp hooks to track when each parameter's gradient is ready
tmp_handles = []
for p in model.parameters():
    if p.requires_grad:
        handle = p.register_post_accumulate_grad_hook(record_grad_order_hook(p))
        tmp_handles.append(handle)

warmup_steps = 10
initial_state = dict(
    model=copy.deepcopy(model.state_dict()),
    optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]
)

for step in range(warmup_steps):
    # Create a local generator with fixed, step-dependent seed
    g = torch.Generator(device="cuda")
    g.manual_seed(1337 + step)

    # Use this generator to sample inputs
    inputs = targets = torch.randint(
        low=0,
        high=args.vocab_size,
        size=(args.train_seq_len,),
        device="cuda",
        dtype=torch.long,
        generator=g,  # <- Local generator ensures deterministic and isolated RNG
    )
    model(inputs.to(torch.int32), targets, get_window_size_blocks(0)).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)

# Remove hooks after warmup
for h in tmp_handles:
    h.remove()

# Restore model/optimizer state to ensure a clean start
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del initial_state

print0(f"Collected {len(gradient_ready_order)} parameters in gradient-ready order")

########################################
#      Overlap Communication Setup     #
########################################

# Layer-aware bucketing using gradient-ready order
def create_buckets_layerwise(ordered_params, bucket_size_mb=25):
    """Group parameters into buckets of ~bucket_size_mb MB, in gradient ready order"""
    buckets = []
    current_bucket = []
    current_size = 0

    for param in ordered_params:
        if not param.requires_grad:
            continue
        param_size_mb = param.numel() * param.element_size() / (1024 * 1024)

        if current_size + param_size_mb > bucket_size_mb and current_bucket:
            buckets.append(current_bucket)
            current_bucket = [param]
            current_size = param_size_mb
        else:
            current_bucket.append(param)
            current_size += param_size_mb

    if current_bucket:
        buckets.append(current_bucket)

    return buckets

# Create buckets for all parameters
all_params = [p for p in model.parameters() if p.requires_grad]
param_buckets = create_buckets_layerwise(gradient_ready_order)
print0(f"Created {len(param_buckets)} gradient buckets")
for i, bucket in enumerate(param_buckets):
    total_size = sum(p.numel() * p.element_size() for p in bucket) / (1024 * 1024)
    print0(f"Bucket {i}: {len(bucket)} params, {total_size:.1f} MB")

# Bucket state tracking
bucket_ready_count = [0] * len(param_buckets)
bucket_handles = [None] * len(param_buckets)
param_to_bucket = {}

# Map each parameter to its bucket index
for bucket_idx, bucket in enumerate(param_buckets):
    for param in bucket:
        param_to_bucket[param] = bucket_idx

def _gradient_hook(param: Tensor):
    """Called when a parameter's gradient is ready"""
    if param.grad is None:
        return

    bucket_idx = param_to_bucket[param]
    bucket_ready_count[bucket_idx] += 1

    # Check if all parameters in this bucket are ready
    if bucket_ready_count[bucket_idx] == len(param_buckets[bucket_idx]):
        # All-reduce this bucket
        bucket_grads = [p.grad for p in param_buckets[bucket_idx]]

        # For multi-tensor operations, we can reduce them together
        if len(bucket_grads) == 1:
            handle = dist.all_reduce(bucket_grads[0], op=dist.ReduceOp.AVG, async_op=True)
        else:
            # Use multi-tensor all-reduce for efficiency
            handle = dist.all_reduce_coalesced(bucket_grads, op=dist.ReduceOp.AVG, async_op=True)

        bucket_handles[bucket_idx] = handle

# Register hooks for all parameters
print0("Registering bucketed gradient hooks...")
for param in all_params:
    param.register_post_accumulate_grad_hook(_gradient_hook)

def wait_for_gradients():
    """Wait for all gradient reductions to complete and reset bucket state"""
    for handle in bucket_handles:
        if handle is not None:
            handle.wait()

    # Reset state for next iteration
    for i in range(len(bucket_ready_count)):
        bucket_ready_count[i] = 0
        bucket_handles[i] = None

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, world_size * args.train_seq_len, rank, world_size)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_batch_size = world_size * args.val_seq_len
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        val_loader = distributed_data_generator(args.val_files, val_batch_size, rank, world_size)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets = next(val_loader)
                val_loss += model(inputs, targets, get_window_size_blocks(step))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs_bucket/{run_id}", exist_ok=True)
            torch.save(log, f"logs_bucket/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    inputs, targets = next(train_loader)
    model(inputs, targets, get_window_size_blocks(step)).backward()
    #for param in model.parameters():
    #    dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
    wait_for_gradients() # does the same thing as commented two lines above, but faster

    # set optimization hyperparameters
    for opt in optimizers:
        for group in opt.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)
    for group in optimizer2.param_groups:
        frac = min(step / 300, 1) # momentum warmup for muon
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers
    for opt in optimizers:
        opt.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Feb  4 2025, 14:57:36) [GCC 11.4.0]
Running PyTorch 2.8.0.dev20250612+cu126 compiled for CUDA 12.6
Fri Jun 13 04:39:12 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.08             Driver Version: 550.127.08     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   32C    P0            121W /  700W |    5824MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   30C    P0            124W /  700W |    1514MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   28C    P0            118W /  700W |    1514MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   30C    P0            120W /  700W |    1514MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   30C    P0            117W /  700W |    1514MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   28C    P0            118W /  700W |    1514MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   29C    P0            118W /  700W |    1514MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   27C    P0            112W /  700W |    1514MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
Collected 75 parameters in gradient-ready order
Created 21 gradient buckets
Bucket 0: 1 params, 73.6 MB
Bucket 1: 1 params, 73.6 MB
Bucket 2: 4 params, 15.8 MB
Bucket 3: 1 params, 147.4 MB
Bucket 4: 4 params, 20.3 MB
Bucket 5: 4 params, 24.8 MB
Bucket 6: 4 params, 18.0 MB
Bucket 7: 5 params, 18.0 MB
Bucket 8: 4 params, 20.3 MB
Bucket 9: 5 params, 24.8 MB
Bucket 10: 4 params, 20.3 MB
Bucket 11: 4 params, 24.8 MB
Bucket 12: 5 params, 18.0 MB
Bucket 13: 5 params, 20.3 MB
Bucket 14: 5 params, 18.0 MB
Bucket 15: 4 params, 20.3 MB
Bucket 16: 5 params, 24.8 MB
Bucket 17: 4 params, 24.8 MB
Bucket 18: 4 params, 2.3 MB
Bucket 19: 1 params, 73.6 MB
Bucket 20: 1 params, 73.6 MB
Registering bucketed gradient hooks...
step:0/1770 val_loss:10.8258 train_time:0ms step_avg:0.02ms
step:1/1770 train_time:110ms step_avg:109.68ms
step:2/1770 train_time:183ms step_avg:91.60ms
step:3/1770 train_time:271ms step_avg:90.32ms
step:4/1770 train_time:364ms step_avg:90.96ms
step:5/1770 train_time:457ms step_avg:91.46ms
step:6/1770 train_time:551ms step_avg:91.89ms
step:7/1770 train_time:645ms step_avg:92.20ms
step:8/1770 train_time:740ms step_avg:92.44ms
step:9/1770 train_time:833ms step_avg:92.55ms
step:10/1770 train_time:927ms step_avg:92.67ms
step:11/1770 train_time:1021ms step_avg:92.79ms
step:12/1770 train_time:1114ms step_avg:92.83ms
step:13/1770 train_time:1210ms step_avg:93.09ms
step:14/1770 train_time:1306ms step_avg:93.27ms
step:15/1770 train_time:1400ms step_avg:93.30ms
step:16/1770 train_time:1494ms step_avg:93.39ms
step:17/1770 train_time:1589ms step_avg:93.47ms
step:18/1770 train_time:1684ms step_avg:93.54ms
step:19/1770 train_time:1778ms step_avg:93.58ms
step:20/1770 train_time:1873ms step_avg:93.67ms
step:21/1770 train_time:1967ms step_avg:93.66ms
step:22/1770 train_time:2060ms step_avg:93.65ms
step:23/1770 train_time:2155ms step_avg:93.68ms
step:24/1770 train_time:2249ms step_avg:93.72ms
step:25/1770 train_time:2344ms step_avg:93.75ms
step:26/1770 train_time:2438ms step_avg:93.78ms
step:27/1770 train_time:2532ms step_avg:93.79ms
step:28/1770 train_time:2627ms step_avg:93.81ms
step:29/1770 train_time:2721ms step_avg:93.82ms
step:30/1770 train_time:2816ms step_avg:93.86ms
step:31/1770 train_time:2910ms step_avg:93.86ms
step:32/1770 train_time:3004ms step_avg:93.89ms
step:33/1770 train_time:3099ms step_avg:93.90ms
step:34/1770 train_time:3193ms step_avg:93.91ms
step:35/1770 train_time:3288ms step_avg:93.94ms
step:36/1770 train_time:3382ms step_avg:93.95ms
step:37/1770 train_time:3477ms step_avg:93.98ms
step:38/1770 train_time:3572ms step_avg:93.99ms
step:39/1770 train_time:3667ms step_avg:94.02ms
step:40/1770 train_time:3761ms step_avg:94.02ms
step:41/1770 train_time:3854ms step_avg:94.01ms
step:42/1770 train_time:3948ms step_avg:94.01ms
step:43/1770 train_time:4042ms step_avg:94.00ms
step:44/1770 train_time:4136ms step_avg:93.99ms
step:45/1770 train_time:4231ms step_avg:94.02ms
step:46/1770 train_time:4324ms step_avg:94.00ms
step:47/1770 train_time:4419ms step_avg:94.02ms
step:48/1770 train_time:4513ms step_avg:94.03ms
step:49/1770 train_time:4608ms step_avg:94.03ms
step:50/1770 train_time:4702ms step_avg:94.03ms
step:51/1770 train_time:4795ms step_avg:94.02ms
step:52/1770 train_time:4889ms step_avg:94.02ms
step:53/1770 train_time:4984ms step_avg:94.03ms
step:54/1770 train_time:5078ms step_avg:94.03ms
step:55/1770 train_time:5172ms step_avg:94.03ms
step:56/1770 train_time:5267ms step_avg:94.05ms
step:57/1770 train_time:5361ms step_avg:94.05ms
step:58/1770 train_time:5455ms step_avg:94.05ms
step:59/1770 train_time:5550ms step_avg:94.06ms
step:60/1770 train_time:5644ms step_avg:94.07ms
step:61/1770 train_time:5739ms step_avg:94.08ms
step:62/1770 train_time:5832ms step_avg:94.07ms
step:63/1770 train_time:5927ms step_avg:94.08ms
step:64/1770 train_time:6021ms step_avg:94.08ms
step:65/1770 train_time:6115ms step_avg:94.07ms
step:66/1770 train_time:6209ms step_avg:94.08ms
step:67/1770 train_time:6305ms step_avg:94.10ms
step:68/1770 train_time:6399ms step_avg:94.10ms
step:69/1770 train_time:6493ms step_avg:94.11ms
step:70/1770 train_time:6588ms step_avg:94.12ms
step:71/1770 train_time:6682ms step_avg:94.12ms
step:72/1770 train_time:6776ms step_avg:94.11ms
step:73/1770 train_time:6870ms step_avg:94.11ms
step:74/1770 train_time:6964ms step_avg:94.11ms
step:75/1770 train_time:7058ms step_avg:94.11ms
step:76/1770 train_time:7152ms step_avg:94.11ms
step:77/1770 train_time:7247ms step_avg:94.11ms
step:78/1770 train_time:7341ms step_avg:94.11ms
step:79/1770 train_time:7435ms step_avg:94.12ms
step:80/1770 train_time:7529ms step_avg:94.11ms
step:81/1770 train_time:7623ms step_avg:94.12ms
step:82/1770 train_time:7718ms step_avg:94.12ms
step:83/1770 train_time:7811ms step_avg:94.11ms
step:84/1770 train_time:7906ms step_avg:94.12ms
step:85/1770 train_time:8000ms step_avg:94.11ms
step:86/1770 train_time:8094ms step_avg:94.12ms
step:87/1770 train_time:8189ms step_avg:94.12ms
step:88/1770 train_time:8283ms step_avg:94.12ms
step:89/1770 train_time:8378ms step_avg:94.13ms
step:90/1770 train_time:8471ms step_avg:94.13ms
step:91/1770 train_time:8567ms step_avg:94.14ms
step:92/1770 train_time:8662ms step_avg:94.16ms
step:93/1770 train_time:8756ms step_avg:94.15ms
step:94/1770 train_time:8850ms step_avg:94.15ms
step:95/1770 train_time:8945ms step_avg:94.15ms
step:96/1770 train_time:9039ms step_avg:94.16ms
step:97/1770 train_time:9133ms step_avg:94.15ms
step:98/1770 train_time:9228ms step_avg:94.16ms
step:99/1770 train_time:9323ms step_avg:94.17ms
step:100/1770 train_time:9417ms step_avg:94.17ms
step:101/1770 train_time:9512ms step_avg:94.18ms
step:102/1770 train_time:9607ms step_avg:94.19ms
step:103/1770 train_time:9702ms step_avg:94.19ms
step:104/1770 train_time:9797ms step_avg:94.20ms
step:105/1770 train_time:9891ms step_avg:94.20ms
step:106/1770 train_time:9986ms step_avg:94.20ms
step:107/1770 train_time:10080ms step_avg:94.20ms
step:108/1770 train_time:10175ms step_avg:94.21ms
step:109/1770 train_time:10269ms step_avg:94.21ms
step:110/1770 train_time:10364ms step_avg:94.22ms
step:111/1770 train_time:10458ms step_avg:94.21ms
step:112/1770 train_time:10552ms step_avg:94.22ms
step:113/1770 train_time:10648ms step_avg:94.23ms
step:114/1770 train_time:10742ms step_avg:94.23ms
step:115/1770 train_time:10837ms step_avg:94.23ms
step:116/1770 train_time:10931ms step_avg:94.23ms
step:117/1770 train_time:11026ms step_avg:94.24ms
step:118/1770 train_time:11121ms step_avg:94.24ms
step:119/1770 train_time:11216ms step_avg:94.25ms
step:120/1770 train_time:11310ms step_avg:94.25ms
step:121/1770 train_time:11405ms step_avg:94.25ms
step:122/1770 train_time:11499ms step_avg:94.25ms
step:123/1770 train_time:11594ms step_avg:94.26ms
step:124/1770 train_time:11689ms step_avg:94.27ms
step:125/1770 train_time:11784ms step_avg:94.27ms
step:125/1770 val_loss:4.6328 train_time:11877ms step_avg:95.02ms
step:126/1770 train_time:11897ms step_avg:94.42ms
step:127/1770 train_time:11980ms step_avg:94.33ms
step:128/1770 train_time:12083ms step_avg:94.40ms
step:129/1770 train_time:12179ms step_avg:94.41ms
step:130/1770 train_time:12274ms step_avg:94.41ms
step:131/1770 train_time:12368ms step_avg:94.41ms
step:132/1770 train_time:12461ms step_avg:94.40ms
step:133/1770 train_time:12555ms step_avg:94.40ms
step:134/1770 train_time:12649ms step_avg:94.39ms
step:135/1770 train_time:12743ms step_avg:94.39ms
step:136/1770 train_time:12837ms step_avg:94.39ms
step:137/1770 train_time:12933ms step_avg:94.40ms
step:138/1770 train_time:13029ms step_avg:94.41ms
step:139/1770 train_time:13125ms step_avg:94.43ms
step:140/1770 train_time:13221ms step_avg:94.44ms
step:141/1770 train_time:13316ms step_avg:94.44ms
step:142/1770 train_time:13411ms step_avg:94.44ms
step:143/1770 train_time:13505ms step_avg:94.44ms
step:144/1770 train_time:13600ms step_avg:94.44ms
step:145/1770 train_time:13694ms step_avg:94.44ms
step:146/1770 train_time:13788ms step_avg:94.44ms
step:147/1770 train_time:13883ms step_avg:94.44ms
step:148/1770 train_time:13979ms step_avg:94.45ms
step:149/1770 train_time:14074ms step_avg:94.46ms
step:150/1770 train_time:14170ms step_avg:94.47ms
step:151/1770 train_time:14265ms step_avg:94.47ms
step:152/1770 train_time:14360ms step_avg:94.48ms
step:153/1770 train_time:14457ms step_avg:94.49ms
step:154/1770 train_time:14550ms step_avg:94.48ms
step:155/1770 train_time:14644ms step_avg:94.48ms
step:156/1770 train_time:14739ms step_avg:94.48ms
step:157/1770 train_time:14834ms step_avg:94.48ms
step:158/1770 train_time:14929ms step_avg:94.49ms
step:159/1770 train_time:15025ms step_avg:94.49ms
step:160/1770 train_time:15121ms step_avg:94.50ms
step:161/1770 train_time:15217ms step_avg:94.52ms
step:162/1770 train_time:15312ms step_avg:94.52ms
step:163/1770 train_time:15408ms step_avg:94.53ms
step:164/1770 train_time:15502ms step_avg:94.52ms
step:165/1770 train_time:15597ms step_avg:94.53ms
step:166/1770 train_time:15691ms step_avg:94.53ms
step:167/1770 train_time:15787ms step_avg:94.53ms
step:168/1770 train_time:15882ms step_avg:94.54ms
step:169/1770 train_time:15978ms step_avg:94.55ms
step:170/1770 train_time:16073ms step_avg:94.55ms
step:171/1770 train_time:16168ms step_avg:94.55ms
step:172/1770 train_time:16265ms step_avg:94.56ms
step:173/1770 train_time:16359ms step_avg:94.56ms
step:174/1770 train_time:16454ms step_avg:94.56ms
step:175/1770 train_time:16549ms step_avg:94.57ms
step:176/1770 train_time:16645ms step_avg:94.57ms
step:177/1770 train_time:16739ms step_avg:94.57ms
step:178/1770 train_time:16834ms step_avg:94.57ms
step:179/1770 train_time:16929ms step_avg:94.57ms
step:180/1770 train_time:17023ms step_avg:94.57ms
step:181/1770 train_time:17119ms step_avg:94.58ms
step:182/1770 train_time:17214ms step_avg:94.58ms
step:183/1770 train_time:17309ms step_avg:94.58ms
step:184/1770 train_time:17404ms step_avg:94.59ms
step:185/1770 train_time:17501ms step_avg:94.60ms
step:186/1770 train_time:17596ms step_avg:94.60ms
step:187/1770 train_time:17691ms step_avg:94.60ms
step:188/1770 train_time:17785ms step_avg:94.60ms
step:189/1770 train_time:17881ms step_avg:94.61ms
step:190/1770 train_time:17975ms step_avg:94.61ms
step:191/1770 train_time:18071ms step_avg:94.61ms
step:192/1770 train_time:18165ms step_avg:94.61ms
step:193/1770 train_time:18261ms step_avg:94.61ms
step:194/1770 train_time:18355ms step_avg:94.62ms
step:195/1770 train_time:18451ms step_avg:94.62ms
step:196/1770 train_time:18546ms step_avg:94.62ms
step:197/1770 train_time:18641ms step_avg:94.63ms
step:198/1770 train_time:18737ms step_avg:94.63ms
step:199/1770 train_time:18832ms step_avg:94.63ms
step:200/1770 train_time:18926ms step_avg:94.63ms
step:201/1770 train_time:19022ms step_avg:94.64ms
step:202/1770 train_time:19117ms step_avg:94.64ms
step:203/1770 train_time:19212ms step_avg:94.64ms
step:204/1770 train_time:19306ms step_avg:94.64ms
step:205/1770 train_time:19402ms step_avg:94.64ms
step:206/1770 train_time:19498ms step_avg:94.65ms
step:207/1770 train_time:19594ms step_avg:94.66ms
step:208/1770 train_time:19689ms step_avg:94.66ms
step:209/1770 train_time:19784ms step_avg:94.66ms
step:210/1770 train_time:19880ms step_avg:94.67ms
step:211/1770 train_time:19976ms step_avg:94.67ms
step:212/1770 train_time:20070ms step_avg:94.67ms
step:213/1770 train_time:20165ms step_avg:94.67ms
step:214/1770 train_time:20261ms step_avg:94.68ms
step:215/1770 train_time:20356ms step_avg:94.68ms
step:216/1770 train_time:20451ms step_avg:94.68ms
step:217/1770 train_time:20546ms step_avg:94.68ms
step:218/1770 train_time:20641ms step_avg:94.68ms
step:219/1770 train_time:20736ms step_avg:94.69ms
step:220/1770 train_time:20831ms step_avg:94.69ms
step:221/1770 train_time:20926ms step_avg:94.69ms
step:222/1770 train_time:21021ms step_avg:94.69ms
step:223/1770 train_time:21117ms step_avg:94.69ms
step:224/1770 train_time:21212ms step_avg:94.70ms
step:225/1770 train_time:21307ms step_avg:94.70ms
step:226/1770 train_time:21402ms step_avg:94.70ms
step:227/1770 train_time:21497ms step_avg:94.70ms
step:228/1770 train_time:21592ms step_avg:94.70ms
step:229/1770 train_time:21688ms step_avg:94.71ms
step:230/1770 train_time:21782ms step_avg:94.71ms
step:231/1770 train_time:21878ms step_avg:94.71ms
step:232/1770 train_time:21973ms step_avg:94.71ms
step:233/1770 train_time:22069ms step_avg:94.71ms
step:234/1770 train_time:22163ms step_avg:94.71ms
step:235/1770 train_time:22259ms step_avg:94.72ms
step:236/1770 train_time:22354ms step_avg:94.72ms
step:237/1770 train_time:22449ms step_avg:94.72ms
step:238/1770 train_time:22543ms step_avg:94.72ms
step:239/1770 train_time:22639ms step_avg:94.72ms
step:240/1770 train_time:22734ms step_avg:94.73ms
step:241/1770 train_time:22830ms step_avg:94.73ms
step:242/1770 train_time:22926ms step_avg:94.74ms
step:243/1770 train_time:23021ms step_avg:94.74ms
step:244/1770 train_time:23117ms step_avg:94.74ms
step:245/1770 train_time:23212ms step_avg:94.74ms
step:246/1770 train_time:23307ms step_avg:94.74ms
step:247/1770 train_time:23401ms step_avg:94.74ms
step:248/1770 train_time:23497ms step_avg:94.75ms
step:249/1770 train_time:23592ms step_avg:94.75ms
step:250/1770 train_time:23687ms step_avg:94.75ms
step:250/1770 val_loss:4.1008 train_time:23781ms step_avg:95.12ms
step:251/1770 train_time:23801ms step_avg:94.82ms
step:252/1770 train_time:23883ms step_avg:94.78ms
step:253/1770 train_time:23982ms step_avg:94.79ms
step:254/1770 train_time:24077ms step_avg:94.79ms
step:255/1770 train_time:24181ms step_avg:94.83ms
step:256/1770 train_time:24266ms step_avg:94.79ms
step:257/1770 train_time:24361ms step_avg:94.79ms
step:258/1770 train_time:24455ms step_avg:94.79ms
step:259/1770 train_time:24550ms step_avg:94.79ms
step:260/1770 train_time:24645ms step_avg:94.79ms
step:261/1770 train_time:24739ms step_avg:94.79ms
step:262/1770 train_time:24836ms step_avg:94.79ms
step:263/1770 train_time:24933ms step_avg:94.80ms
step:264/1770 train_time:25028ms step_avg:94.80ms
step:265/1770 train_time:25125ms step_avg:94.81ms
step:266/1770 train_time:25221ms step_avg:94.81ms
step:267/1770 train_time:25316ms step_avg:94.82ms
step:268/1770 train_time:25411ms step_avg:94.82ms
step:269/1770 train_time:25506ms step_avg:94.82ms
step:270/1770 train_time:25601ms step_avg:94.82ms
step:271/1770 train_time:25696ms step_avg:94.82ms
step:272/1770 train_time:25792ms step_avg:94.82ms
step:273/1770 train_time:25889ms step_avg:94.83ms
step:274/1770 train_time:25986ms step_avg:94.84ms
step:275/1770 train_time:26082ms step_avg:94.84ms
step:276/1770 train_time:26177ms step_avg:94.85ms
step:277/1770 train_time:26274ms step_avg:94.85ms
step:278/1770 train_time:26369ms step_avg:94.85ms
step:279/1770 train_time:26464ms step_avg:94.85ms
step:280/1770 train_time:26559ms step_avg:94.85ms
step:281/1770 train_time:26654ms step_avg:94.85ms
step:282/1770 train_time:26749ms step_avg:94.85ms
step:283/1770 train_time:26845ms step_avg:94.86ms
step:284/1770 train_time:26941ms step_avg:94.86ms
step:285/1770 train_time:27037ms step_avg:94.87ms
step:286/1770 train_time:27132ms step_avg:94.87ms
step:287/1770 train_time:27228ms step_avg:94.87ms
step:288/1770 train_time:27324ms step_avg:94.88ms
step:289/1770 train_time:27419ms step_avg:94.88ms
step:290/1770 train_time:27514ms step_avg:94.88ms
step:291/1770 train_time:27610ms step_avg:94.88ms
step:292/1770 train_time:27706ms step_avg:94.88ms
step:293/1770 train_time:27801ms step_avg:94.88ms
step:294/1770 train_time:27896ms step_avg:94.88ms
step:295/1770 train_time:27992ms step_avg:94.89ms
step:296/1770 train_time:28088ms step_avg:94.89ms
step:297/1770 train_time:28185ms step_avg:94.90ms
step:298/1770 train_time:28282ms step_avg:94.91ms
step:299/1770 train_time:28377ms step_avg:94.91ms
step:300/1770 train_time:28472ms step_avg:94.91ms
step:301/1770 train_time:28567ms step_avg:94.91ms
step:302/1770 train_time:28663ms step_avg:94.91ms
step:303/1770 train_time:28758ms step_avg:94.91ms
step:304/1770 train_time:28854ms step_avg:94.91ms
step:305/1770 train_time:28949ms step_avg:94.92ms
step:306/1770 train_time:29046ms step_avg:94.92ms
step:307/1770 train_time:29141ms step_avg:94.92ms
step:308/1770 train_time:29237ms step_avg:94.93ms
step:309/1770 train_time:29334ms step_avg:94.93ms
step:310/1770 train_time:29429ms step_avg:94.93ms
step:311/1770 train_time:29525ms step_avg:94.94ms
step:312/1770 train_time:29620ms step_avg:94.94ms
step:313/1770 train_time:29716ms step_avg:94.94ms
step:314/1770 train_time:29811ms step_avg:94.94ms
step:315/1770 train_time:29906ms step_avg:94.94ms
step:316/1770 train_time:30003ms step_avg:94.95ms
step:317/1770 train_time:30098ms step_avg:94.95ms
step:318/1770 train_time:30194ms step_avg:94.95ms
step:319/1770 train_time:30290ms step_avg:94.95ms
step:320/1770 train_time:30386ms step_avg:94.96ms
step:321/1770 train_time:30482ms step_avg:94.96ms
step:322/1770 train_time:30578ms step_avg:94.96ms
step:323/1770 train_time:30673ms step_avg:94.96ms
step:324/1770 train_time:30768ms step_avg:94.96ms
step:325/1770 train_time:30864ms step_avg:94.97ms
step:326/1770 train_time:30959ms step_avg:94.97ms
step:327/1770 train_time:31056ms step_avg:94.97ms
step:328/1770 train_time:31150ms step_avg:94.97ms
step:329/1770 train_time:31246ms step_avg:94.97ms
step:330/1770 train_time:31342ms step_avg:94.98ms
step:331/1770 train_time:31437ms step_avg:94.98ms
step:332/1770 train_time:31533ms step_avg:94.98ms
step:333/1770 train_time:31628ms step_avg:94.98ms
step:334/1770 train_time:31724ms step_avg:94.98ms
step:335/1770 train_time:31819ms step_avg:94.98ms
step:336/1770 train_time:31914ms step_avg:94.98ms
step:337/1770 train_time:32009ms step_avg:94.98ms
step:338/1770 train_time:32105ms step_avg:94.99ms
step:339/1770 train_time:32201ms step_avg:94.99ms
step:340/1770 train_time:32297ms step_avg:94.99ms
step:341/1770 train_time:32392ms step_avg:94.99ms
step:342/1770 train_time:32488ms step_avg:94.99ms
step:343/1770 train_time:32584ms step_avg:95.00ms
step:344/1770 train_time:32679ms step_avg:95.00ms
step:345/1770 train_time:32774ms step_avg:95.00ms
step:346/1770 train_time:32869ms step_avg:95.00ms
step:347/1770 train_time:32965ms step_avg:95.00ms
step:348/1770 train_time:33061ms step_avg:95.00ms
step:349/1770 train_time:33156ms step_avg:95.00ms
step:350/1770 train_time:33252ms step_avg:95.00ms
step:351/1770 train_time:33347ms step_avg:95.01ms
step:352/1770 train_time:33444ms step_avg:95.01ms
step:353/1770 train_time:33539ms step_avg:95.01ms
step:354/1770 train_time:33635ms step_avg:95.01ms
step:355/1770 train_time:33731ms step_avg:95.02ms
step:356/1770 train_time:33826ms step_avg:95.02ms
step:357/1770 train_time:33922ms step_avg:95.02ms
step:358/1770 train_time:34017ms step_avg:95.02ms
step:359/1770 train_time:34112ms step_avg:95.02ms
step:360/1770 train_time:34208ms step_avg:95.02ms
step:361/1770 train_time:34304ms step_avg:95.02ms
step:362/1770 train_time:34400ms step_avg:95.03ms
step:363/1770 train_time:34496ms step_avg:95.03ms
step:364/1770 train_time:34591ms step_avg:95.03ms
step:365/1770 train_time:34687ms step_avg:95.03ms
step:366/1770 train_time:34783ms step_avg:95.04ms
step:367/1770 train_time:34878ms step_avg:95.04ms
step:368/1770 train_time:34974ms step_avg:95.04ms
step:369/1770 train_time:35069ms step_avg:95.04ms
step:370/1770 train_time:35165ms step_avg:95.04ms
step:371/1770 train_time:35261ms step_avg:95.04ms
step:372/1770 train_time:35357ms step_avg:95.04ms
step:373/1770 train_time:35453ms step_avg:95.05ms
step:374/1770 train_time:35548ms step_avg:95.05ms
step:375/1770 train_time:35644ms step_avg:95.05ms
step:375/1770 val_loss:3.8923 train_time:35739ms step_avg:95.31ms
step:376/1770 train_time:35759ms step_avg:95.10ms
step:377/1770 train_time:35841ms step_avg:95.07ms
step:378/1770 train_time:35942ms step_avg:95.08ms
step:379/1770 train_time:36038ms step_avg:95.09ms
step:380/1770 train_time:36133ms step_avg:95.09ms
step:381/1770 train_time:36228ms step_avg:95.09ms
step:382/1770 train_time:36323ms step_avg:95.09ms
step:383/1770 train_time:36418ms step_avg:95.09ms
step:384/1770 train_time:36512ms step_avg:95.08ms
step:385/1770 train_time:36607ms step_avg:95.08ms
step:386/1770 train_time:36702ms step_avg:95.08ms
step:387/1770 train_time:36799ms step_avg:95.09ms
step:388/1770 train_time:36896ms step_avg:95.09ms
step:389/1770 train_time:36992ms step_avg:95.10ms
step:390/1770 train_time:37089ms step_avg:95.10ms
step:391/1770 train_time:37184ms step_avg:95.10ms
step:392/1770 train_time:37280ms step_avg:95.10ms
step:393/1770 train_time:37375ms step_avg:95.10ms
step:394/1770 train_time:37470ms step_avg:95.10ms
step:395/1770 train_time:37565ms step_avg:95.10ms
step:396/1770 train_time:37662ms step_avg:95.11ms
step:397/1770 train_time:37759ms step_avg:95.11ms
step:398/1770 train_time:37857ms step_avg:95.12ms
step:399/1770 train_time:37955ms step_avg:95.13ms
step:400/1770 train_time:38053ms step_avg:95.13ms
step:401/1770 train_time:38150ms step_avg:95.14ms
step:402/1770 train_time:38249ms step_avg:95.15ms
step:403/1770 train_time:38347ms step_avg:95.15ms
step:404/1770 train_time:38445ms step_avg:95.16ms
step:405/1770 train_time:38542ms step_avg:95.17ms
step:406/1770 train_time:38639ms step_avg:95.17ms
step:407/1770 train_time:38736ms step_avg:95.18ms
step:408/1770 train_time:38834ms step_avg:95.18ms
step:409/1770 train_time:38932ms step_avg:95.19ms
step:410/1770 train_time:39031ms step_avg:95.20ms
step:411/1770 train_time:39129ms step_avg:95.20ms
step:412/1770 train_time:39228ms step_avg:95.21ms
step:413/1770 train_time:39326ms step_avg:95.22ms
step:414/1770 train_time:39423ms step_avg:95.23ms
step:415/1770 train_time:39521ms step_avg:95.23ms
step:416/1770 train_time:39618ms step_avg:95.24ms
step:417/1770 train_time:39716ms step_avg:95.24ms
step:418/1770 train_time:39814ms step_avg:95.25ms
step:419/1770 train_time:39911ms step_avg:95.25ms
step:420/1770 train_time:40009ms step_avg:95.26ms
step:421/1770 train_time:40108ms step_avg:95.27ms
step:422/1770 train_time:40206ms step_avg:95.28ms
step:423/1770 train_time:40304ms step_avg:95.28ms
step:424/1770 train_time:40401ms step_avg:95.28ms
step:425/1770 train_time:40498ms step_avg:95.29ms
step:426/1770 train_time:40596ms step_avg:95.30ms
step:427/1770 train_time:40694ms step_avg:95.30ms
step:428/1770 train_time:40791ms step_avg:95.31ms
step:429/1770 train_time:40889ms step_avg:95.31ms
step:430/1770 train_time:40987ms step_avg:95.32ms
step:431/1770 train_time:41085ms step_avg:95.33ms
step:432/1770 train_time:41183ms step_avg:95.33ms
step:433/1770 train_time:41281ms step_avg:95.34ms
step:434/1770 train_time:41379ms step_avg:95.34ms
step:435/1770 train_time:41476ms step_avg:95.35ms
step:436/1770 train_time:41574ms step_avg:95.35ms
step:437/1770 train_time:41672ms step_avg:95.36ms
step:438/1770 train_time:41770ms step_avg:95.36ms
step:439/1770 train_time:41868ms step_avg:95.37ms
step:440/1770 train_time:41966ms step_avg:95.38ms
step:441/1770 train_time:42063ms step_avg:95.38ms
step:442/1770 train_time:42161ms step_avg:95.39ms
step:443/1770 train_time:42260ms step_avg:95.39ms
step:444/1770 train_time:42357ms step_avg:95.40ms
step:445/1770 train_time:42455ms step_avg:95.40ms
step:446/1770 train_time:42553ms step_avg:95.41ms
step:447/1770 train_time:42650ms step_avg:95.41ms
step:448/1770 train_time:42749ms step_avg:95.42ms
step:449/1770 train_time:42846ms step_avg:95.43ms
step:450/1770 train_time:42944ms step_avg:95.43ms
step:451/1770 train_time:43042ms step_avg:95.44ms
step:452/1770 train_time:43139ms step_avg:95.44ms
step:453/1770 train_time:43237ms step_avg:95.45ms
step:454/1770 train_time:43334ms step_avg:95.45ms
step:455/1770 train_time:43433ms step_avg:95.46ms
step:456/1770 train_time:43531ms step_avg:95.46ms
step:457/1770 train_time:43629ms step_avg:95.47ms
step:458/1770 train_time:43727ms step_avg:95.47ms
step:459/1770 train_time:43824ms step_avg:95.48ms
step:460/1770 train_time:43922ms step_avg:95.48ms
step:461/1770 train_time:44019ms step_avg:95.49ms
step:462/1770 train_time:44117ms step_avg:95.49ms
step:463/1770 train_time:44215ms step_avg:95.50ms
step:464/1770 train_time:44313ms step_avg:95.50ms
step:465/1770 train_time:44412ms step_avg:95.51ms
step:466/1770 train_time:44510ms step_avg:95.51ms
step:467/1770 train_time:44608ms step_avg:95.52ms
step:468/1770 train_time:44707ms step_avg:95.53ms
step:469/1770 train_time:44804ms step_avg:95.53ms
step:470/1770 train_time:44901ms step_avg:95.53ms
step:471/1770 train_time:44999ms step_avg:95.54ms
step:472/1770 train_time:45096ms step_avg:95.54ms
step:473/1770 train_time:45193ms step_avg:95.55ms
step:474/1770 train_time:45290ms step_avg:95.55ms
step:475/1770 train_time:45389ms step_avg:95.55ms
step:476/1770 train_time:45487ms step_avg:95.56ms
step:477/1770 train_time:45585ms step_avg:95.57ms
step:478/1770 train_time:45684ms step_avg:95.57ms
step:479/1770 train_time:45782ms step_avg:95.58ms
step:480/1770 train_time:45880ms step_avg:95.58ms
step:481/1770 train_time:45978ms step_avg:95.59ms
step:482/1770 train_time:46076ms step_avg:95.59ms
step:483/1770 train_time:46174ms step_avg:95.60ms
step:484/1770 train_time:46271ms step_avg:95.60ms
step:485/1770 train_time:46369ms step_avg:95.61ms
step:486/1770 train_time:46467ms step_avg:95.61ms
step:487/1770 train_time:46564ms step_avg:95.61ms
step:488/1770 train_time:46662ms step_avg:95.62ms
step:489/1770 train_time:46760ms step_avg:95.62ms
step:490/1770 train_time:46858ms step_avg:95.63ms
step:491/1770 train_time:46955ms step_avg:95.63ms
step:492/1770 train_time:47053ms step_avg:95.64ms
step:493/1770 train_time:47150ms step_avg:95.64ms
step:494/1770 train_time:47248ms step_avg:95.64ms
step:495/1770 train_time:47347ms step_avg:95.65ms
step:496/1770 train_time:47445ms step_avg:95.66ms
step:497/1770 train_time:47542ms step_avg:95.66ms
step:498/1770 train_time:47639ms step_avg:95.66ms
step:499/1770 train_time:47737ms step_avg:95.66ms
step:500/1770 train_time:47834ms step_avg:95.67ms
step:500/1770 val_loss:3.7452 train_time:47931ms step_avg:95.86ms
step:501/1770 train_time:47950ms step_avg:95.71ms
step:502/1770 train_time:48037ms step_avg:95.69ms
step:503/1770 train_time:48140ms step_avg:95.70ms
step:504/1770 train_time:48239ms step_avg:95.71ms
step:505/1770 train_time:48337ms step_avg:95.72ms
step:506/1770 train_time:48435ms step_avg:95.72ms
step:507/1770 train_time:48532ms step_avg:95.72ms
step:508/1770 train_time:48629ms step_avg:95.73ms
step:509/1770 train_time:48726ms step_avg:95.73ms
step:510/1770 train_time:48824ms step_avg:95.73ms
step:511/1770 train_time:48920ms step_avg:95.73ms
step:512/1770 train_time:49020ms step_avg:95.74ms
step:513/1770 train_time:49119ms step_avg:95.75ms
step:514/1770 train_time:49218ms step_avg:95.76ms
step:515/1770 train_time:49317ms step_avg:95.76ms
step:516/1770 train_time:49415ms step_avg:95.76ms
step:517/1770 train_time:49513ms step_avg:95.77ms
step:518/1770 train_time:49610ms step_avg:95.77ms
step:519/1770 train_time:49708ms step_avg:95.78ms
step:520/1770 train_time:49805ms step_avg:95.78ms
step:521/1770 train_time:49903ms step_avg:95.78ms
step:522/1770 train_time:50001ms step_avg:95.79ms
step:523/1770 train_time:50099ms step_avg:95.79ms
step:524/1770 train_time:50198ms step_avg:95.80ms
step:525/1770 train_time:50296ms step_avg:95.80ms
step:526/1770 train_time:50394ms step_avg:95.81ms
step:527/1770 train_time:50493ms step_avg:95.81ms
step:528/1770 train_time:50591ms step_avg:95.82ms
step:529/1770 train_time:50690ms step_avg:95.82ms
step:530/1770 train_time:50789ms step_avg:95.83ms
step:531/1770 train_time:50886ms step_avg:95.83ms
step:532/1770 train_time:50985ms step_avg:95.84ms
step:533/1770 train_time:51083ms step_avg:95.84ms
step:534/1770 train_time:51181ms step_avg:95.85ms
step:535/1770 train_time:51280ms step_avg:95.85ms
step:536/1770 train_time:51378ms step_avg:95.85ms
step:537/1770 train_time:51476ms step_avg:95.86ms
step:538/1770 train_time:51574ms step_avg:95.86ms
step:539/1770 train_time:51673ms step_avg:95.87ms
step:540/1770 train_time:51772ms step_avg:95.87ms
step:541/1770 train_time:51870ms step_avg:95.88ms
step:542/1770 train_time:51969ms step_avg:95.88ms
step:543/1770 train_time:52067ms step_avg:95.89ms
step:544/1770 train_time:52165ms step_avg:95.89ms
step:545/1770 train_time:52263ms step_avg:95.89ms
step:546/1770 train_time:52360ms step_avg:95.90ms
step:547/1770 train_time:52459ms step_avg:95.90ms
step:548/1770 train_time:52557ms step_avg:95.91ms
step:549/1770 train_time:52655ms step_avg:95.91ms
step:550/1770 train_time:52754ms step_avg:95.92ms
step:551/1770 train_time:52853ms step_avg:95.92ms
step:552/1770 train_time:52951ms step_avg:95.93ms
step:553/1770 train_time:53049ms step_avg:95.93ms
step:554/1770 train_time:53148ms step_avg:95.93ms
step:555/1770 train_time:53246ms step_avg:95.94ms
step:556/1770 train_time:53344ms step_avg:95.94ms
step:557/1770 train_time:53442ms step_avg:95.95ms
step:558/1770 train_time:53541ms step_avg:95.95ms
step:559/1770 train_time:53639ms step_avg:95.95ms
step:560/1770 train_time:53737ms step_avg:95.96ms
step:561/1770 train_time:53835ms step_avg:95.96ms
step:562/1770 train_time:53933ms step_avg:95.97ms
step:563/1770 train_time:54032ms step_avg:95.97ms
step:564/1770 train_time:54131ms step_avg:95.98ms
step:565/1770 train_time:54230ms step_avg:95.98ms
step:566/1770 train_time:54330ms step_avg:95.99ms
step:567/1770 train_time:54429ms step_avg:95.99ms
step:568/1770 train_time:54527ms step_avg:96.00ms
step:569/1770 train_time:54625ms step_avg:96.00ms
step:570/1770 train_time:54723ms step_avg:96.01ms
step:571/1770 train_time:54822ms step_avg:96.01ms
step:572/1770 train_time:54919ms step_avg:96.01ms
step:573/1770 train_time:55018ms step_avg:96.02ms
step:574/1770 train_time:55116ms step_avg:96.02ms
step:575/1770 train_time:55216ms step_avg:96.03ms
step:576/1770 train_time:55315ms step_avg:96.03ms
step:577/1770 train_time:55415ms step_avg:96.04ms
step:578/1770 train_time:55515ms step_avg:96.05ms
step:579/1770 train_time:55614ms step_avg:96.05ms
step:580/1770 train_time:55713ms step_avg:96.06ms
step:581/1770 train_time:55812ms step_avg:96.06ms
step:582/1770 train_time:55911ms step_avg:96.07ms
step:583/1770 train_time:56009ms step_avg:96.07ms
step:584/1770 train_time:56106ms step_avg:96.07ms
step:585/1770 train_time:56205ms step_avg:96.08ms
step:586/1770 train_time:56303ms step_avg:96.08ms
step:587/1770 train_time:56401ms step_avg:96.08ms
step:588/1770 train_time:56499ms step_avg:96.09ms
step:589/1770 train_time:56597ms step_avg:96.09ms
step:590/1770 train_time:56696ms step_avg:96.09ms
step:591/1770 train_time:56794ms step_avg:96.10ms
step:592/1770 train_time:56892ms step_avg:96.10ms
step:593/1770 train_time:56991ms step_avg:96.11ms
step:594/1770 train_time:57089ms step_avg:96.11ms
step:595/1770 train_time:57188ms step_avg:96.11ms
step:596/1770 train_time:57286ms step_avg:96.12ms
step:597/1770 train_time:57385ms step_avg:96.12ms
step:598/1770 train_time:57483ms step_avg:96.13ms
step:599/1770 train_time:57581ms step_avg:96.13ms
step:600/1770 train_time:57678ms step_avg:96.13ms
step:601/1770 train_time:57777ms step_avg:96.13ms
step:602/1770 train_time:57875ms step_avg:96.14ms
step:603/1770 train_time:57973ms step_avg:96.14ms
step:604/1770 train_time:58072ms step_avg:96.14ms
step:605/1770 train_time:58170ms step_avg:96.15ms
step:606/1770 train_time:58269ms step_avg:96.15ms
step:607/1770 train_time:58366ms step_avg:96.15ms
step:608/1770 train_time:58464ms step_avg:96.16ms
step:609/1770 train_time:58563ms step_avg:96.16ms
step:610/1770 train_time:58660ms step_avg:96.16ms
step:611/1770 train_time:58759ms step_avg:96.17ms
step:612/1770 train_time:58856ms step_avg:96.17ms
step:613/1770 train_time:58954ms step_avg:96.17ms
step:614/1770 train_time:59052ms step_avg:96.18ms
step:615/1770 train_time:59150ms step_avg:96.18ms
step:616/1770 train_time:59248ms step_avg:96.18ms
step:617/1770 train_time:59346ms step_avg:96.18ms
step:618/1770 train_time:59444ms step_avg:96.19ms
step:619/1770 train_time:59542ms step_avg:96.19ms
step:620/1770 train_time:59640ms step_avg:96.19ms
step:621/1770 train_time:59738ms step_avg:96.20ms
step:622/1770 train_time:59837ms step_avg:96.20ms
step:623/1770 train_time:59935ms step_avg:96.20ms
step:624/1770 train_time:60033ms step_avg:96.21ms
step:625/1770 train_time:60132ms step_avg:96.21ms
step:625/1770 val_loss:3.6597 train_time:60229ms step_avg:96.37ms
step:626/1770 train_time:60249ms step_avg:96.24ms
step:627/1770 train_time:60337ms step_avg:96.23ms
step:628/1770 train_time:60437ms step_avg:96.24ms
step:629/1770 train_time:60536ms step_avg:96.24ms
step:630/1770 train_time:60635ms step_avg:96.25ms
step:631/1770 train_time:60733ms step_avg:96.25ms
step:632/1770 train_time:60830ms step_avg:96.25ms
step:633/1770 train_time:60928ms step_avg:96.25ms
step:634/1770 train_time:61026ms step_avg:96.25ms
step:635/1770 train_time:61123ms step_avg:96.26ms
step:636/1770 train_time:61222ms step_avg:96.26ms
step:637/1770 train_time:61322ms step_avg:96.27ms
step:638/1770 train_time:61420ms step_avg:96.27ms
step:639/1770 train_time:61519ms step_avg:96.27ms
step:640/1770 train_time:61618ms step_avg:96.28ms
step:641/1770 train_time:61717ms step_avg:96.28ms
step:642/1770 train_time:61816ms step_avg:96.29ms
step:643/1770 train_time:61915ms step_avg:96.29ms
step:644/1770 train_time:62014ms step_avg:96.29ms
step:645/1770 train_time:62112ms step_avg:96.30ms
step:646/1770 train_time:62209ms step_avg:96.30ms
step:647/1770 train_time:62308ms step_avg:96.30ms
step:648/1770 train_time:62406ms step_avg:96.30ms
step:649/1770 train_time:62504ms step_avg:96.31ms
step:650/1770 train_time:62602ms step_avg:96.31ms
step:651/1770 train_time:62700ms step_avg:96.31ms
step:652/1770 train_time:62798ms step_avg:96.32ms
step:653/1770 train_time:62896ms step_avg:96.32ms
step:654/1770 train_time:62995ms step_avg:96.32ms
step:655/1770 train_time:63094ms step_avg:96.33ms
step:656/1770 train_time:63193ms step_avg:96.33ms
step:657/1770 train_time:63292ms step_avg:96.33ms
step:658/1770 train_time:63391ms step_avg:96.34ms
step:659/1770 train_time:63491ms step_avg:96.34ms
step:660/1770 train_time:63591ms step_avg:96.35ms
step:661/1770 train_time:63691ms step_avg:96.36ms
step:662/1770 train_time:63792ms step_avg:96.36ms
step:663/1770 train_time:63892ms step_avg:96.37ms
step:664/1770 train_time:63992ms step_avg:96.37ms
step:665/1770 train_time:64092ms step_avg:96.38ms
step:666/1770 train_time:64192ms step_avg:96.38ms
step:667/1770 train_time:64292ms step_avg:96.39ms
step:668/1770 train_time:64393ms step_avg:96.40ms
step:669/1770 train_time:64493ms step_avg:96.40ms
step:670/1770 train_time:64593ms step_avg:96.41ms
step:671/1770 train_time:64693ms step_avg:96.41ms
step:672/1770 train_time:64793ms step_avg:96.42ms
step:673/1770 train_time:64892ms step_avg:96.42ms
step:674/1770 train_time:64993ms step_avg:96.43ms
step:675/1770 train_time:65093ms step_avg:96.43ms
step:676/1770 train_time:65192ms step_avg:96.44ms
step:677/1770 train_time:65292ms step_avg:96.44ms
step:678/1770 train_time:65392ms step_avg:96.45ms
step:679/1770 train_time:65492ms step_avg:96.45ms
step:680/1770 train_time:65592ms step_avg:96.46ms
step:681/1770 train_time:65692ms step_avg:96.46ms
step:682/1770 train_time:65792ms step_avg:96.47ms
step:683/1770 train_time:65893ms step_avg:96.48ms
step:684/1770 train_time:65993ms step_avg:96.48ms
step:685/1770 train_time:66093ms step_avg:96.49ms
step:686/1770 train_time:66193ms step_avg:96.49ms
step:687/1770 train_time:66293ms step_avg:96.50ms
step:688/1770 train_time:66393ms step_avg:96.50ms
step:689/1770 train_time:66492ms step_avg:96.51ms
step:690/1770 train_time:66592ms step_avg:96.51ms
step:691/1770 train_time:66692ms step_avg:96.52ms
step:692/1770 train_time:66792ms step_avg:96.52ms
step:693/1770 train_time:66893ms step_avg:96.53ms
step:694/1770 train_time:66993ms step_avg:96.53ms
step:695/1770 train_time:67092ms step_avg:96.54ms
step:696/1770 train_time:67192ms step_avg:96.54ms
step:697/1770 train_time:67292ms step_avg:96.55ms
step:698/1770 train_time:67392ms step_avg:96.55ms
step:699/1770 train_time:67493ms step_avg:96.56ms
step:700/1770 train_time:67593ms step_avg:96.56ms
step:701/1770 train_time:67692ms step_avg:96.57ms
step:702/1770 train_time:67792ms step_avg:96.57ms
step:703/1770 train_time:67892ms step_avg:96.58ms
step:704/1770 train_time:67993ms step_avg:96.58ms
step:705/1770 train_time:68093ms step_avg:96.59ms
step:706/1770 train_time:68193ms step_avg:96.59ms
step:707/1770 train_time:68293ms step_avg:96.60ms
step:708/1770 train_time:68393ms step_avg:96.60ms
step:709/1770 train_time:68493ms step_avg:96.60ms
step:710/1770 train_time:68593ms step_avg:96.61ms
step:711/1770 train_time:68693ms step_avg:96.61ms
step:712/1770 train_time:68793ms step_avg:96.62ms
step:713/1770 train_time:68894ms step_avg:96.63ms
step:714/1770 train_time:68994ms step_avg:96.63ms
step:715/1770 train_time:69094ms step_avg:96.63ms
step:716/1770 train_time:69194ms step_avg:96.64ms
step:717/1770 train_time:69295ms step_avg:96.65ms
step:718/1770 train_time:69395ms step_avg:96.65ms
step:719/1770 train_time:69495ms step_avg:96.66ms
step:720/1770 train_time:69597ms step_avg:96.66ms
step:721/1770 train_time:69697ms step_avg:96.67ms
step:722/1770 train_time:69799ms step_avg:96.67ms
step:723/1770 train_time:69900ms step_avg:96.68ms
step:724/1770 train_time:70001ms step_avg:96.69ms
step:725/1770 train_time:70102ms step_avg:96.69ms
step:726/1770 train_time:70203ms step_avg:96.70ms
step:727/1770 train_time:70302ms step_avg:96.70ms
step:728/1770 train_time:70403ms step_avg:96.71ms
step:729/1770 train_time:70503ms step_avg:96.71ms
step:730/1770 train_time:70603ms step_avg:96.72ms
step:731/1770 train_time:70703ms step_avg:96.72ms
step:732/1770 train_time:70803ms step_avg:96.73ms
step:733/1770 train_time:70903ms step_avg:96.73ms
step:734/1770 train_time:71003ms step_avg:96.73ms
step:735/1770 train_time:71103ms step_avg:96.74ms
step:736/1770 train_time:71203ms step_avg:96.74ms
step:737/1770 train_time:71303ms step_avg:96.75ms
step:738/1770 train_time:71403ms step_avg:96.75ms
step:739/1770 train_time:71503ms step_avg:96.76ms
step:740/1770 train_time:71604ms step_avg:96.76ms
step:741/1770 train_time:71704ms step_avg:96.77ms
step:742/1770 train_time:71803ms step_avg:96.77ms
step:743/1770 train_time:71903ms step_avg:96.77ms
step:744/1770 train_time:72003ms step_avg:96.78ms
step:745/1770 train_time:72102ms step_avg:96.78ms
step:746/1770 train_time:72202ms step_avg:96.79ms
step:747/1770 train_time:72302ms step_avg:96.79ms
step:748/1770 train_time:72402ms step_avg:96.79ms
step:749/1770 train_time:72502ms step_avg:96.80ms
step:750/1770 train_time:72602ms step_avg:96.80ms
step:750/1770 val_loss:3.5981 train_time:72702ms step_avg:96.94ms
step:751/1770 train_time:72719ms step_avg:96.83ms
step:752/1770 train_time:72815ms step_avg:96.83ms
step:753/1770 train_time:72915ms step_avg:96.83ms
step:754/1770 train_time:73015ms step_avg:96.84ms
step:755/1770 train_time:73115ms step_avg:96.84ms
step:756/1770 train_time:73213ms step_avg:96.84ms
step:757/1770 train_time:73313ms step_avg:96.85ms
step:758/1770 train_time:73412ms step_avg:96.85ms
step:759/1770 train_time:73511ms step_avg:96.85ms
step:760/1770 train_time:73611ms step_avg:96.86ms
step:761/1770 train_time:73712ms step_avg:96.86ms
step:762/1770 train_time:73815ms step_avg:96.87ms
step:763/1770 train_time:73916ms step_avg:96.88ms
step:764/1770 train_time:74016ms step_avg:96.88ms
step:765/1770 train_time:74116ms step_avg:96.88ms
step:766/1770 train_time:74215ms step_avg:96.89ms
step:767/1770 train_time:74314ms step_avg:96.89ms
step:768/1770 train_time:74414ms step_avg:96.89ms
step:769/1770 train_time:74513ms step_avg:96.90ms
step:770/1770 train_time:74612ms step_avg:96.90ms
step:771/1770 train_time:74712ms step_avg:96.90ms
step:772/1770 train_time:74814ms step_avg:96.91ms
step:773/1770 train_time:74914ms step_avg:96.91ms
step:774/1770 train_time:75015ms step_avg:96.92ms
step:775/1770 train_time:75115ms step_avg:96.92ms
step:776/1770 train_time:75215ms step_avg:96.93ms
step:777/1770 train_time:75314ms step_avg:96.93ms
step:778/1770 train_time:75414ms step_avg:96.93ms
step:779/1770 train_time:75513ms step_avg:96.94ms
step:780/1770 train_time:75612ms step_avg:96.94ms
step:781/1770 train_time:75711ms step_avg:96.94ms
step:782/1770 train_time:75811ms step_avg:96.95ms
step:783/1770 train_time:75912ms step_avg:96.95ms
step:784/1770 train_time:76013ms step_avg:96.96ms
step:785/1770 train_time:76113ms step_avg:96.96ms
step:786/1770 train_time:76214ms step_avg:96.96ms
step:787/1770 train_time:76314ms step_avg:96.97ms
step:788/1770 train_time:76413ms step_avg:96.97ms
step:789/1770 train_time:76513ms step_avg:96.97ms
step:790/1770 train_time:76613ms step_avg:96.98ms
step:791/1770 train_time:76713ms step_avg:96.98ms
step:792/1770 train_time:76813ms step_avg:96.99ms
step:793/1770 train_time:76913ms step_avg:96.99ms
step:794/1770 train_time:77013ms step_avg:96.99ms
step:795/1770 train_time:77114ms step_avg:97.00ms
step:796/1770 train_time:77214ms step_avg:97.00ms
step:797/1770 train_time:77315ms step_avg:97.01ms
step:798/1770 train_time:77415ms step_avg:97.01ms
step:799/1770 train_time:77515ms step_avg:97.02ms
step:800/1770 train_time:77615ms step_avg:97.02ms
step:801/1770 train_time:77715ms step_avg:97.02ms
step:802/1770 train_time:77816ms step_avg:97.03ms
step:803/1770 train_time:77917ms step_avg:97.03ms
step:804/1770 train_time:78017ms step_avg:97.04ms
step:805/1770 train_time:78120ms step_avg:97.04ms
step:806/1770 train_time:78221ms step_avg:97.05ms
step:807/1770 train_time:78321ms step_avg:97.05ms
step:808/1770 train_time:78422ms step_avg:97.06ms
step:809/1770 train_time:78523ms step_avg:97.06ms
step:810/1770 train_time:78623ms step_avg:97.07ms
step:811/1770 train_time:78723ms step_avg:97.07ms
step:812/1770 train_time:78824ms step_avg:97.07ms
step:813/1770 train_time:78924ms step_avg:97.08ms
step:814/1770 train_time:79025ms step_avg:97.08ms
step:815/1770 train_time:79126ms step_avg:97.09ms
step:816/1770 train_time:79227ms step_avg:97.09ms
step:817/1770 train_time:79327ms step_avg:97.10ms
step:818/1770 train_time:79428ms step_avg:97.10ms
step:819/1770 train_time:79528ms step_avg:97.10ms
step:820/1770 train_time:79629ms step_avg:97.11ms
step:821/1770 train_time:79731ms step_avg:97.11ms
step:822/1770 train_time:79832ms step_avg:97.12ms
step:823/1770 train_time:79933ms step_avg:97.12ms
step:824/1770 train_time:80034ms step_avg:97.13ms
step:825/1770 train_time:80134ms step_avg:97.13ms
step:826/1770 train_time:80234ms step_avg:97.14ms
step:827/1770 train_time:80334ms step_avg:97.14ms
step:828/1770 train_time:80434ms step_avg:97.14ms
step:829/1770 train_time:80534ms step_avg:97.15ms
step:830/1770 train_time:80634ms step_avg:97.15ms
step:831/1770 train_time:80735ms step_avg:97.15ms
step:832/1770 train_time:80835ms step_avg:97.16ms
step:833/1770 train_time:80936ms step_avg:97.16ms
step:834/1770 train_time:81037ms step_avg:97.17ms
step:835/1770 train_time:81136ms step_avg:97.17ms
step:836/1770 train_time:81236ms step_avg:97.17ms
step:837/1770 train_time:81337ms step_avg:97.18ms
step:838/1770 train_time:81438ms step_avg:97.18ms
step:839/1770 train_time:81538ms step_avg:97.18ms
step:840/1770 train_time:81638ms step_avg:97.19ms
step:841/1770 train_time:81739ms step_avg:97.19ms
step:842/1770 train_time:81839ms step_avg:97.20ms
step:843/1770 train_time:81939ms step_avg:97.20ms
step:844/1770 train_time:82041ms step_avg:97.20ms
step:845/1770 train_time:82140ms step_avg:97.21ms
step:846/1770 train_time:82241ms step_avg:97.21ms
step:847/1770 train_time:82341ms step_avg:97.21ms
step:848/1770 train_time:82441ms step_avg:97.22ms
step:849/1770 train_time:82542ms step_avg:97.22ms
step:850/1770 train_time:82643ms step_avg:97.23ms
step:851/1770 train_time:82743ms step_avg:97.23ms
step:852/1770 train_time:82843ms step_avg:97.23ms
step:853/1770 train_time:82943ms step_avg:97.24ms
step:854/1770 train_time:83043ms step_avg:97.24ms
step:855/1770 train_time:83143ms step_avg:97.24ms
step:856/1770 train_time:83243ms step_avg:97.25ms
step:857/1770 train_time:83343ms step_avg:97.25ms
step:858/1770 train_time:83443ms step_avg:97.25ms
step:859/1770 train_time:83543ms step_avg:97.26ms
step:860/1770 train_time:83644ms step_avg:97.26ms
step:861/1770 train_time:83744ms step_avg:97.26ms
step:862/1770 train_time:83844ms step_avg:97.27ms
step:863/1770 train_time:83944ms step_avg:97.27ms
step:864/1770 train_time:84045ms step_avg:97.27ms
step:865/1770 train_time:84145ms step_avg:97.28ms
step:866/1770 train_time:84246ms step_avg:97.28ms
step:867/1770 train_time:84347ms step_avg:97.29ms
step:868/1770 train_time:84448ms step_avg:97.29ms
step:869/1770 train_time:84549ms step_avg:97.29ms
step:870/1770 train_time:84649ms step_avg:97.30ms
step:871/1770 train_time:84750ms step_avg:97.30ms
step:872/1770 train_time:84852ms step_avg:97.31ms
step:873/1770 train_time:84953ms step_avg:97.31ms
step:874/1770 train_time:85054ms step_avg:97.32ms
step:875/1770 train_time:85154ms step_avg:97.32ms
step:875/1770 val_loss:3.5502 train_time:85253ms step_avg:97.43ms
step:876/1770 train_time:85271ms step_avg:97.34ms
step:877/1770 train_time:85361ms step_avg:97.33ms
step:878/1770 train_time:85465ms step_avg:97.34ms
step:879/1770 train_time:85565ms step_avg:97.34ms
step:880/1770 train_time:85665ms step_avg:97.35ms
step:881/1770 train_time:85764ms step_avg:97.35ms
step:882/1770 train_time:85863ms step_avg:97.35ms
step:883/1770 train_time:85963ms step_avg:97.35ms
step:884/1770 train_time:86062ms step_avg:97.36ms
step:885/1770 train_time:86162ms step_avg:97.36ms
step:886/1770 train_time:86263ms step_avg:97.36ms
step:887/1770 train_time:86365ms step_avg:97.37ms
step:888/1770 train_time:86467ms step_avg:97.37ms
step:889/1770 train_time:86567ms step_avg:97.38ms
step:890/1770 train_time:86668ms step_avg:97.38ms
step:891/1770 train_time:86769ms step_avg:97.38ms
step:892/1770 train_time:86869ms step_avg:97.39ms
step:893/1770 train_time:86969ms step_avg:97.39ms
step:894/1770 train_time:87069ms step_avg:97.39ms
step:895/1770 train_time:87170ms step_avg:97.40ms
step:896/1770 train_time:87271ms step_avg:97.40ms
step:897/1770 train_time:87372ms step_avg:97.40ms
step:898/1770 train_time:87473ms step_avg:97.41ms
step:899/1770 train_time:87574ms step_avg:97.41ms
step:900/1770 train_time:87674ms step_avg:97.42ms
step:901/1770 train_time:87775ms step_avg:97.42ms
step:902/1770 train_time:87877ms step_avg:97.42ms
step:903/1770 train_time:87977ms step_avg:97.43ms
step:904/1770 train_time:88078ms step_avg:97.43ms
step:905/1770 train_time:88180ms step_avg:97.44ms
step:906/1770 train_time:88282ms step_avg:97.44ms
step:907/1770 train_time:88383ms step_avg:97.45ms
step:908/1770 train_time:88483ms step_avg:97.45ms
step:909/1770 train_time:88583ms step_avg:97.45ms
step:910/1770 train_time:88683ms step_avg:97.45ms
step:911/1770 train_time:88783ms step_avg:97.46ms
step:912/1770 train_time:88884ms step_avg:97.46ms
step:913/1770 train_time:88983ms step_avg:97.46ms
step:914/1770 train_time:89083ms step_avg:97.47ms
step:915/1770 train_time:89184ms step_avg:97.47ms
step:916/1770 train_time:89284ms step_avg:97.47ms
step:917/1770 train_time:89385ms step_avg:97.47ms
step:918/1770 train_time:89485ms step_avg:97.48ms
step:919/1770 train_time:89585ms step_avg:97.48ms
step:920/1770 train_time:89687ms step_avg:97.49ms
step:921/1770 train_time:89788ms step_avg:97.49ms
step:922/1770 train_time:89890ms step_avg:97.49ms
step:923/1770 train_time:89992ms step_avg:97.50ms
step:924/1770 train_time:90093ms step_avg:97.50ms
step:925/1770 train_time:90195ms step_avg:97.51ms
step:926/1770 train_time:90298ms step_avg:97.51ms
step:927/1770 train_time:90400ms step_avg:97.52ms
step:928/1770 train_time:90503ms step_avg:97.52ms
step:929/1770 train_time:90605ms step_avg:97.53ms
step:930/1770 train_time:90706ms step_avg:97.53ms
step:931/1770 train_time:90807ms step_avg:97.54ms
step:932/1770 train_time:90909ms step_avg:97.54ms
step:933/1770 train_time:91011ms step_avg:97.55ms
step:934/1770 train_time:91113ms step_avg:97.55ms
step:935/1770 train_time:91215ms step_avg:97.56ms
step:936/1770 train_time:91316ms step_avg:97.56ms
step:937/1770 train_time:91418ms step_avg:97.57ms
step:938/1770 train_time:91522ms step_avg:97.57ms
step:939/1770 train_time:91624ms step_avg:97.58ms
step:940/1770 train_time:91726ms step_avg:97.58ms
step:941/1770 train_time:91827ms step_avg:97.58ms
step:942/1770 train_time:91928ms step_avg:97.59ms
step:943/1770 train_time:92031ms step_avg:97.59ms
step:944/1770 train_time:92133ms step_avg:97.60ms
step:945/1770 train_time:92235ms step_avg:97.60ms
step:946/1770 train_time:92336ms step_avg:97.61ms
step:947/1770 train_time:92439ms step_avg:97.61ms
step:948/1770 train_time:92542ms step_avg:97.62ms
step:949/1770 train_time:92644ms step_avg:97.62ms
step:950/1770 train_time:92746ms step_avg:97.63ms
step:951/1770 train_time:92847ms step_avg:97.63ms
step:952/1770 train_time:92948ms step_avg:97.63ms
step:953/1770 train_time:93050ms step_avg:97.64ms
step:954/1770 train_time:93151ms step_avg:97.64ms
step:955/1770 train_time:93253ms step_avg:97.65ms
step:956/1770 train_time:93355ms step_avg:97.65ms
step:957/1770 train_time:93457ms step_avg:97.66ms
step:958/1770 train_time:93559ms step_avg:97.66ms
step:959/1770 train_time:93662ms step_avg:97.67ms
step:960/1770 train_time:93764ms step_avg:97.67ms
step:961/1770 train_time:93866ms step_avg:97.67ms
step:962/1770 train_time:93968ms step_avg:97.68ms
step:963/1770 train_time:94070ms step_avg:97.68ms
step:964/1770 train_time:94172ms step_avg:97.69ms
step:965/1770 train_time:94273ms step_avg:97.69ms
step:966/1770 train_time:94374ms step_avg:97.70ms
step:967/1770 train_time:94476ms step_avg:97.70ms
step:968/1770 train_time:94579ms step_avg:97.71ms
step:969/1770 train_time:94682ms step_avg:97.71ms
step:970/1770 train_time:94785ms step_avg:97.72ms
step:971/1770 train_time:94887ms step_avg:97.72ms
step:972/1770 train_time:94988ms step_avg:97.72ms
step:973/1770 train_time:95089ms step_avg:97.73ms
step:974/1770 train_time:95190ms step_avg:97.73ms
step:975/1770 train_time:95291ms step_avg:97.73ms
step:976/1770 train_time:95393ms step_avg:97.74ms
step:977/1770 train_time:95495ms step_avg:97.74ms
step:978/1770 train_time:95597ms step_avg:97.75ms
step:979/1770 train_time:95701ms step_avg:97.75ms
step:980/1770 train_time:95804ms step_avg:97.76ms
step:981/1770 train_time:95905ms step_avg:97.76ms
step:982/1770 train_time:96007ms step_avg:97.77ms
step:983/1770 train_time:96108ms step_avg:97.77ms
step:984/1770 train_time:96210ms step_avg:97.77ms
step:985/1770 train_time:96311ms step_avg:97.78ms
step:986/1770 train_time:96413ms step_avg:97.78ms
step:987/1770 train_time:96515ms step_avg:97.79ms
step:988/1770 train_time:96616ms step_avg:97.79ms
step:989/1770 train_time:96721ms step_avg:97.80ms
step:990/1770 train_time:96824ms step_avg:97.80ms
step:991/1770 train_time:96925ms step_avg:97.81ms
step:992/1770 train_time:97026ms step_avg:97.81ms
step:993/1770 train_time:97127ms step_avg:97.81ms
step:994/1770 train_time:97229ms step_avg:97.82ms
step:995/1770 train_time:97330ms step_avg:97.82ms
step:996/1770 train_time:97432ms step_avg:97.82ms
step:997/1770 train_time:97534ms step_avg:97.83ms
step:998/1770 train_time:97635ms step_avg:97.83ms
step:999/1770 train_time:97738ms step_avg:97.84ms
step:1000/1770 train_time:97842ms step_avg:97.84ms
step:1000/1770 val_loss:3.5117 train_time:97943ms step_avg:97.94ms
step:1001/1770 train_time:97961ms step_avg:97.86ms
step:1002/1770 train_time:98053ms step_avg:97.86ms
step:1003/1770 train_time:98156ms step_avg:97.86ms
step:1004/1770 train_time:98259ms step_avg:97.87ms
step:1005/1770 train_time:98360ms step_avg:97.87ms
step:1006/1770 train_time:98461ms step_avg:97.87ms
step:1007/1770 train_time:98561ms step_avg:97.88ms
step:1008/1770 train_time:98662ms step_avg:97.88ms
step:1009/1770 train_time:98762ms step_avg:97.88ms
step:1010/1770 train_time:98863ms step_avg:97.88ms
step:1011/1770 train_time:98967ms step_avg:97.89ms
step:1012/1770 train_time:99071ms step_avg:97.90ms
step:1013/1770 train_time:99173ms step_avg:97.90ms
step:1014/1770 train_time:99275ms step_avg:97.90ms
step:1015/1770 train_time:99377ms step_avg:97.91ms
step:1016/1770 train_time:99479ms step_avg:97.91ms
step:1017/1770 train_time:99581ms step_avg:97.92ms
step:1018/1770 train_time:99682ms step_avg:97.92ms
step:1019/1770 train_time:99783ms step_avg:97.92ms
step:1020/1770 train_time:99884ms step_avg:97.93ms
step:1021/1770 train_time:99987ms step_avg:97.93ms
step:1022/1770 train_time:100090ms step_avg:97.94ms
step:1023/1770 train_time:100192ms step_avg:97.94ms
step:1024/1770 train_time:100295ms step_avg:97.94ms
step:1025/1770 train_time:100398ms step_avg:97.95ms
step:1026/1770 train_time:100501ms step_avg:97.95ms
step:1027/1770 train_time:100601ms step_avg:97.96ms
step:1028/1770 train_time:100703ms step_avg:97.96ms
step:1029/1770 train_time:100804ms step_avg:97.96ms
step:1030/1770 train_time:100905ms step_avg:97.97ms
step:1031/1770 train_time:101007ms step_avg:97.97ms
step:1032/1770 train_time:101109ms step_avg:97.97ms
step:1033/1770 train_time:101211ms step_avg:97.98ms
step:1034/1770 train_time:101314ms step_avg:97.98ms
step:1035/1770 train_time:101419ms step_avg:97.99ms
step:1036/1770 train_time:101522ms step_avg:97.99ms
step:1037/1770 train_time:101623ms step_avg:98.00ms
step:1038/1770 train_time:101724ms step_avg:98.00ms
step:1039/1770 train_time:101825ms step_avg:98.00ms
step:1040/1770 train_time:101926ms step_avg:98.01ms
step:1041/1770 train_time:102027ms step_avg:98.01ms
step:1042/1770 train_time:102129ms step_avg:98.01ms
step:1043/1770 train_time:102232ms step_avg:98.02ms
step:1044/1770 train_time:102335ms step_avg:98.02ms
step:1045/1770 train_time:102437ms step_avg:98.03ms
step:1046/1770 train_time:102540ms step_avg:98.03ms
step:1047/1770 train_time:102643ms step_avg:98.04ms
step:1048/1770 train_time:102745ms step_avg:98.04ms
step:1049/1770 train_time:102847ms step_avg:98.04ms
step:1050/1770 train_time:102948ms step_avg:98.05ms
step:1051/1770 train_time:103049ms step_avg:98.05ms
step:1052/1770 train_time:103151ms step_avg:98.05ms
step:1053/1770 train_time:103253ms step_avg:98.06ms
step:1054/1770 train_time:103356ms step_avg:98.06ms
step:1055/1770 train_time:103459ms step_avg:98.07ms
step:1056/1770 train_time:103561ms step_avg:98.07ms
step:1057/1770 train_time:103663ms step_avg:98.07ms
step:1058/1770 train_time:103766ms step_avg:98.08ms
step:1059/1770 train_time:103868ms step_avg:98.08ms
step:1060/1770 train_time:103969ms step_avg:98.08ms
step:1061/1770 train_time:104071ms step_avg:98.09ms
step:1062/1770 train_time:104174ms step_avg:98.09ms
step:1063/1770 train_time:104277ms step_avg:98.10ms
step:1064/1770 train_time:104380ms step_avg:98.10ms
step:1065/1770 train_time:104483ms step_avg:98.11ms
step:1066/1770 train_time:104585ms step_avg:98.11ms
step:1067/1770 train_time:104688ms step_avg:98.11ms
step:1068/1770 train_time:104790ms step_avg:98.12ms
step:1069/1770 train_time:104893ms step_avg:98.12ms
step:1070/1770 train_time:104995ms step_avg:98.13ms
step:1071/1770 train_time:105097ms step_avg:98.13ms
step:1072/1770 train_time:105199ms step_avg:98.13ms
step:1073/1770 train_time:105300ms step_avg:98.14ms
step:1074/1770 train_time:105402ms step_avg:98.14ms
step:1075/1770 train_time:105504ms step_avg:98.14ms
step:1076/1770 train_time:105606ms step_avg:98.15ms
step:1077/1770 train_time:105709ms step_avg:98.15ms
step:1078/1770 train_time:105810ms step_avg:98.15ms
step:1079/1770 train_time:105912ms step_avg:98.16ms
step:1080/1770 train_time:106014ms step_avg:98.16ms
step:1081/1770 train_time:106117ms step_avg:98.17ms
step:1082/1770 train_time:106219ms step_avg:98.17ms
step:1083/1770 train_time:106321ms step_avg:98.17ms
step:1084/1770 train_time:106423ms step_avg:98.18ms
step:1085/1770 train_time:106524ms step_avg:98.18ms
step:1086/1770 train_time:106625ms step_avg:98.18ms
step:1087/1770 train_time:106726ms step_avg:98.18ms
step:1088/1770 train_time:106829ms step_avg:98.19ms
step:1089/1770 train_time:106931ms step_avg:98.19ms
step:1090/1770 train_time:107033ms step_avg:98.20ms
step:1091/1770 train_time:107136ms step_avg:98.20ms
step:1092/1770 train_time:107239ms step_avg:98.20ms
step:1093/1770 train_time:107341ms step_avg:98.21ms
step:1094/1770 train_time:107444ms step_avg:98.21ms
step:1095/1770 train_time:107545ms step_avg:98.21ms
step:1096/1770 train_time:107646ms step_avg:98.22ms
step:1097/1770 train_time:107748ms step_avg:98.22ms
step:1098/1770 train_time:107850ms step_avg:98.22ms
step:1099/1770 train_time:107952ms step_avg:98.23ms
step:1100/1770 train_time:108056ms step_avg:98.23ms
step:1101/1770 train_time:108159ms step_avg:98.24ms
step:1102/1770 train_time:108261ms step_avg:98.24ms
step:1103/1770 train_time:108363ms step_avg:98.24ms
step:1104/1770 train_time:108465ms step_avg:98.25ms
step:1105/1770 train_time:108567ms step_avg:98.25ms
step:1106/1770 train_time:108668ms step_avg:98.25ms
step:1107/1770 train_time:108770ms step_avg:98.26ms
step:1108/1770 train_time:108872ms step_avg:98.26ms
step:1109/1770 train_time:108974ms step_avg:98.26ms
step:1110/1770 train_time:109077ms step_avg:98.27ms
step:1111/1770 train_time:109180ms step_avg:98.27ms
step:1112/1770 train_time:109283ms step_avg:98.28ms
step:1113/1770 train_time:109385ms step_avg:98.28ms
step:1114/1770 train_time:109487ms step_avg:98.28ms
step:1115/1770 train_time:109589ms step_avg:98.29ms
step:1116/1770 train_time:109690ms step_avg:98.29ms
step:1117/1770 train_time:109792ms step_avg:98.29ms
step:1118/1770 train_time:109893ms step_avg:98.29ms
step:1119/1770 train_time:109996ms step_avg:98.30ms
step:1120/1770 train_time:110099ms step_avg:98.30ms
step:1121/1770 train_time:110202ms step_avg:98.31ms
step:1122/1770 train_time:110304ms step_avg:98.31ms
step:1123/1770 train_time:110406ms step_avg:98.31ms
step:1124/1770 train_time:110507ms step_avg:98.32ms
step:1125/1770 train_time:110609ms step_avg:98.32ms
step:1125/1770 val_loss:3.4719 train_time:110711ms step_avg:98.41ms
step:1126/1770 train_time:110728ms step_avg:98.34ms
step:1127/1770 train_time:110820ms step_avg:98.33ms
step:1128/1770 train_time:110924ms step_avg:98.34ms
step:1129/1770 train_time:111028ms step_avg:98.34ms
step:1130/1770 train_time:111129ms step_avg:98.34ms
step:1131/1770 train_time:111230ms step_avg:98.35ms
step:1132/1770 train_time:111331ms step_avg:98.35ms
step:1133/1770 train_time:111433ms step_avg:98.35ms
step:1134/1770 train_time:111534ms step_avg:98.35ms
step:1135/1770 train_time:111635ms step_avg:98.36ms
step:1136/1770 train_time:111739ms step_avg:98.36ms
step:1137/1770 train_time:111841ms step_avg:98.37ms
step:1138/1770 train_time:111945ms step_avg:98.37ms
step:1139/1770 train_time:112048ms step_avg:98.37ms
step:1140/1770 train_time:112149ms step_avg:98.38ms
step:1141/1770 train_time:112250ms step_avg:98.38ms
step:1142/1770 train_time:112352ms step_avg:98.38ms
step:1143/1770 train_time:112452ms step_avg:98.38ms
step:1144/1770 train_time:112554ms step_avg:98.39ms
step:1145/1770 train_time:112655ms step_avg:98.39ms
step:1146/1770 train_time:112760ms step_avg:98.39ms
step:1147/1770 train_time:112862ms step_avg:98.40ms
step:1148/1770 train_time:112964ms step_avg:98.40ms
step:1149/1770 train_time:113067ms step_avg:98.40ms
step:1150/1770 train_time:113169ms step_avg:98.41ms
step:1151/1770 train_time:113271ms step_avg:98.41ms
step:1152/1770 train_time:113373ms step_avg:98.41ms
step:1153/1770 train_time:113476ms step_avg:98.42ms
step:1154/1770 train_time:113578ms step_avg:98.42ms
step:1155/1770 train_time:113679ms step_avg:98.42ms
step:1156/1770 train_time:113781ms step_avg:98.43ms
step:1157/1770 train_time:113884ms step_avg:98.43ms
step:1158/1770 train_time:113987ms step_avg:98.43ms
step:1159/1770 train_time:114089ms step_avg:98.44ms
step:1160/1770 train_time:114191ms step_avg:98.44ms
step:1161/1770 train_time:114292ms step_avg:98.44ms
step:1162/1770 train_time:114394ms step_avg:98.45ms
step:1163/1770 train_time:114496ms step_avg:98.45ms
step:1164/1770 train_time:114598ms step_avg:98.45ms
step:1165/1770 train_time:114699ms step_avg:98.45ms
step:1166/1770 train_time:114801ms step_avg:98.46ms
step:1167/1770 train_time:114904ms step_avg:98.46ms
step:1168/1770 train_time:115007ms step_avg:98.46ms
step:1169/1770 train_time:115109ms step_avg:98.47ms
step:1170/1770 train_time:115211ms step_avg:98.47ms
step:1171/1770 train_time:115313ms step_avg:98.47ms
step:1172/1770 train_time:115415ms step_avg:98.48ms
step:1173/1770 train_time:115517ms step_avg:98.48ms
step:1174/1770 train_time:115619ms step_avg:98.48ms
step:1175/1770 train_time:115721ms step_avg:98.49ms
step:1176/1770 train_time:115824ms step_avg:98.49ms
step:1177/1770 train_time:115926ms step_avg:98.49ms
step:1178/1770 train_time:116028ms step_avg:98.50ms
step:1179/1770 train_time:116130ms step_avg:98.50ms
step:1180/1770 train_time:116232ms step_avg:98.50ms
step:1181/1770 train_time:116335ms step_avg:98.51ms
step:1182/1770 train_time:116437ms step_avg:98.51ms
step:1183/1770 train_time:116540ms step_avg:98.51ms
step:1184/1770 train_time:116644ms step_avg:98.52ms
step:1185/1770 train_time:116746ms step_avg:98.52ms
step:1186/1770 train_time:116850ms step_avg:98.52ms
step:1187/1770 train_time:116955ms step_avg:98.53ms
step:1188/1770 train_time:117059ms step_avg:98.53ms
step:1189/1770 train_time:117163ms step_avg:98.54ms
step:1190/1770 train_time:117267ms step_avg:98.54ms
step:1191/1770 train_time:117371ms step_avg:98.55ms
step:1192/1770 train_time:117473ms step_avg:98.55ms
step:1193/1770 train_time:117576ms step_avg:98.55ms
step:1194/1770 train_time:117680ms step_avg:98.56ms
step:1195/1770 train_time:117784ms step_avg:98.56ms
step:1196/1770 train_time:117889ms step_avg:98.57ms
step:1197/1770 train_time:117991ms step_avg:98.57ms
step:1198/1770 train_time:118094ms step_avg:98.58ms
step:1199/1770 train_time:118198ms step_avg:98.58ms
step:1200/1770 train_time:118302ms step_avg:98.58ms
step:1201/1770 train_time:118405ms step_avg:98.59ms
step:1202/1770 train_time:118509ms step_avg:98.59ms
step:1203/1770 train_time:118611ms step_avg:98.60ms
step:1204/1770 train_time:118715ms step_avg:98.60ms
step:1205/1770 train_time:118819ms step_avg:98.61ms
step:1206/1770 train_time:118924ms step_avg:98.61ms
step:1207/1770 train_time:119027ms step_avg:98.61ms
step:1208/1770 train_time:119130ms step_avg:98.62ms
step:1209/1770 train_time:119232ms step_avg:98.62ms
step:1210/1770 train_time:119336ms step_avg:98.63ms
step:1211/1770 train_time:119441ms step_avg:98.63ms
step:1212/1770 train_time:119545ms step_avg:98.63ms
step:1213/1770 train_time:119649ms step_avg:98.64ms
step:1214/1770 train_time:119751ms step_avg:98.64ms
step:1215/1770 train_time:119854ms step_avg:98.65ms
step:1216/1770 train_time:119958ms step_avg:98.65ms
step:1217/1770 train_time:120060ms step_avg:98.65ms
step:1218/1770 train_time:120164ms step_avg:98.66ms
step:1219/1770 train_time:120269ms step_avg:98.66ms
step:1220/1770 train_time:120373ms step_avg:98.67ms
step:1221/1770 train_time:120476ms step_avg:98.67ms
step:1222/1770 train_time:120581ms step_avg:98.67ms
step:1223/1770 train_time:120685ms step_avg:98.68ms
step:1224/1770 train_time:120790ms step_avg:98.68ms
step:1225/1770 train_time:120894ms step_avg:98.69ms
step:1226/1770 train_time:120996ms step_avg:98.69ms
step:1227/1770 train_time:121101ms step_avg:98.70ms
step:1228/1770 train_time:121208ms step_avg:98.70ms
step:1229/1770 train_time:121311ms step_avg:98.71ms
step:1230/1770 train_time:121415ms step_avg:98.71ms
step:1231/1770 train_time:121518ms step_avg:98.71ms
step:1232/1770 train_time:121620ms step_avg:98.72ms
step:1233/1770 train_time:121725ms step_avg:98.72ms
step:1234/1770 train_time:121829ms step_avg:98.73ms
step:1235/1770 train_time:121932ms step_avg:98.73ms
step:1236/1770 train_time:122035ms step_avg:98.73ms
step:1237/1770 train_time:122138ms step_avg:98.74ms
step:1238/1770 train_time:122242ms step_avg:98.74ms
step:1239/1770 train_time:122347ms step_avg:98.75ms
step:1240/1770 train_time:122450ms step_avg:98.75ms
step:1241/1770 train_time:122553ms step_avg:98.75ms
step:1242/1770 train_time:122657ms step_avg:98.76ms
step:1243/1770 train_time:122761ms step_avg:98.76ms
step:1244/1770 train_time:122865ms step_avg:98.77ms
step:1245/1770 train_time:122967ms step_avg:98.77ms
step:1246/1770 train_time:123071ms step_avg:98.77ms
step:1247/1770 train_time:123174ms step_avg:98.78ms
step:1248/1770 train_time:123277ms step_avg:98.78ms
step:1249/1770 train_time:123380ms step_avg:98.78ms
step:1250/1770 train_time:123483ms step_avg:98.79ms
step:1250/1770 val_loss:3.4244 train_time:123587ms step_avg:98.87ms
step:1251/1770 train_time:123603ms step_avg:98.80ms
step:1252/1770 train_time:123702ms step_avg:98.80ms
step:1253/1770 train_time:123805ms step_avg:98.81ms
step:1254/1770 train_time:123907ms step_avg:98.81ms
step:1255/1770 train_time:124012ms step_avg:98.81ms
step:1256/1770 train_time:124114ms step_avg:98.82ms
step:1257/1770 train_time:124217ms step_avg:98.82ms
step:1258/1770 train_time:124320ms step_avg:98.82ms
step:1259/1770 train_time:124423ms step_avg:98.83ms
step:1260/1770 train_time:124527ms step_avg:98.83ms
step:1261/1770 train_time:124633ms step_avg:98.84ms
step:1262/1770 train_time:124738ms step_avg:98.84ms
step:1263/1770 train_time:124842ms step_avg:98.85ms
step:1264/1770 train_time:124947ms step_avg:98.85ms
step:1265/1770 train_time:125049ms step_avg:98.85ms
step:1266/1770 train_time:125152ms step_avg:98.86ms
step:1267/1770 train_time:125255ms step_avg:98.86ms
step:1268/1770 train_time:125358ms step_avg:98.86ms
step:1269/1770 train_time:125461ms step_avg:98.87ms
step:1270/1770 train_time:125565ms step_avg:98.87ms
step:1271/1770 train_time:125670ms step_avg:98.88ms
step:1272/1770 train_time:125773ms step_avg:98.88ms
step:1273/1770 train_time:125878ms step_avg:98.88ms
step:1274/1770 train_time:125982ms step_avg:98.89ms
step:1275/1770 train_time:126086ms step_avg:98.89ms
step:1276/1770 train_time:126190ms step_avg:98.89ms
step:1277/1770 train_time:126292ms step_avg:98.90ms
step:1278/1770 train_time:126395ms step_avg:98.90ms
step:1279/1770 train_time:126497ms step_avg:98.90ms
step:1280/1770 train_time:126602ms step_avg:98.91ms
step:1281/1770 train_time:126705ms step_avg:98.91ms
step:1282/1770 train_time:126809ms step_avg:98.92ms
step:1283/1770 train_time:126912ms step_avg:98.92ms
step:1284/1770 train_time:127016ms step_avg:98.92ms
step:1285/1770 train_time:127119ms step_avg:98.93ms
step:1286/1770 train_time:127222ms step_avg:98.93ms
step:1287/1770 train_time:127326ms step_avg:98.93ms
step:1288/1770 train_time:127430ms step_avg:98.94ms
step:1289/1770 train_time:127532ms step_avg:98.94ms
step:1290/1770 train_time:127634ms step_avg:98.94ms
step:1291/1770 train_time:127738ms step_avg:98.94ms
step:1292/1770 train_time:127842ms step_avg:98.95ms
step:1293/1770 train_time:127946ms step_avg:98.95ms
step:1294/1770 train_time:128050ms step_avg:98.96ms
step:1295/1770 train_time:128154ms step_avg:98.96ms
step:1296/1770 train_time:128256ms step_avg:98.96ms
step:1297/1770 train_time:128360ms step_avg:98.97ms
step:1298/1770 train_time:128464ms step_avg:98.97ms
step:1299/1770 train_time:128567ms step_avg:98.97ms
step:1300/1770 train_time:128671ms step_avg:98.98ms
step:1301/1770 train_time:128774ms step_avg:98.98ms
step:1302/1770 train_time:128877ms step_avg:98.98ms
step:1303/1770 train_time:128981ms step_avg:98.99ms
step:1304/1770 train_time:129086ms step_avg:98.99ms
step:1305/1770 train_time:129190ms step_avg:99.00ms
step:1306/1770 train_time:129293ms step_avg:99.00ms
step:1307/1770 train_time:129395ms step_avg:99.00ms
step:1308/1770 train_time:129498ms step_avg:99.00ms
step:1309/1770 train_time:129601ms step_avg:99.01ms
step:1310/1770 train_time:129705ms step_avg:99.01ms
step:1311/1770 train_time:129808ms step_avg:99.01ms
step:1312/1770 train_time:129911ms step_avg:99.02ms
step:1313/1770 train_time:130013ms step_avg:99.02ms
step:1314/1770 train_time:130117ms step_avg:99.02ms
step:1315/1770 train_time:130220ms step_avg:99.03ms
step:1316/1770 train_time:130323ms step_avg:99.03ms
step:1317/1770 train_time:130428ms step_avg:99.03ms
step:1318/1770 train_time:130534ms step_avg:99.04ms
step:1319/1770 train_time:130637ms step_avg:99.04ms
step:1320/1770 train_time:130740ms step_avg:99.05ms
step:1321/1770 train_time:130843ms step_avg:99.05ms
step:1322/1770 train_time:130949ms step_avg:99.05ms
step:1323/1770 train_time:131052ms step_avg:99.06ms
step:1324/1770 train_time:131155ms step_avg:99.06ms
step:1325/1770 train_time:131260ms step_avg:99.06ms
step:1326/1770 train_time:131362ms step_avg:99.07ms
step:1327/1770 train_time:131470ms step_avg:99.07ms
step:1328/1770 train_time:131573ms step_avg:99.08ms
step:1329/1770 train_time:131677ms step_avg:99.08ms
step:1330/1770 train_time:131779ms step_avg:99.08ms
step:1331/1770 train_time:131883ms step_avg:99.09ms
step:1332/1770 train_time:131988ms step_avg:99.09ms
step:1333/1770 train_time:132091ms step_avg:99.09ms
step:1334/1770 train_time:132194ms step_avg:99.10ms
step:1335/1770 train_time:132296ms step_avg:99.10ms
step:1336/1770 train_time:132400ms step_avg:99.10ms
step:1337/1770 train_time:132504ms step_avg:99.11ms
step:1338/1770 train_time:132608ms step_avg:99.11ms
step:1339/1770 train_time:132712ms step_avg:99.11ms
step:1340/1770 train_time:132816ms step_avg:99.12ms
step:1341/1770 train_time:132921ms step_avg:99.12ms
step:1342/1770 train_time:133026ms step_avg:99.13ms
step:1343/1770 train_time:133130ms step_avg:99.13ms
step:1344/1770 train_time:133233ms step_avg:99.13ms
step:1345/1770 train_time:133336ms step_avg:99.13ms
step:1346/1770 train_time:133440ms step_avg:99.14ms
step:1347/1770 train_time:133543ms step_avg:99.14ms
step:1348/1770 train_time:133648ms step_avg:99.15ms
step:1349/1770 train_time:133752ms step_avg:99.15ms
step:1350/1770 train_time:133855ms step_avg:99.15ms
step:1351/1770 train_time:133960ms step_avg:99.16ms
step:1352/1770 train_time:134064ms step_avg:99.16ms
step:1353/1770 train_time:134169ms step_avg:99.16ms
step:1354/1770 train_time:134272ms step_avg:99.17ms
step:1355/1770 train_time:134374ms step_avg:99.17ms
step:1356/1770 train_time:134478ms step_avg:99.17ms
step:1357/1770 train_time:134582ms step_avg:99.18ms
step:1358/1770 train_time:134686ms step_avg:99.18ms
step:1359/1770 train_time:134791ms step_avg:99.18ms
step:1360/1770 train_time:134895ms step_avg:99.19ms
step:1361/1770 train_time:134999ms step_avg:99.19ms
step:1362/1770 train_time:135103ms step_avg:99.19ms
step:1363/1770 train_time:135206ms step_avg:99.20ms
step:1364/1770 train_time:135310ms step_avg:99.20ms
step:1365/1770 train_time:135412ms step_avg:99.20ms
step:1366/1770 train_time:135515ms step_avg:99.21ms
step:1367/1770 train_time:135619ms step_avg:99.21ms
step:1368/1770 train_time:135723ms step_avg:99.21ms
step:1369/1770 train_time:135828ms step_avg:99.22ms
step:1370/1770 train_time:135931ms step_avg:99.22ms
step:1371/1770 train_time:136034ms step_avg:99.22ms
step:1372/1770 train_time:136137ms step_avg:99.23ms
step:1373/1770 train_time:136241ms step_avg:99.23ms
step:1374/1770 train_time:136347ms step_avg:99.23ms
step:1375/1770 train_time:136450ms step_avg:99.24ms
step:1375/1770 val_loss:3.3812 train_time:136553ms step_avg:99.31ms
step:1376/1770 train_time:136571ms step_avg:99.25ms
step:1377/1770 train_time:136663ms step_avg:99.25ms
step:1378/1770 train_time:136767ms step_avg:99.25ms
step:1379/1770 train_time:136871ms step_avg:99.25ms
step:1380/1770 train_time:136973ms step_avg:99.26ms
step:1381/1770 train_time:137076ms step_avg:99.26ms
step:1382/1770 train_time:137179ms step_avg:99.26ms
step:1383/1770 train_time:137282ms step_avg:99.26ms
step:1384/1770 train_time:137385ms step_avg:99.27ms
step:1385/1770 train_time:137489ms step_avg:99.27ms
step:1386/1770 train_time:137594ms step_avg:99.27ms
step:1387/1770 train_time:137699ms step_avg:99.28ms
step:1388/1770 train_time:137803ms step_avg:99.28ms
step:1389/1770 train_time:137908ms step_avg:99.29ms
step:1390/1770 train_time:138012ms step_avg:99.29ms
step:1391/1770 train_time:138114ms step_avg:99.29ms
step:1392/1770 train_time:138217ms step_avg:99.29ms
step:1393/1770 train_time:138319ms step_avg:99.30ms
step:1394/1770 train_time:138422ms step_avg:99.30ms
step:1395/1770 train_time:138526ms step_avg:99.30ms
step:1396/1770 train_time:138631ms step_avg:99.31ms
step:1397/1770 train_time:138735ms step_avg:99.31ms
step:1398/1770 train_time:138840ms step_avg:99.31ms
step:1399/1770 train_time:138945ms step_avg:99.32ms
step:1400/1770 train_time:139050ms step_avg:99.32ms
step:1401/1770 train_time:139153ms step_avg:99.32ms
step:1402/1770 train_time:139256ms step_avg:99.33ms
step:1403/1770 train_time:139359ms step_avg:99.33ms
step:1404/1770 train_time:139464ms step_avg:99.33ms
step:1405/1770 train_time:139567ms step_avg:99.34ms
step:1406/1770 train_time:139671ms step_avg:99.34ms
step:1407/1770 train_time:139774ms step_avg:99.34ms
step:1408/1770 train_time:139878ms step_avg:99.35ms
step:1409/1770 train_time:139982ms step_avg:99.35ms
step:1410/1770 train_time:140087ms step_avg:99.35ms
step:1411/1770 train_time:140191ms step_avg:99.36ms
step:1412/1770 train_time:140293ms step_avg:99.36ms
step:1413/1770 train_time:140397ms step_avg:99.36ms
step:1414/1770 train_time:140500ms step_avg:99.36ms
step:1415/1770 train_time:140604ms step_avg:99.37ms
step:1416/1770 train_time:140709ms step_avg:99.37ms
step:1417/1770 train_time:140812ms step_avg:99.37ms
step:1418/1770 train_time:140914ms step_avg:99.38ms
step:1419/1770 train_time:141019ms step_avg:99.38ms
step:1420/1770 train_time:141122ms step_avg:99.38ms
step:1421/1770 train_time:141226ms step_avg:99.39ms
step:1422/1770 train_time:141330ms step_avg:99.39ms
step:1423/1770 train_time:141433ms step_avg:99.39ms
step:1424/1770 train_time:141537ms step_avg:99.39ms
step:1425/1770 train_time:141640ms step_avg:99.40ms
step:1426/1770 train_time:141745ms step_avg:99.40ms
step:1427/1770 train_time:141849ms step_avg:99.40ms
step:1428/1770 train_time:141953ms step_avg:99.41ms
step:1429/1770 train_time:142057ms step_avg:99.41ms
step:1430/1770 train_time:142161ms step_avg:99.41ms
step:1431/1770 train_time:142265ms step_avg:99.42ms
step:1432/1770 train_time:142369ms step_avg:99.42ms
step:1433/1770 train_time:142472ms step_avg:99.42ms
step:1434/1770 train_time:142574ms step_avg:99.42ms
step:1435/1770 train_time:142676ms step_avg:99.43ms
step:1436/1770 train_time:142782ms step_avg:99.43ms
step:1437/1770 train_time:142886ms step_avg:99.43ms
step:1438/1770 train_time:142990ms step_avg:99.44ms
step:1439/1770 train_time:143092ms step_avg:99.44ms
step:1440/1770 train_time:143196ms step_avg:99.44ms
step:1441/1770 train_time:143302ms step_avg:99.45ms
step:1442/1770 train_time:143405ms step_avg:99.45ms
step:1443/1770 train_time:143510ms step_avg:99.45ms
step:1444/1770 train_time:143613ms step_avg:99.46ms
step:1445/1770 train_time:143718ms step_avg:99.46ms
step:1446/1770 train_time:143822ms step_avg:99.46ms
step:1447/1770 train_time:143927ms step_avg:99.47ms
step:1448/1770 train_time:144031ms step_avg:99.47ms
step:1449/1770 train_time:144136ms step_avg:99.47ms
step:1450/1770 train_time:144240ms step_avg:99.48ms
step:1451/1770 train_time:144346ms step_avg:99.48ms
step:1452/1770 train_time:144451ms step_avg:99.48ms
step:1453/1770 train_time:144554ms step_avg:99.49ms
step:1454/1770 train_time:144659ms step_avg:99.49ms
step:1455/1770 train_time:144764ms step_avg:99.49ms
step:1456/1770 train_time:144871ms step_avg:99.50ms
step:1457/1770 train_time:144975ms step_avg:99.50ms
step:1458/1770 train_time:145080ms step_avg:99.51ms
step:1459/1770 train_time:145185ms step_avg:99.51ms
step:1460/1770 train_time:145289ms step_avg:99.51ms
step:1461/1770 train_time:145393ms step_avg:99.52ms
step:1462/1770 train_time:145498ms step_avg:99.52ms
step:1463/1770 train_time:145602ms step_avg:99.52ms
step:1464/1770 train_time:145708ms step_avg:99.53ms
step:1465/1770 train_time:145812ms step_avg:99.53ms
step:1466/1770 train_time:145918ms step_avg:99.53ms
step:1467/1770 train_time:146022ms step_avg:99.54ms
step:1468/1770 train_time:146127ms step_avg:99.54ms
step:1469/1770 train_time:146232ms step_avg:99.55ms
step:1470/1770 train_time:146336ms step_avg:99.55ms
step:1471/1770 train_time:146441ms step_avg:99.55ms
step:1472/1770 train_time:146545ms step_avg:99.56ms
step:1473/1770 train_time:146651ms step_avg:99.56ms
step:1474/1770 train_time:146757ms step_avg:99.56ms
step:1475/1770 train_time:146862ms step_avg:99.57ms
step:1476/1770 train_time:146966ms step_avg:99.57ms
step:1477/1770 train_time:147073ms step_avg:99.58ms
step:1478/1770 train_time:147179ms step_avg:99.58ms
step:1479/1770 train_time:147284ms step_avg:99.58ms
step:1480/1770 train_time:147389ms step_avg:99.59ms
step:1481/1770 train_time:147496ms step_avg:99.59ms
step:1482/1770 train_time:147600ms step_avg:99.60ms
step:1483/1770 train_time:147705ms step_avg:99.60ms
step:1484/1770 train_time:147809ms step_avg:99.60ms
step:1485/1770 train_time:147914ms step_avg:99.61ms
step:1486/1770 train_time:148018ms step_avg:99.61ms
step:1487/1770 train_time:148122ms step_avg:99.61ms
step:1488/1770 train_time:148228ms step_avg:99.62ms
step:1489/1770 train_time:148333ms step_avg:99.62ms
step:1490/1770 train_time:148437ms step_avg:99.62ms
step:1491/1770 train_time:148541ms step_avg:99.63ms
step:1492/1770 train_time:148647ms step_avg:99.63ms
step:1493/1770 train_time:148754ms step_avg:99.63ms
step:1494/1770 train_time:148860ms step_avg:99.64ms
step:1495/1770 train_time:148965ms step_avg:99.64ms
step:1496/1770 train_time:149069ms step_avg:99.65ms
step:1497/1770 train_time:149174ms step_avg:99.65ms
step:1498/1770 train_time:149278ms step_avg:99.65ms
step:1499/1770 train_time:149382ms step_avg:99.65ms
step:1500/1770 train_time:149486ms step_avg:99.66ms
step:1500/1770 val_loss:3.3434 train_time:149590ms step_avg:99.73ms
step:1501/1770 train_time:149607ms step_avg:99.67ms
step:1502/1770 train_time:149703ms step_avg:99.67ms
step:1503/1770 train_time:149807ms step_avg:99.67ms
step:1504/1770 train_time:149912ms step_avg:99.68ms
step:1505/1770 train_time:150017ms step_avg:99.68ms
step:1506/1770 train_time:150121ms step_avg:99.68ms
step:1507/1770 train_time:150225ms step_avg:99.68ms
step:1508/1770 train_time:150330ms step_avg:99.69ms
step:1509/1770 train_time:150435ms step_avg:99.69ms
step:1510/1770 train_time:150539ms step_avg:99.69ms
step:1511/1770 train_time:150646ms step_avg:99.70ms
step:1512/1770 train_time:150752ms step_avg:99.70ms
step:1513/1770 train_time:150859ms step_avg:99.71ms
step:1514/1770 train_time:150963ms step_avg:99.71ms
step:1515/1770 train_time:151067ms step_avg:99.71ms
step:1516/1770 train_time:151171ms step_avg:99.72ms
step:1517/1770 train_time:151276ms step_avg:99.72ms
step:1518/1770 train_time:151382ms step_avg:99.72ms
step:1519/1770 train_time:151485ms step_avg:99.73ms
step:1520/1770 train_time:151590ms step_avg:99.73ms
step:1521/1770 train_time:151695ms step_avg:99.73ms
step:1522/1770 train_time:151801ms step_avg:99.74ms
step:1523/1770 train_time:151906ms step_avg:99.74ms
step:1524/1770 train_time:152011ms step_avg:99.74ms
step:1525/1770 train_time:152116ms step_avg:99.75ms
step:1526/1770 train_time:152220ms step_avg:99.75ms
step:1527/1770 train_time:152323ms step_avg:99.75ms
step:1528/1770 train_time:152428ms step_avg:99.76ms
step:1529/1770 train_time:152533ms step_avg:99.76ms
step:1530/1770 train_time:152638ms step_avg:99.76ms
step:1531/1770 train_time:152743ms step_avg:99.77ms
step:1532/1770 train_time:152849ms step_avg:99.77ms
step:1533/1770 train_time:152955ms step_avg:99.77ms
step:1534/1770 train_time:153061ms step_avg:99.78ms
step:1535/1770 train_time:153164ms step_avg:99.78ms
step:1536/1770 train_time:153269ms step_avg:99.78ms
step:1537/1770 train_time:153373ms step_avg:99.79ms
step:1538/1770 train_time:153479ms step_avg:99.79ms
step:1539/1770 train_time:153583ms step_avg:99.79ms
step:1540/1770 train_time:153690ms step_avg:99.80ms
step:1541/1770 train_time:153796ms step_avg:99.80ms
step:1542/1770 train_time:153900ms step_avg:99.81ms
step:1543/1770 train_time:154005ms step_avg:99.81ms
step:1544/1770 train_time:154112ms step_avg:99.81ms
step:1545/1770 train_time:154217ms step_avg:99.82ms
step:1546/1770 train_time:154321ms step_avg:99.82ms
step:1547/1770 train_time:154425ms step_avg:99.82ms
step:1548/1770 train_time:154529ms step_avg:99.83ms
step:1549/1770 train_time:154634ms step_avg:99.83ms
step:1550/1770 train_time:154739ms step_avg:99.83ms
step:1551/1770 train_time:154843ms step_avg:99.83ms
step:1552/1770 train_time:154949ms step_avg:99.84ms
step:1553/1770 train_time:155054ms step_avg:99.84ms
step:1554/1770 train_time:155158ms step_avg:99.84ms
step:1555/1770 train_time:155263ms step_avg:99.85ms
step:1556/1770 train_time:155366ms step_avg:99.85ms
step:1557/1770 train_time:155470ms step_avg:99.85ms
step:1558/1770 train_time:155575ms step_avg:99.86ms
step:1559/1770 train_time:155680ms step_avg:99.86ms
step:1560/1770 train_time:155785ms step_avg:99.86ms
step:1561/1770 train_time:155892ms step_avg:99.87ms
step:1562/1770 train_time:155998ms step_avg:99.87ms
step:1563/1770 train_time:156102ms step_avg:99.87ms
step:1564/1770 train_time:156206ms step_avg:99.88ms
step:1565/1770 train_time:156310ms step_avg:99.88ms
step:1566/1770 train_time:156414ms step_avg:99.88ms
step:1567/1770 train_time:156519ms step_avg:99.88ms
step:1568/1770 train_time:156624ms step_avg:99.89ms
step:1569/1770 train_time:156731ms step_avg:99.89ms
step:1570/1770 train_time:156835ms step_avg:99.89ms
step:1571/1770 train_time:156940ms step_avg:99.90ms
step:1572/1770 train_time:157044ms step_avg:99.90ms
step:1573/1770 train_time:157150ms step_avg:99.90ms
step:1574/1770 train_time:157255ms step_avg:99.91ms
step:1575/1770 train_time:157359ms step_avg:99.91ms
step:1576/1770 train_time:157463ms step_avg:99.91ms
step:1577/1770 train_time:157569ms step_avg:99.92ms
step:1578/1770 train_time:157676ms step_avg:99.92ms
step:1579/1770 train_time:157780ms step_avg:99.92ms
step:1580/1770 train_time:157884ms step_avg:99.93ms
step:1581/1770 train_time:157991ms step_avg:99.93ms
step:1582/1770 train_time:158097ms step_avg:99.93ms
step:1583/1770 train_time:158201ms step_avg:99.94ms
step:1584/1770 train_time:158307ms step_avg:99.94ms
step:1585/1770 train_time:158411ms step_avg:99.94ms
step:1586/1770 train_time:158518ms step_avg:99.95ms
step:1587/1770 train_time:158623ms step_avg:99.95ms
step:1588/1770 train_time:158727ms step_avg:99.95ms
step:1589/1770 train_time:158833ms step_avg:99.96ms
step:1590/1770 train_time:158938ms step_avg:99.96ms
step:1591/1770 train_time:159042ms step_avg:99.96ms
step:1592/1770 train_time:159148ms step_avg:99.97ms
step:1593/1770 train_time:159252ms step_avg:99.97ms
step:1594/1770 train_time:159356ms step_avg:99.97ms
step:1595/1770 train_time:159462ms step_avg:99.98ms
step:1596/1770 train_time:159567ms step_avg:99.98ms
step:1597/1770 train_time:159671ms step_avg:99.98ms
step:1598/1770 train_time:159776ms step_avg:99.99ms
step:1599/1770 train_time:159882ms step_avg:99.99ms
step:1600/1770 train_time:159989ms step_avg:99.99ms
step:1601/1770 train_time:160094ms step_avg:100.00ms
step:1602/1770 train_time:160199ms step_avg:100.00ms
step:1603/1770 train_time:160303ms step_avg:100.00ms
step:1604/1770 train_time:160407ms step_avg:100.00ms
step:1605/1770 train_time:160511ms step_avg:100.01ms
step:1606/1770 train_time:160615ms step_avg:100.01ms
step:1607/1770 train_time:160724ms step_avg:100.01ms
step:1608/1770 train_time:160829ms step_avg:100.02ms
step:1609/1770 train_time:160934ms step_avg:100.02ms
step:1610/1770 train_time:161040ms step_avg:100.03ms
step:1611/1770 train_time:161146ms step_avg:100.03ms
step:1612/1770 train_time:161252ms step_avg:100.03ms
step:1613/1770 train_time:161357ms step_avg:100.04ms
step:1614/1770 train_time:161461ms step_avg:100.04ms
step:1615/1770 train_time:161566ms step_avg:100.04ms
step:1616/1770 train_time:161670ms step_avg:100.04ms
step:1617/1770 train_time:161778ms step_avg:100.05ms
step:1618/1770 train_time:161883ms step_avg:100.05ms
step:1619/1770 train_time:161988ms step_avg:100.05ms
step:1620/1770 train_time:162095ms step_avg:100.06ms
step:1621/1770 train_time:162200ms step_avg:100.06ms
step:1622/1770 train_time:162305ms step_avg:100.06ms
step:1623/1770 train_time:162412ms step_avg:100.07ms
step:1624/1770 train_time:162516ms step_avg:100.07ms
step:1625/1770 train_time:162620ms step_avg:100.07ms
step:1625/1770 val_loss:3.3095 train_time:162724ms step_avg:100.14ms
step:1626/1770 train_time:162742ms step_avg:100.09ms
step:1627/1770 train_time:162837ms step_avg:100.08ms
step:1628/1770 train_time:162942ms step_avg:100.09ms
step:1629/1770 train_time:163047ms step_avg:100.09ms
step:1630/1770 train_time:163151ms step_avg:100.09ms
step:1631/1770 train_time:163254ms step_avg:100.09ms
step:1632/1770 train_time:163358ms step_avg:100.10ms
step:1633/1770 train_time:163462ms step_avg:100.10ms
step:1634/1770 train_time:163566ms step_avg:100.10ms
step:1635/1770 train_time:163671ms step_avg:100.10ms
step:1636/1770 train_time:163777ms step_avg:100.11ms
step:1637/1770 train_time:163883ms step_avg:100.11ms
step:1638/1770 train_time:163988ms step_avg:100.11ms
step:1639/1770 train_time:164094ms step_avg:100.12ms
step:1640/1770 train_time:164198ms step_avg:100.12ms
step:1641/1770 train_time:164303ms step_avg:100.12ms
step:1642/1770 train_time:164407ms step_avg:100.13ms
step:1643/1770 train_time:164512ms step_avg:100.13ms
step:1644/1770 train_time:164617ms step_avg:100.13ms
step:1645/1770 train_time:164722ms step_avg:100.13ms
step:1646/1770 train_time:164828ms step_avg:100.14ms
step:1647/1770 train_time:164934ms step_avg:100.14ms
step:1648/1770 train_time:165038ms step_avg:100.14ms
step:1649/1770 train_time:165143ms step_avg:100.15ms
step:1650/1770 train_time:165248ms step_avg:100.15ms
step:1651/1770 train_time:165352ms step_avg:100.15ms
step:1652/1770 train_time:165456ms step_avg:100.16ms
step:1653/1770 train_time:165560ms step_avg:100.16ms
step:1654/1770 train_time:165667ms step_avg:100.16ms
step:1655/1770 train_time:165774ms step_avg:100.17ms
step:1656/1770 train_time:165879ms step_avg:100.17ms
step:1657/1770 train_time:165987ms step_avg:100.17ms
step:1658/1770 train_time:166092ms step_avg:100.18ms
step:1659/1770 train_time:166197ms step_avg:100.18ms
step:1660/1770 train_time:166302ms step_avg:100.18ms
step:1661/1770 train_time:166408ms step_avg:100.19ms
step:1662/1770 train_time:166512ms step_avg:100.19ms
step:1663/1770 train_time:166616ms step_avg:100.19ms
step:1664/1770 train_time:166721ms step_avg:100.19ms
step:1665/1770 train_time:166826ms step_avg:100.20ms
step:1666/1770 train_time:166932ms step_avg:100.20ms
step:1667/1770 train_time:167036ms step_avg:100.20ms
step:1668/1770 train_time:167141ms step_avg:100.20ms
step:1669/1770 train_time:167245ms step_avg:100.21ms
step:1670/1770 train_time:167350ms step_avg:100.21ms
step:1671/1770 train_time:167454ms step_avg:100.21ms
step:1672/1770 train_time:167560ms step_avg:100.22ms
step:1673/1770 train_time:167666ms step_avg:100.22ms
step:1674/1770 train_time:167770ms step_avg:100.22ms
step:1675/1770 train_time:167873ms step_avg:100.22ms
step:1676/1770 train_time:167979ms step_avg:100.23ms
step:1677/1770 train_time:168087ms step_avg:100.23ms
step:1678/1770 train_time:168191ms step_avg:100.23ms
step:1679/1770 train_time:168296ms step_avg:100.24ms
step:1680/1770 train_time:168401ms step_avg:100.24ms
step:1681/1770 train_time:168506ms step_avg:100.24ms
step:1682/1770 train_time:168612ms step_avg:100.25ms
step:1683/1770 train_time:168716ms step_avg:100.25ms
step:1684/1770 train_time:168820ms step_avg:100.25ms
step:1685/1770 train_time:168927ms step_avg:100.25ms
step:1686/1770 train_time:169032ms step_avg:100.26ms
step:1687/1770 train_time:169138ms step_avg:100.26ms
step:1688/1770 train_time:169242ms step_avg:100.26ms
step:1689/1770 train_time:169347ms step_avg:100.26ms
step:1690/1770 train_time:169451ms step_avg:100.27ms
step:1691/1770 train_time:169556ms step_avg:100.27ms
step:1692/1770 train_time:169661ms step_avg:100.27ms
step:1693/1770 train_time:169768ms step_avg:100.28ms
step:1694/1770 train_time:169872ms step_avg:100.28ms
step:1695/1770 train_time:169977ms step_avg:100.28ms
step:1696/1770 train_time:170083ms step_avg:100.28ms
step:1697/1770 train_time:170190ms step_avg:100.29ms
step:1698/1770 train_time:170294ms step_avg:100.29ms
step:1699/1770 train_time:170399ms step_avg:100.29ms
step:1700/1770 train_time:170503ms step_avg:100.30ms
step:1701/1770 train_time:170608ms step_avg:100.30ms
step:1702/1770 train_time:170713ms step_avg:100.30ms
step:1703/1770 train_time:170818ms step_avg:100.30ms
step:1704/1770 train_time:170923ms step_avg:100.31ms
step:1705/1770 train_time:171028ms step_avg:100.31ms
step:1706/1770 train_time:171133ms step_avg:100.31ms
step:1707/1770 train_time:171239ms step_avg:100.32ms
step:1708/1770 train_time:171345ms step_avg:100.32ms
step:1709/1770 train_time:171451ms step_avg:100.32ms
step:1710/1770 train_time:171560ms step_avg:100.33ms
step:1711/1770 train_time:171670ms step_avg:100.33ms
step:1712/1770 train_time:171775ms step_avg:100.34ms
step:1713/1770 train_time:171879ms step_avg:100.34ms
step:1714/1770 train_time:171984ms step_avg:100.34ms
step:1715/1770 train_time:172089ms step_avg:100.34ms
step:1716/1770 train_time:172196ms step_avg:100.35ms
step:1717/1770 train_time:172300ms step_avg:100.35ms
step:1718/1770 train_time:172407ms step_avg:100.35ms
step:1719/1770 train_time:172513ms step_avg:100.36ms
step:1720/1770 train_time:172620ms step_avg:100.36ms
step:1721/1770 train_time:172726ms step_avg:100.36ms
step:1722/1770 train_time:172835ms step_avg:100.37ms
step:1723/1770 train_time:172942ms step_avg:100.37ms
step:1724/1770 train_time:173049ms step_avg:100.38ms
step:1725/1770 train_time:173157ms step_avg:100.38ms
step:1726/1770 train_time:173264ms step_avg:100.38ms
step:1727/1770 train_time:173369ms step_avg:100.39ms
step:1728/1770 train_time:173477ms step_avg:100.39ms
step:1729/1770 train_time:173582ms step_avg:100.39ms
step:1730/1770 train_time:173690ms step_avg:100.40ms
step:1731/1770 train_time:173796ms step_avg:100.40ms
step:1732/1770 train_time:173901ms step_avg:100.40ms
step:1733/1770 train_time:174008ms step_avg:100.41ms
step:1734/1770 train_time:174113ms step_avg:100.41ms
step:1735/1770 train_time:174220ms step_avg:100.41ms
step:1736/1770 train_time:174324ms step_avg:100.42ms
step:1737/1770 train_time:174431ms step_avg:100.42ms
step:1738/1770 train_time:174536ms step_avg:100.42ms
step:1739/1770 train_time:174642ms step_avg:100.43ms
step:1740/1770 train_time:174748ms step_avg:100.43ms
step:1741/1770 train_time:174855ms step_avg:100.43ms
step:1742/1770 train_time:174963ms step_avg:100.44ms
step:1743/1770 train_time:175070ms step_avg:100.44ms
step:1744/1770 train_time:175175ms step_avg:100.44ms
step:1745/1770 train_time:175280ms step_avg:100.45ms
step:1746/1770 train_time:175388ms step_avg:100.45ms
step:1747/1770 train_time:175492ms step_avg:100.45ms
step:1748/1770 train_time:175598ms step_avg:100.46ms
step:1749/1770 train_time:175704ms step_avg:100.46ms
step:1750/1770 train_time:175811ms step_avg:100.46ms
step:1750/1770 val_loss:3.2828 train_time:175916ms step_avg:100.52ms
step:1751/1770 train_time:175934ms step_avg:100.48ms
step:1752/1770 train_time:176030ms step_avg:100.47ms
step:1753/1770 train_time:176136ms step_avg:100.48ms
step:1754/1770 train_time:176241ms step_avg:100.48ms
step:1755/1770 train_time:176347ms step_avg:100.48ms
step:1756/1770 train_time:176453ms step_avg:100.49ms
step:1757/1770 train_time:176559ms step_avg:100.49ms
step:1758/1770 train_time:176664ms step_avg:100.49ms
step:1759/1770 train_time:176770ms step_avg:100.49ms
step:1760/1770 train_time:176875ms step_avg:100.50ms
step:1761/1770 train_time:176983ms step_avg:100.50ms
step:1762/1770 train_time:177092ms step_avg:100.51ms
step:1763/1770 train_time:177196ms step_avg:100.51ms
step:1764/1770 train_time:177303ms step_avg:100.51ms
step:1765/1770 train_time:177408ms step_avg:100.51ms
step:1766/1770 train_time:177518ms step_avg:100.52ms
step:1767/1770 train_time:177622ms step_avg:100.52ms
step:1768/1770 train_time:177728ms step_avg:100.52ms
step:1769/1770 train_time:177832ms step_avg:100.53ms
step:1770/1770 train_time:177938ms step_avg:100.53ms
step:1770/1770 val_loss:3.2800 train_time:178045ms step_avg:100.59ms
peak memory allocated: 30724 MiB reserved: 46492 MiB
