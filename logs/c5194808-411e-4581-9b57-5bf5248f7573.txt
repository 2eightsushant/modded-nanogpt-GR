import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
import copy
import glob
from dataclasses import dataclass
from functools import lru_cache, partial # Added partial for hook registration
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
#torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params: list[Tensor] = [*params]
        param_groups = []
        for size in {p.numel() for p in params}:
            b = torch.empty(world_size, size, dtype=torch.bfloat16, device="cuda")
            group = dict(params=[p for p in params if p.numel() == size],
                         update_buffer=b, update_buffer_views=[b[i] for i in range(world_size)])
            param_groups.append(group)
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            update_buffer: Tensor = group["update_buffer"]
            update_buffer_views: list[Tensor] = group["update_buffer_views"]
            # generate weight updates in distributed fashion
            params: list[Tensor] = group["params"]
            handle = None
            params_world = None
            def update_prev(): # optimized Muon implementation contributed by @YouJiacheng
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffer_views):
                    p_world.add_(g_world.view_as(p_world),
                                 alpha=-group["lr"] * max(1, p_world.size(-2) / p_world.size(-1))**0.5)
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if "momentum_buffer" not in state:
                        state["momentum_buffer"] = torch.zeros_like(g)
                    buf: Tensor = state["momentum_buffer"]
                    buf.lerp_(g, 1 - group["momentum"])
                    g = g.lerp_(buf, group["momentum"]) if group["nesterov"] else buf
                    g = zeropower_via_newtonschulz5(g, steps=group["ns_steps"]).flatten()
                else:
                    g = update_buffer_views[self.rank]
                if base_i > 0:
                    update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=0.12).transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, block_mask: BlockMask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, max_seq_len, i) for i in range(num_layers)])
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128),
                                    use_fp8=True, x_s=(model_dim**0.5)/448, w_s=24/448, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        self.skip_weights = nn.Parameter(torch.ones(num_layers//2))

    def create_blockmasks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        docs = (input_seq == 50256).cumsum(0)

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_blockmask: Tensor):
            num_blocks = dense_blockmask.sum(dim=-1, dtype=torch.int32)
            indices = dense_blockmask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
        causal_blockmask_any = block_idx[:, None] >= block_idx
        causal_blockmask_all = block_idx[:, None] > block_idx
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
        document_blockmask_any = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
        document_blockmask_all = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
        blockmask_any = causal_blockmask_any & document_blockmask_any
        blockmask_all = causal_blockmask_all & document_blockmask_all
        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(blockmask_any & ~blockmask_all)
        full_kv_num_blocks, full_kv_indices = dense_to_ordered(blockmask_all)
        def build_bm(window_size_blocks: Tensor) -> BlockMask:
            return BlockMask.from_kv_blocks(
                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(window_size_blocks - full_kv_num_blocks, 1)),
                partial_kv_indices,
                torch.clamp_max(full_kv_num_blocks, window_size_blocks - 1),
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        long_bm, short_bm = self.create_blockmasks(input_seq, sliding_window_num_blocks)
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, long_bm, short_bm, short_bm, short_bm, long_bm]
        assert len(block_masks) == len(self.blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977

        # U-net design by @brendanh0gan
        skip_connections = []
        n = len(self.skip_weights)
        for i in range(len(self.blocks)):
            if i >= n:
                x = x + self.skip_weights[i - n] * skip_connections.pop()
            x = self.blocks[i](x, ve[i], x0, block_masks[i])
            if i < n:
                skip_connections.append(x)

        x = norm(x)
        logits = self.lm_head(x).float()
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / (7.5 * x.size(-1)**0.5))
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq, reduction='sum' if self.training else 'mean')
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn't helpful.
        pos += batch_size
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_seq_len = 48*1024 # FlexAttention sequence length
    val_seq_len = 4*64*1024 # FlexAttention sequence length for validation
    # optimization
    num_iterations = 1770 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    # architecture
    vocab_size = 50257
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert world_size == 8 # this code is designed for 8xH100
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs("logs_bucket", exist_ok=True)
    logfile = f"logs_bucket/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

########################################
#    Construct model and optimizer     #
########################################

model: nn.Module = GPT(vocab_size=args.vocab_size, num_layers=12, num_heads=6, model_dim=768,
                       max_seq_len=max(args.train_seq_len, args.val_seq_len)).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
adam_params = [dict(params=head_params, lr=0.22), dict(params=embed_params, lr=0.6), dict(params=scalar_params, lr=0.04)]
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = torch.optim.Adam(adam_params, betas=(0.8, 0.95), eps=1e-10, fused=True)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then decay
def get_lr(step: int):
    x = step / args.num_iterations # progress in training
    assert 0 <= x < 1
    if x < 1 - args.cooldown_frac:
        return 1.0
    else:
        w = (1 - x) / args.cooldown_frac
        return w * 1.0 + (1 - w) * 0.1

# attention window size schedule: linearly increase
@lru_cache(1)
def get_window_size_blocks_helper(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)
def get_window_size_blocks(step: int):
    x = step / args.num_iterations # progress in training
    assert 0 <= x <= 1
    # Linearly increase the block-wise sliding window size over training 128 -> 1792
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * x, n=128)
    return get_window_size_blocks_helper(window_size)

model: nn.Module = torch.compile(model, dynamic=False)

########################################
#     Warmup kernel and Layerwise Bucketing   #
########################################

# Track the gradient readiness order during warmup
gradient_ready_order = []

def record_grad_order_hook(param):
    def hook(*_):
        if param.grad is not None and all(param is not p for p in gradient_ready_order):
            gradient_ready_order.append(param)
    return hook

# Register temp hooks to track when each parameter's gradient is ready
tmp_handles = []
for p in model.parameters():
    if p.requires_grad:
        handle = p.register_post_accumulate_grad_hook(record_grad_order_hook(p))
        tmp_handles.append(handle)

warmup_steps = 10
initial_state = dict(
    model=copy.deepcopy(model.state_dict()),
    optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]
)

for step in range(warmup_steps):
    # Create a local generator with fixed, step-dependent seed
    g = torch.Generator(device="cuda")
    g.manual_seed(1337 + step)

    # Use this generator to sample inputs
    inputs = targets = torch.randint(
        low=0,
        high=args.vocab_size,
        size=(args.train_seq_len,),
        device="cuda",
        dtype=torch.long,
        generator=g,  # <- Local generator ensures deterministic and isolated RNG
    )
    model(inputs.to(torch.int32), targets, get_window_size_blocks(0)).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)

# Remove hooks after warmup
for h in tmp_handles:
    h.remove()

# Restore model/optimizer state to ensure a clean start
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del initial_state

print0(f"Collected {len(gradient_ready_order)} parameters in gradient-ready order")

########################################
#      Overlap Communication Setup     #
########################################

# Layer-aware bucketing using gradient-ready order
def create_buckets_layerwise(ordered_params, bucket_size_mb=25):
    """Group parameters into buckets of ~bucket_size_mb MB, in gradient ready order"""
    buckets = []
    current_bucket = []
    current_size = 0

    for param in ordered_params:
        if not param.requires_grad:
            continue
        param_size_mb = param.numel() * param.element_size() / (1024 * 1024)

        if current_size + param_size_mb > bucket_size_mb and current_bucket:
            buckets.append(current_bucket)
            current_bucket = [param]
            current_size = param_size_mb
        else:
            current_bucket.append(param)
            current_size += param_size_mb

    if current_bucket:
        buckets.append(current_bucket)

    return buckets

# Create buckets for all parameters
all_params = [p for p in model.parameters() if p.requires_grad]
param_buckets = create_buckets_layerwise(gradient_ready_order)
print0(f"Created {len(param_buckets)} gradient buckets")
for i, bucket in enumerate(param_buckets):
    total_size = sum(p.numel() * p.element_size() for p in bucket) / (1024 * 1024)
    print0(f"Bucket {i}: {len(bucket)} params, {total_size:.1f} MB")

# Bucket state tracking
bucket_ready_count = [0] * len(param_buckets)
bucket_handles = [None] * len(param_buckets)
param_to_bucket = {}

# Map each parameter to its bucket index
for bucket_idx, bucket in enumerate(param_buckets):
    for param in bucket:
        param_to_bucket[param] = bucket_idx

def _gradient_hook(param: Tensor):
    """Called when a parameter's gradient is ready"""
    if param.grad is None:
        return

    bucket_idx = param_to_bucket[param]
    bucket_ready_count[bucket_idx] += 1

    # Check if all parameters in this bucket are ready
    if bucket_ready_count[bucket_idx] == len(param_buckets[bucket_idx]):
        # All-reduce this bucket
        bucket_grads = [p.grad for p in param_buckets[bucket_idx]]

        # For multi-tensor operations, we can reduce them together
        if len(bucket_grads) == 1:
            handle = dist.all_reduce(bucket_grads[0], op=dist.ReduceOp.AVG, async_op=True)
        else:
            # Use multi-tensor all-reduce for efficiency
            handle = dist.all_reduce_coalesced(bucket_grads, op=dist.ReduceOp.AVG, async_op=True)

        bucket_handles[bucket_idx] = handle

# Register hooks for all parameters
print0("Registering bucketed gradient hooks...")
for param in all_params:
    param.register_post_accumulate_grad_hook(_gradient_hook)

def wait_for_gradients():
    """Wait for all gradient reductions to complete and reset bucket state"""
    for handle in bucket_handles:
        if handle is not None:
            handle.wait()

    # Reset state for next iteration
    for i in range(len(bucket_ready_count)):
        bucket_ready_count[i] = 0
        bucket_handles[i] = None

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, world_size * args.train_seq_len, rank, world_size)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_batch_size = world_size * args.val_seq_len
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        val_loader = distributed_data_generator(args.val_files, val_batch_size, rank, world_size)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets = next(val_loader)
                val_loss += model(inputs, targets, get_window_size_blocks(step))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs_bucket/{run_id}", exist_ok=True)
            torch.save(log, f"logs_bucket/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    inputs, targets = next(train_loader)
    model(inputs, targets, get_window_size_blocks(step)).backward()
    #for param in model.parameters():
    #    dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
    wait_for_gradients() # does the same thing as commented two lines above, but faster

    # set optimization hyperparameters
    for opt in optimizers:
        for group in opt.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)
    for group in optimizer2.param_groups:
        frac = min(step / 300, 1) # momentum warmup for muon
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers
    for opt in optimizers:
        opt.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0]
Running PyTorch 2.8.0.dev20250612+cu126 compiled for CUDA 12.6
Fri Jun 13 06:18:42 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   38C    P0            117W /  700W |    5824MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   29C    P0            110W /  700W |    1514MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   29C    P0            113W /  700W |    1514MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   37C    P0            121W /  700W |    1514MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   38C    P0            117W /  700W |    1514MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   30C    P0            112W /  700W |    1514MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   37C    P0            119W /  700W |    1514MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   30C    P0            114W /  700W |    1514MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
Collected 75 parameters in gradient-ready order
Created 21 gradient buckets
Bucket 0: 1 params, 73.6 MB
Bucket 1: 1 params, 73.6 MB
Bucket 2: 4 params, 15.8 MB
Bucket 3: 1 params, 147.4 MB
Bucket 4: 4 params, 20.3 MB
Bucket 5: 4 params, 24.8 MB
Bucket 6: 4 params, 18.0 MB
Bucket 7: 5 params, 18.0 MB
Bucket 8: 4 params, 20.3 MB
Bucket 9: 5 params, 24.8 MB
Bucket 10: 4 params, 20.3 MB
Bucket 11: 4 params, 24.8 MB
Bucket 12: 5 params, 18.0 MB
Bucket 13: 5 params, 20.3 MB
Bucket 14: 5 params, 18.0 MB
Bucket 15: 4 params, 20.3 MB
Bucket 16: 5 params, 24.8 MB
Bucket 17: 4 params, 24.8 MB
Bucket 18: 4 params, 2.3 MB
Bucket 19: 1 params, 73.6 MB
Bucket 20: 1 params, 73.6 MB
Registering bucketed gradient hooks...
step:0/1770 val_loss:10.8258 train_time:0ms step_avg:0.05ms
step:1/1770 train_time:73ms step_avg:73.30ms
step:2/1770 train_time:148ms step_avg:74.22ms
step:3/1770 train_time:237ms step_avg:78.94ms
step:4/1770 train_time:330ms step_avg:82.51ms
step:5/1770 train_time:424ms step_avg:84.83ms
step:6/1770 train_time:519ms step_avg:86.45ms
step:7/1770 train_time:613ms step_avg:87.56ms
step:8/1770 train_time:707ms step_avg:88.34ms
step:9/1770 train_time:801ms step_avg:88.97ms
step:10/1770 train_time:894ms step_avg:89.42ms
step:11/1770 train_time:989ms step_avg:89.87ms
step:12/1770 train_time:1084ms step_avg:90.35ms
step:13/1770 train_time:1181ms step_avg:90.88ms
step:14/1770 train_time:1278ms step_avg:91.28ms
step:15/1770 train_time:1372ms step_avg:91.49ms
step:16/1770 train_time:1466ms step_avg:91.64ms
step:17/1770 train_time:1561ms step_avg:91.82ms
step:18/1770 train_time:1656ms step_avg:91.99ms
step:19/1770 train_time:1750ms step_avg:92.13ms
step:20/1770 train_time:1845ms step_avg:92.26ms
step:21/1770 train_time:1940ms step_avg:92.38ms
step:22/1770 train_time:2035ms step_avg:92.49ms
step:23/1770 train_time:2130ms step_avg:92.59ms
step:24/1770 train_time:2225ms step_avg:92.71ms
step:25/1770 train_time:2321ms step_avg:92.83ms
step:26/1770 train_time:2416ms step_avg:92.91ms
step:27/1770 train_time:2510ms step_avg:92.96ms
step:28/1770 train_time:2604ms step_avg:93.01ms
step:29/1770 train_time:2699ms step_avg:93.07ms
step:30/1770 train_time:2793ms step_avg:93.12ms
step:31/1770 train_time:2888ms step_avg:93.15ms
step:32/1770 train_time:2983ms step_avg:93.22ms
step:33/1770 train_time:3078ms step_avg:93.28ms
step:34/1770 train_time:3173ms step_avg:93.34ms
step:35/1770 train_time:3268ms step_avg:93.36ms
step:36/1770 train_time:3363ms step_avg:93.42ms
step:37/1770 train_time:3458ms step_avg:93.46ms
step:38/1770 train_time:3553ms step_avg:93.49ms
step:39/1770 train_time:3647ms step_avg:93.51ms
step:40/1770 train_time:3743ms step_avg:93.56ms
step:41/1770 train_time:3838ms step_avg:93.60ms
step:42/1770 train_time:3932ms step_avg:93.61ms
step:43/1770 train_time:4026ms step_avg:93.63ms
step:44/1770 train_time:4121ms step_avg:93.66ms
step:45/1770 train_time:4216ms step_avg:93.69ms
step:46/1770 train_time:4310ms step_avg:93.71ms
step:47/1770 train_time:4405ms step_avg:93.71ms
step:48/1770 train_time:4500ms step_avg:93.75ms
step:49/1770 train_time:4596ms step_avg:93.79ms
step:50/1770 train_time:4690ms step_avg:93.80ms
step:51/1770 train_time:4784ms step_avg:93.81ms
step:52/1770 train_time:4879ms step_avg:93.84ms
step:53/1770 train_time:4975ms step_avg:93.86ms
step:54/1770 train_time:5069ms step_avg:93.86ms
step:55/1770 train_time:5163ms step_avg:93.87ms
step:56/1770 train_time:5258ms step_avg:93.90ms
step:57/1770 train_time:5352ms step_avg:93.90ms
step:58/1770 train_time:5447ms step_avg:93.92ms
step:59/1770 train_time:5543ms step_avg:93.94ms
step:60/1770 train_time:5637ms step_avg:93.96ms
step:61/1770 train_time:5732ms step_avg:93.97ms
step:62/1770 train_time:5826ms step_avg:93.97ms
step:63/1770 train_time:5921ms step_avg:93.99ms
step:64/1770 train_time:6016ms step_avg:94.00ms
step:65/1770 train_time:6110ms step_avg:94.01ms
step:66/1770 train_time:6204ms step_avg:94.00ms
step:67/1770 train_time:6299ms step_avg:94.01ms
step:68/1770 train_time:6393ms step_avg:94.02ms
step:69/1770 train_time:6487ms step_avg:94.02ms
step:70/1770 train_time:6583ms step_avg:94.04ms
step:71/1770 train_time:6677ms step_avg:94.05ms
step:72/1770 train_time:6772ms step_avg:94.05ms
step:73/1770 train_time:6866ms step_avg:94.05ms
step:74/1770 train_time:6961ms step_avg:94.07ms
step:75/1770 train_time:7057ms step_avg:94.10ms
step:76/1770 train_time:7151ms step_avg:94.10ms
step:77/1770 train_time:7245ms step_avg:94.10ms
step:78/1770 train_time:7340ms step_avg:94.11ms
step:79/1770 train_time:7435ms step_avg:94.11ms
step:80/1770 train_time:7529ms step_avg:94.11ms
step:81/1770 train_time:7623ms step_avg:94.11ms
step:82/1770 train_time:7718ms step_avg:94.12ms
step:83/1770 train_time:7812ms step_avg:94.13ms
step:84/1770 train_time:7907ms step_avg:94.13ms
step:85/1770 train_time:8001ms step_avg:94.13ms
step:86/1770 train_time:8096ms step_avg:94.14ms
step:87/1770 train_time:8191ms step_avg:94.15ms
step:88/1770 train_time:8286ms step_avg:94.16ms
step:89/1770 train_time:8381ms step_avg:94.17ms
step:90/1770 train_time:8475ms step_avg:94.17ms
step:91/1770 train_time:8569ms step_avg:94.17ms
step:92/1770 train_time:8664ms step_avg:94.17ms
step:93/1770 train_time:8759ms step_avg:94.18ms
step:94/1770 train_time:8853ms step_avg:94.18ms
step:95/1770 train_time:8947ms step_avg:94.18ms
step:96/1770 train_time:9043ms step_avg:94.20ms
step:97/1770 train_time:9138ms step_avg:94.21ms
step:98/1770 train_time:9233ms step_avg:94.21ms
step:99/1770 train_time:9327ms step_avg:94.21ms
step:100/1770 train_time:9422ms step_avg:94.22ms
step:101/1770 train_time:9517ms step_avg:94.23ms
step:102/1770 train_time:9612ms step_avg:94.23ms
step:103/1770 train_time:9706ms step_avg:94.23ms
step:104/1770 train_time:9801ms step_avg:94.24ms
step:105/1770 train_time:9895ms step_avg:94.24ms
step:106/1770 train_time:9989ms step_avg:94.24ms
step:107/1770 train_time:10085ms step_avg:94.25ms
step:108/1770 train_time:10181ms step_avg:94.27ms
step:109/1770 train_time:10276ms step_avg:94.28ms
step:110/1770 train_time:10370ms step_avg:94.27ms
step:111/1770 train_time:10464ms step_avg:94.27ms
step:112/1770 train_time:10559ms step_avg:94.28ms
step:113/1770 train_time:10654ms step_avg:94.28ms
step:114/1770 train_time:10748ms step_avg:94.28ms
step:115/1770 train_time:10843ms step_avg:94.29ms
step:116/1770 train_time:10938ms step_avg:94.29ms
step:117/1770 train_time:11032ms step_avg:94.29ms
step:118/1770 train_time:11126ms step_avg:94.29ms
step:119/1770 train_time:11222ms step_avg:94.30ms
step:120/1770 train_time:11317ms step_avg:94.31ms
step:121/1770 train_time:11412ms step_avg:94.31ms
step:122/1770 train_time:11507ms step_avg:94.32ms
step:123/1770 train_time:11602ms step_avg:94.32ms
step:124/1770 train_time:11697ms step_avg:94.33ms
step:125/1770 train_time:11792ms step_avg:94.33ms
step:125/1770 val_loss:4.6424 train_time:11885ms step_avg:95.08ms
step:126/1770 train_time:11904ms step_avg:94.48ms
step:127/1770 train_time:11987ms step_avg:94.39ms
step:128/1770 train_time:12088ms step_avg:94.44ms
step:129/1770 train_time:12185ms step_avg:94.46ms
step:130/1770 train_time:12279ms step_avg:94.46ms
step:131/1770 train_time:12374ms step_avg:94.46ms
step:132/1770 train_time:12468ms step_avg:94.46ms
step:133/1770 train_time:12562ms step_avg:94.45ms
step:134/1770 train_time:12657ms step_avg:94.45ms
step:135/1770 train_time:12751ms step_avg:94.45ms
step:136/1770 train_time:12846ms step_avg:94.45ms
step:137/1770 train_time:12940ms step_avg:94.45ms
step:138/1770 train_time:13037ms step_avg:94.47ms
step:139/1770 train_time:13134ms step_avg:94.49ms
step:140/1770 train_time:13231ms step_avg:94.51ms
step:141/1770 train_time:13326ms step_avg:94.51ms
step:142/1770 train_time:13420ms step_avg:94.51ms
step:143/1770 train_time:13516ms step_avg:94.52ms
step:144/1770 train_time:13611ms step_avg:94.52ms
step:145/1770 train_time:13706ms step_avg:94.52ms
step:146/1770 train_time:13800ms step_avg:94.52ms
step:147/1770 train_time:13895ms step_avg:94.52ms
step:148/1770 train_time:13990ms step_avg:94.53ms
step:149/1770 train_time:14086ms step_avg:94.54ms
step:150/1770 train_time:14181ms step_avg:94.54ms
step:151/1770 train_time:14278ms step_avg:94.56ms
step:152/1770 train_time:14374ms step_avg:94.56ms
step:153/1770 train_time:14470ms step_avg:94.57ms
step:154/1770 train_time:14564ms step_avg:94.57ms
step:155/1770 train_time:14659ms step_avg:94.57ms
step:156/1770 train_time:14754ms step_avg:94.58ms
step:157/1770 train_time:14849ms step_avg:94.58ms
step:158/1770 train_time:14943ms step_avg:94.58ms
step:159/1770 train_time:15038ms step_avg:94.58ms
step:160/1770 train_time:15133ms step_avg:94.58ms
step:161/1770 train_time:15229ms step_avg:94.59ms
step:162/1770 train_time:15324ms step_avg:94.59ms
step:163/1770 train_time:15419ms step_avg:94.60ms
step:164/1770 train_time:15515ms step_avg:94.61ms
step:165/1770 train_time:15611ms step_avg:94.61ms
step:166/1770 train_time:15706ms step_avg:94.61ms
step:167/1770 train_time:15800ms step_avg:94.61ms
step:168/1770 train_time:15896ms step_avg:94.62ms
step:169/1770 train_time:15991ms step_avg:94.62ms
step:170/1770 train_time:16086ms step_avg:94.63ms
step:171/1770 train_time:16182ms step_avg:94.63ms
step:172/1770 train_time:16278ms step_avg:94.64ms
step:173/1770 train_time:16374ms step_avg:94.65ms
step:174/1770 train_time:16470ms step_avg:94.66ms
step:175/1770 train_time:16565ms step_avg:94.66ms
step:176/1770 train_time:16660ms step_avg:94.66ms
step:177/1770 train_time:16756ms step_avg:94.67ms
step:178/1770 train_time:16851ms step_avg:94.67ms
step:179/1770 train_time:16946ms step_avg:94.67ms
step:180/1770 train_time:17041ms step_avg:94.67ms
step:181/1770 train_time:17137ms step_avg:94.68ms
step:182/1770 train_time:17233ms step_avg:94.69ms
step:183/1770 train_time:17329ms step_avg:94.69ms
step:184/1770 train_time:17423ms step_avg:94.69ms
step:185/1770 train_time:17519ms step_avg:94.70ms
step:186/1770 train_time:17614ms step_avg:94.70ms
step:187/1770 train_time:17709ms step_avg:94.70ms
step:188/1770 train_time:17804ms step_avg:94.70ms
step:189/1770 train_time:17899ms step_avg:94.71ms
step:190/1770 train_time:17995ms step_avg:94.71ms
step:191/1770 train_time:18091ms step_avg:94.71ms
step:192/1770 train_time:18186ms step_avg:94.72ms
step:193/1770 train_time:18281ms step_avg:94.72ms
step:194/1770 train_time:18377ms step_avg:94.72ms
step:195/1770 train_time:18473ms step_avg:94.73ms
step:196/1770 train_time:18569ms step_avg:94.74ms
step:197/1770 train_time:18664ms step_avg:94.74ms
step:198/1770 train_time:18759ms step_avg:94.74ms
step:199/1770 train_time:18854ms step_avg:94.75ms
step:200/1770 train_time:18949ms step_avg:94.75ms
step:201/1770 train_time:19044ms step_avg:94.75ms
step:202/1770 train_time:19139ms step_avg:94.75ms
step:203/1770 train_time:19234ms step_avg:94.75ms
step:204/1770 train_time:19330ms step_avg:94.76ms
step:205/1770 train_time:19425ms step_avg:94.76ms
step:206/1770 train_time:19520ms step_avg:94.76ms
step:207/1770 train_time:19615ms step_avg:94.76ms
step:208/1770 train_time:19711ms step_avg:94.77ms
step:209/1770 train_time:19807ms step_avg:94.77ms
step:210/1770 train_time:19901ms step_avg:94.77ms
step:211/1770 train_time:19996ms step_avg:94.77ms
step:212/1770 train_time:20091ms step_avg:94.77ms
step:213/1770 train_time:20186ms step_avg:94.77ms
step:214/1770 train_time:20281ms step_avg:94.77ms
step:215/1770 train_time:20377ms step_avg:94.77ms
step:216/1770 train_time:20472ms step_avg:94.78ms
step:217/1770 train_time:20568ms step_avg:94.78ms
step:218/1770 train_time:20663ms step_avg:94.78ms
step:219/1770 train_time:20758ms step_avg:94.78ms
step:220/1770 train_time:20853ms step_avg:94.79ms
step:221/1770 train_time:20948ms step_avg:94.79ms
step:222/1770 train_time:21043ms step_avg:94.79ms
step:223/1770 train_time:21137ms step_avg:94.79ms
step:224/1770 train_time:21233ms step_avg:94.79ms
step:225/1770 train_time:21329ms step_avg:94.79ms
step:226/1770 train_time:21423ms step_avg:94.79ms
step:227/1770 train_time:21518ms step_avg:94.79ms
step:228/1770 train_time:21615ms step_avg:94.80ms
step:229/1770 train_time:21710ms step_avg:94.80ms
step:230/1770 train_time:21805ms step_avg:94.80ms
step:231/1770 train_time:21900ms step_avg:94.80ms
step:232/1770 train_time:21995ms step_avg:94.81ms
step:233/1770 train_time:22090ms step_avg:94.81ms
step:234/1770 train_time:22185ms step_avg:94.81ms
step:235/1770 train_time:22280ms step_avg:94.81ms
step:236/1770 train_time:22375ms step_avg:94.81ms
step:237/1770 train_time:22470ms step_avg:94.81ms
step:238/1770 train_time:22565ms step_avg:94.81ms
step:239/1770 train_time:22660ms step_avg:94.81ms
step:240/1770 train_time:22755ms step_avg:94.81ms
step:241/1770 train_time:22850ms step_avg:94.81ms
step:242/1770 train_time:22945ms step_avg:94.82ms
step:243/1770 train_time:23040ms step_avg:94.81ms
step:244/1770 train_time:23135ms step_avg:94.82ms
step:245/1770 train_time:23230ms step_avg:94.82ms
step:246/1770 train_time:23326ms step_avg:94.82ms
step:247/1770 train_time:23420ms step_avg:94.82ms
step:248/1770 train_time:23516ms step_avg:94.82ms
step:249/1770 train_time:23611ms step_avg:94.82ms
step:250/1770 train_time:23707ms step_avg:94.83ms
step:250/1770 val_loss:4.1011 train_time:23800ms step_avg:95.20ms
step:251/1770 train_time:23819ms step_avg:94.90ms
step:252/1770 train_time:23903ms step_avg:94.85ms
step:253/1770 train_time:24004ms step_avg:94.88ms
step:254/1770 train_time:24100ms step_avg:94.88ms
step:255/1770 train_time:24195ms step_avg:94.88ms
step:256/1770 train_time:24290ms step_avg:94.88ms
step:257/1770 train_time:24384ms step_avg:94.88ms
step:258/1770 train_time:24479ms step_avg:94.88ms
step:259/1770 train_time:24574ms step_avg:94.88ms
step:260/1770 train_time:24668ms step_avg:94.88ms
step:261/1770 train_time:24762ms step_avg:94.87ms
step:262/1770 train_time:24858ms step_avg:94.88ms
step:263/1770 train_time:24955ms step_avg:94.89ms
step:264/1770 train_time:25052ms step_avg:94.89ms
step:265/1770 train_time:25147ms step_avg:94.89ms
step:266/1770 train_time:25242ms step_avg:94.90ms
step:267/1770 train_time:25338ms step_avg:94.90ms
step:268/1770 train_time:25433ms step_avg:94.90ms
step:269/1770 train_time:25528ms step_avg:94.90ms
step:270/1770 train_time:25623ms step_avg:94.90ms
step:271/1770 train_time:25719ms step_avg:94.90ms
step:272/1770 train_time:25815ms step_avg:94.91ms
step:273/1770 train_time:25910ms step_avg:94.91ms
step:274/1770 train_time:26006ms step_avg:94.91ms
step:275/1770 train_time:26102ms step_avg:94.92ms
step:276/1770 train_time:26198ms step_avg:94.92ms
step:277/1770 train_time:26294ms step_avg:94.92ms
step:278/1770 train_time:26390ms step_avg:94.93ms
step:279/1770 train_time:26484ms step_avg:94.93ms
step:280/1770 train_time:26580ms step_avg:94.93ms
step:281/1770 train_time:26677ms step_avg:94.93ms
step:282/1770 train_time:26773ms step_avg:94.94ms
step:283/1770 train_time:26868ms step_avg:94.94ms
step:284/1770 train_time:26964ms step_avg:94.94ms
step:285/1770 train_time:27060ms step_avg:94.95ms
step:286/1770 train_time:27156ms step_avg:94.95ms
step:287/1770 train_time:27252ms step_avg:94.95ms
step:288/1770 train_time:27348ms step_avg:94.96ms
step:289/1770 train_time:27443ms step_avg:94.96ms
step:290/1770 train_time:27539ms step_avg:94.96ms
step:291/1770 train_time:27635ms step_avg:94.96ms
step:292/1770 train_time:27730ms step_avg:94.97ms
step:293/1770 train_time:27825ms step_avg:94.97ms
step:294/1770 train_time:27921ms step_avg:94.97ms
step:295/1770 train_time:28016ms step_avg:94.97ms
step:296/1770 train_time:28112ms step_avg:94.97ms
step:297/1770 train_time:28208ms step_avg:94.98ms
step:298/1770 train_time:28303ms step_avg:94.98ms
step:299/1770 train_time:28399ms step_avg:94.98ms
step:300/1770 train_time:28495ms step_avg:94.98ms
step:301/1770 train_time:28591ms step_avg:94.99ms
step:302/1770 train_time:28686ms step_avg:94.99ms
step:303/1770 train_time:28782ms step_avg:94.99ms
step:304/1770 train_time:28877ms step_avg:94.99ms
step:305/1770 train_time:28972ms step_avg:94.99ms
step:306/1770 train_time:29068ms step_avg:94.99ms
step:307/1770 train_time:29164ms step_avg:95.00ms
step:308/1770 train_time:29260ms step_avg:95.00ms
step:309/1770 train_time:29355ms step_avg:95.00ms
step:310/1770 train_time:29451ms step_avg:95.00ms
step:311/1770 train_time:29546ms step_avg:95.00ms
step:312/1770 train_time:29642ms step_avg:95.01ms
step:313/1770 train_time:29738ms step_avg:95.01ms
step:314/1770 train_time:29834ms step_avg:95.01ms
step:315/1770 train_time:29929ms step_avg:95.01ms
step:316/1770 train_time:30025ms step_avg:95.02ms
step:317/1770 train_time:30120ms step_avg:95.02ms
step:318/1770 train_time:30217ms step_avg:95.02ms
step:319/1770 train_time:30313ms step_avg:95.02ms
step:320/1770 train_time:30408ms step_avg:95.03ms
step:321/1770 train_time:30504ms step_avg:95.03ms
step:322/1770 train_time:30599ms step_avg:95.03ms
step:323/1770 train_time:30695ms step_avg:95.03ms
step:324/1770 train_time:30791ms step_avg:95.03ms
step:325/1770 train_time:30887ms step_avg:95.04ms
step:326/1770 train_time:30982ms step_avg:95.04ms
step:327/1770 train_time:31078ms step_avg:95.04ms
step:328/1770 train_time:31173ms step_avg:95.04ms
step:329/1770 train_time:31269ms step_avg:95.04ms
step:330/1770 train_time:31364ms step_avg:95.04ms
step:331/1770 train_time:31460ms step_avg:95.04ms
step:332/1770 train_time:31556ms step_avg:95.05ms
step:333/1770 train_time:31652ms step_avg:95.05ms
step:334/1770 train_time:31747ms step_avg:95.05ms
step:335/1770 train_time:31842ms step_avg:95.05ms
step:336/1770 train_time:31938ms step_avg:95.05ms
step:337/1770 train_time:32034ms step_avg:95.06ms
step:338/1770 train_time:32129ms step_avg:95.06ms
step:339/1770 train_time:32225ms step_avg:95.06ms
step:340/1770 train_time:32321ms step_avg:95.06ms
step:341/1770 train_time:32418ms step_avg:95.07ms
step:342/1770 train_time:32514ms step_avg:95.07ms
step:343/1770 train_time:32610ms step_avg:95.07ms
step:344/1770 train_time:32705ms step_avg:95.07ms
step:345/1770 train_time:32800ms step_avg:95.07ms
step:346/1770 train_time:32896ms step_avg:95.07ms
step:347/1770 train_time:32991ms step_avg:95.08ms
step:348/1770 train_time:33086ms step_avg:95.08ms
step:349/1770 train_time:33182ms step_avg:95.08ms
step:350/1770 train_time:33278ms step_avg:95.08ms
step:351/1770 train_time:33374ms step_avg:95.08ms
step:352/1770 train_time:33470ms step_avg:95.08ms
step:353/1770 train_time:33566ms step_avg:95.09ms
step:354/1770 train_time:33661ms step_avg:95.09ms
step:355/1770 train_time:33756ms step_avg:95.09ms
step:356/1770 train_time:33852ms step_avg:95.09ms
step:357/1770 train_time:33947ms step_avg:95.09ms
step:358/1770 train_time:34042ms step_avg:95.09ms
step:359/1770 train_time:34138ms step_avg:95.09ms
step:360/1770 train_time:34234ms step_avg:95.09ms
step:361/1770 train_time:34329ms step_avg:95.09ms
step:362/1770 train_time:34425ms step_avg:95.10ms
step:363/1770 train_time:34521ms step_avg:95.10ms
step:364/1770 train_time:34617ms step_avg:95.10ms
step:365/1770 train_time:34712ms step_avg:95.10ms
step:366/1770 train_time:34807ms step_avg:95.10ms
step:367/1770 train_time:34902ms step_avg:95.10ms
step:368/1770 train_time:34998ms step_avg:95.10ms
step:369/1770 train_time:35095ms step_avg:95.11ms
step:370/1770 train_time:35191ms step_avg:95.11ms
step:371/1770 train_time:35286ms step_avg:95.11ms
step:372/1770 train_time:35381ms step_avg:95.11ms
step:373/1770 train_time:35477ms step_avg:95.11ms
step:374/1770 train_time:35573ms step_avg:95.11ms
step:375/1770 train_time:35669ms step_avg:95.12ms
step:375/1770 val_loss:3.8931 train_time:35763ms step_avg:95.37ms
step:376/1770 train_time:35781ms step_avg:95.16ms
step:377/1770 train_time:35866ms step_avg:95.13ms
step:378/1770 train_time:35970ms step_avg:95.16ms
step:379/1770 train_time:36067ms step_avg:95.16ms
step:380/1770 train_time:36163ms step_avg:95.17ms
step:381/1770 train_time:36258ms step_avg:95.17ms
step:382/1770 train_time:36353ms step_avg:95.16ms
step:383/1770 train_time:36448ms step_avg:95.16ms
step:384/1770 train_time:36543ms step_avg:95.16ms
step:385/1770 train_time:36638ms step_avg:95.16ms
step:386/1770 train_time:36733ms step_avg:95.16ms
step:387/1770 train_time:36829ms step_avg:95.17ms
step:388/1770 train_time:36927ms step_avg:95.17ms
step:389/1770 train_time:37024ms step_avg:95.18ms
step:390/1770 train_time:37120ms step_avg:95.18ms
step:391/1770 train_time:37216ms step_avg:95.18ms
step:392/1770 train_time:37311ms step_avg:95.18ms
step:393/1770 train_time:37406ms step_avg:95.18ms
step:394/1770 train_time:37502ms step_avg:95.18ms
step:395/1770 train_time:37598ms step_avg:95.18ms
step:396/1770 train_time:37695ms step_avg:95.19ms
step:397/1770 train_time:37793ms step_avg:95.20ms
step:398/1770 train_time:37891ms step_avg:95.20ms
step:399/1770 train_time:37989ms step_avg:95.21ms
step:400/1770 train_time:38088ms step_avg:95.22ms
step:401/1770 train_time:38187ms step_avg:95.23ms
step:402/1770 train_time:38287ms step_avg:95.24ms
step:403/1770 train_time:38385ms step_avg:95.25ms
step:404/1770 train_time:38484ms step_avg:95.26ms
step:405/1770 train_time:38582ms step_avg:95.26ms
step:406/1770 train_time:38680ms step_avg:95.27ms
step:407/1770 train_time:38778ms step_avg:95.28ms
step:408/1770 train_time:38876ms step_avg:95.29ms
step:409/1770 train_time:38975ms step_avg:95.29ms
step:410/1770 train_time:39073ms step_avg:95.30ms
step:411/1770 train_time:39170ms step_avg:95.30ms
step:412/1770 train_time:39268ms step_avg:95.31ms
step:413/1770 train_time:39366ms step_avg:95.32ms
step:414/1770 train_time:39464ms step_avg:95.32ms
step:415/1770 train_time:39562ms step_avg:95.33ms
step:416/1770 train_time:39660ms step_avg:95.34ms
step:417/1770 train_time:39758ms step_avg:95.34ms
step:418/1770 train_time:39855ms step_avg:95.35ms
step:419/1770 train_time:39953ms step_avg:95.35ms
step:420/1770 train_time:40051ms step_avg:95.36ms
step:421/1770 train_time:40149ms step_avg:95.36ms
step:422/1770 train_time:40247ms step_avg:95.37ms
step:423/1770 train_time:40345ms step_avg:95.38ms
step:424/1770 train_time:40443ms step_avg:95.38ms
step:425/1770 train_time:40541ms step_avg:95.39ms
step:426/1770 train_time:40638ms step_avg:95.40ms
step:427/1770 train_time:40736ms step_avg:95.40ms
step:428/1770 train_time:40835ms step_avg:95.41ms
step:429/1770 train_time:40933ms step_avg:95.41ms
step:430/1770 train_time:41031ms step_avg:95.42ms
step:431/1770 train_time:41129ms step_avg:95.43ms
step:432/1770 train_time:41227ms step_avg:95.43ms
step:433/1770 train_time:41325ms step_avg:95.44ms
step:434/1770 train_time:41423ms step_avg:95.45ms
step:435/1770 train_time:41521ms step_avg:95.45ms
step:436/1770 train_time:41619ms step_avg:95.46ms
step:437/1770 train_time:41717ms step_avg:95.46ms
step:438/1770 train_time:41814ms step_avg:95.47ms
step:439/1770 train_time:41911ms step_avg:95.47ms
step:440/1770 train_time:42009ms step_avg:95.48ms
step:441/1770 train_time:42108ms step_avg:95.48ms
step:442/1770 train_time:42206ms step_avg:95.49ms
step:443/1770 train_time:42305ms step_avg:95.50ms
step:444/1770 train_time:42403ms step_avg:95.50ms
step:445/1770 train_time:42501ms step_avg:95.51ms
step:446/1770 train_time:42598ms step_avg:95.51ms
step:447/1770 train_time:42696ms step_avg:95.52ms
step:448/1770 train_time:42794ms step_avg:95.52ms
step:449/1770 train_time:42892ms step_avg:95.53ms
step:450/1770 train_time:42990ms step_avg:95.53ms
step:451/1770 train_time:43087ms step_avg:95.54ms
step:452/1770 train_time:43186ms step_avg:95.54ms
step:453/1770 train_time:43284ms step_avg:95.55ms
step:454/1770 train_time:43382ms step_avg:95.55ms
step:455/1770 train_time:43479ms step_avg:95.56ms
step:456/1770 train_time:43577ms step_avg:95.56ms
step:457/1770 train_time:43674ms step_avg:95.57ms
step:458/1770 train_time:43772ms step_avg:95.57ms
step:459/1770 train_time:43870ms step_avg:95.58ms
step:460/1770 train_time:43968ms step_avg:95.58ms
step:461/1770 train_time:44066ms step_avg:95.59ms
step:462/1770 train_time:44164ms step_avg:95.59ms
step:463/1770 train_time:44263ms step_avg:95.60ms
step:464/1770 train_time:44361ms step_avg:95.60ms
step:465/1770 train_time:44459ms step_avg:95.61ms
step:466/1770 train_time:44557ms step_avg:95.62ms
step:467/1770 train_time:44655ms step_avg:95.62ms
step:468/1770 train_time:44754ms step_avg:95.63ms
step:469/1770 train_time:44851ms step_avg:95.63ms
step:470/1770 train_time:44949ms step_avg:95.64ms
step:471/1770 train_time:45048ms step_avg:95.64ms
step:472/1770 train_time:45145ms step_avg:95.65ms
step:473/1770 train_time:45243ms step_avg:95.65ms
step:474/1770 train_time:45341ms step_avg:95.66ms
step:475/1770 train_time:45439ms step_avg:95.66ms
step:476/1770 train_time:45536ms step_avg:95.66ms
step:477/1770 train_time:45634ms step_avg:95.67ms
step:478/1770 train_time:45732ms step_avg:95.67ms
step:479/1770 train_time:45830ms step_avg:95.68ms
step:480/1770 train_time:45928ms step_avg:95.68ms
step:481/1770 train_time:46027ms step_avg:95.69ms
step:482/1770 train_time:46125ms step_avg:95.69ms
step:483/1770 train_time:46223ms step_avg:95.70ms
step:484/1770 train_time:46321ms step_avg:95.70ms
step:485/1770 train_time:46419ms step_avg:95.71ms
step:486/1770 train_time:46516ms step_avg:95.71ms
step:487/1770 train_time:46614ms step_avg:95.72ms
step:488/1770 train_time:46712ms step_avg:95.72ms
step:489/1770 train_time:46810ms step_avg:95.73ms
step:490/1770 train_time:46908ms step_avg:95.73ms
step:491/1770 train_time:47007ms step_avg:95.74ms
step:492/1770 train_time:47105ms step_avg:95.74ms
step:493/1770 train_time:47202ms step_avg:95.75ms
step:494/1770 train_time:47300ms step_avg:95.75ms
step:495/1770 train_time:47398ms step_avg:95.75ms
step:496/1770 train_time:47495ms step_avg:95.76ms
step:497/1770 train_time:47593ms step_avg:95.76ms
step:498/1770 train_time:47692ms step_avg:95.77ms
step:499/1770 train_time:47789ms step_avg:95.77ms
step:500/1770 train_time:47887ms step_avg:95.77ms
step:500/1770 val_loss:3.7468 train_time:47985ms step_avg:95.97ms
step:501/1770 train_time:48003ms step_avg:95.81ms
step:502/1770 train_time:48091ms step_avg:95.80ms
step:503/1770 train_time:48191ms step_avg:95.81ms
step:504/1770 train_time:48289ms step_avg:95.81ms
step:505/1770 train_time:48387ms step_avg:95.82ms
step:506/1770 train_time:48484ms step_avg:95.82ms
step:507/1770 train_time:48581ms step_avg:95.82ms
step:508/1770 train_time:48679ms step_avg:95.82ms
step:509/1770 train_time:48776ms step_avg:95.83ms
step:510/1770 train_time:48873ms step_avg:95.83ms
step:511/1770 train_time:48971ms step_avg:95.83ms
step:512/1770 train_time:49070ms step_avg:95.84ms
step:513/1770 train_time:49169ms step_avg:95.85ms
step:514/1770 train_time:49266ms step_avg:95.85ms
step:515/1770 train_time:49364ms step_avg:95.85ms
step:516/1770 train_time:49461ms step_avg:95.86ms
step:517/1770 train_time:49559ms step_avg:95.86ms
step:518/1770 train_time:49657ms step_avg:95.86ms
step:519/1770 train_time:49755ms step_avg:95.87ms
step:520/1770 train_time:49853ms step_avg:95.87ms
step:521/1770 train_time:49951ms step_avg:95.87ms
step:522/1770 train_time:50048ms step_avg:95.88ms
step:523/1770 train_time:50147ms step_avg:95.88ms
step:524/1770 train_time:50245ms step_avg:95.89ms
step:525/1770 train_time:50343ms step_avg:95.89ms
step:526/1770 train_time:50441ms step_avg:95.90ms
step:527/1770 train_time:50539ms step_avg:95.90ms
step:528/1770 train_time:50637ms step_avg:95.90ms
step:529/1770 train_time:50736ms step_avg:95.91ms
step:530/1770 train_time:50834ms step_avg:95.91ms
step:531/1770 train_time:50932ms step_avg:95.92ms
step:532/1770 train_time:51031ms step_avg:95.92ms
step:533/1770 train_time:51130ms step_avg:95.93ms
step:534/1770 train_time:51227ms step_avg:95.93ms
step:535/1770 train_time:51326ms step_avg:95.94ms
step:536/1770 train_time:51424ms step_avg:95.94ms
step:537/1770 train_time:51522ms step_avg:95.94ms
step:538/1770 train_time:51620ms step_avg:95.95ms
step:539/1770 train_time:51718ms step_avg:95.95ms
step:540/1770 train_time:51816ms step_avg:95.96ms
step:541/1770 train_time:51915ms step_avg:95.96ms
step:542/1770 train_time:52013ms step_avg:95.97ms
step:543/1770 train_time:52112ms step_avg:95.97ms
step:544/1770 train_time:52212ms step_avg:95.98ms
step:545/1770 train_time:52311ms step_avg:95.98ms
step:546/1770 train_time:52410ms step_avg:95.99ms
step:547/1770 train_time:52509ms step_avg:96.00ms
step:548/1770 train_time:52608ms step_avg:96.00ms
step:549/1770 train_time:52707ms step_avg:96.01ms
step:550/1770 train_time:52805ms step_avg:96.01ms
step:551/1770 train_time:52903ms step_avg:96.01ms
step:552/1770 train_time:53002ms step_avg:96.02ms
step:553/1770 train_time:53100ms step_avg:96.02ms
step:554/1770 train_time:53198ms step_avg:96.02ms
step:555/1770 train_time:53296ms step_avg:96.03ms
step:556/1770 train_time:53395ms step_avg:96.03ms
step:557/1770 train_time:53493ms step_avg:96.04ms
step:558/1770 train_time:53592ms step_avg:96.04ms
step:559/1770 train_time:53692ms step_avg:96.05ms
step:560/1770 train_time:53790ms step_avg:96.05ms
step:561/1770 train_time:53889ms step_avg:96.06ms
step:562/1770 train_time:53988ms step_avg:96.06ms
step:563/1770 train_time:54085ms step_avg:96.07ms
step:564/1770 train_time:54183ms step_avg:96.07ms
step:565/1770 train_time:54280ms step_avg:96.07ms
step:566/1770 train_time:54377ms step_avg:96.07ms
step:567/1770 train_time:54476ms step_avg:96.08ms
step:568/1770 train_time:54575ms step_avg:96.08ms
step:569/1770 train_time:54674ms step_avg:96.09ms
step:570/1770 train_time:54772ms step_avg:96.09ms
step:571/1770 train_time:54871ms step_avg:96.10ms
step:572/1770 train_time:54970ms step_avg:96.10ms
step:573/1770 train_time:55069ms step_avg:96.11ms
step:574/1770 train_time:55169ms step_avg:96.11ms
step:575/1770 train_time:55266ms step_avg:96.11ms
step:576/1770 train_time:55364ms step_avg:96.12ms
step:577/1770 train_time:55461ms step_avg:96.12ms
step:578/1770 train_time:55560ms step_avg:96.12ms
step:579/1770 train_time:55659ms step_avg:96.13ms
step:580/1770 train_time:55756ms step_avg:96.13ms
step:581/1770 train_time:55855ms step_avg:96.14ms
step:582/1770 train_time:55953ms step_avg:96.14ms
step:583/1770 train_time:56052ms step_avg:96.14ms
step:584/1770 train_time:56150ms step_avg:96.15ms
step:585/1770 train_time:56249ms step_avg:96.15ms
step:586/1770 train_time:56347ms step_avg:96.16ms
step:587/1770 train_time:56446ms step_avg:96.16ms
step:588/1770 train_time:56543ms step_avg:96.16ms
step:589/1770 train_time:56641ms step_avg:96.16ms
step:590/1770 train_time:56739ms step_avg:96.17ms
step:591/1770 train_time:56838ms step_avg:96.17ms
step:592/1770 train_time:56937ms step_avg:96.18ms
step:593/1770 train_time:57035ms step_avg:96.18ms
step:594/1770 train_time:57135ms step_avg:96.19ms
step:595/1770 train_time:57234ms step_avg:96.19ms
step:596/1770 train_time:57333ms step_avg:96.20ms
step:597/1770 train_time:57432ms step_avg:96.20ms
step:598/1770 train_time:57531ms step_avg:96.21ms
step:599/1770 train_time:57629ms step_avg:96.21ms
step:600/1770 train_time:57727ms step_avg:96.21ms
step:601/1770 train_time:57825ms step_avg:96.22ms
step:602/1770 train_time:57924ms step_avg:96.22ms
step:603/1770 train_time:58022ms step_avg:96.22ms
step:604/1770 train_time:58120ms step_avg:96.23ms
step:605/1770 train_time:58218ms step_avg:96.23ms
step:606/1770 train_time:58316ms step_avg:96.23ms
step:607/1770 train_time:58416ms step_avg:96.24ms
step:608/1770 train_time:58515ms step_avg:96.24ms
step:609/1770 train_time:58614ms step_avg:96.25ms
step:610/1770 train_time:58713ms step_avg:96.25ms
step:611/1770 train_time:58813ms step_avg:96.26ms
step:612/1770 train_time:58912ms step_avg:96.26ms
step:613/1770 train_time:59010ms step_avg:96.27ms
step:614/1770 train_time:59109ms step_avg:96.27ms
step:615/1770 train_time:59208ms step_avg:96.27ms
step:616/1770 train_time:59306ms step_avg:96.28ms
step:617/1770 train_time:59404ms step_avg:96.28ms
step:618/1770 train_time:59502ms step_avg:96.28ms
step:619/1770 train_time:59600ms step_avg:96.28ms
step:620/1770 train_time:59698ms step_avg:96.29ms
step:621/1770 train_time:59797ms step_avg:96.29ms
step:622/1770 train_time:59896ms step_avg:96.30ms
step:623/1770 train_time:59995ms step_avg:96.30ms
step:624/1770 train_time:60094ms step_avg:96.30ms
step:625/1770 train_time:60193ms step_avg:96.31ms
step:625/1770 val_loss:3.6603 train_time:60291ms step_avg:96.47ms
step:626/1770 train_time:60310ms step_avg:96.34ms
step:627/1770 train_time:60396ms step_avg:96.33ms
step:628/1770 train_time:60500ms step_avg:96.34ms
step:629/1770 train_time:60598ms step_avg:96.34ms
step:630/1770 train_time:60697ms step_avg:96.34ms
step:631/1770 train_time:60795ms step_avg:96.35ms
step:632/1770 train_time:60892ms step_avg:96.35ms
step:633/1770 train_time:60990ms step_avg:96.35ms
step:634/1770 train_time:61087ms step_avg:96.35ms
step:635/1770 train_time:61184ms step_avg:96.35ms
step:636/1770 train_time:61284ms step_avg:96.36ms
step:637/1770 train_time:61384ms step_avg:96.36ms
step:638/1770 train_time:61484ms step_avg:96.37ms
step:639/1770 train_time:61583ms step_avg:96.37ms
step:640/1770 train_time:61681ms step_avg:96.38ms
step:641/1770 train_time:61779ms step_avg:96.38ms
step:642/1770 train_time:61877ms step_avg:96.38ms
step:643/1770 train_time:61975ms step_avg:96.38ms
step:644/1770 train_time:62074ms step_avg:96.39ms
step:645/1770 train_time:62172ms step_avg:96.39ms
step:646/1770 train_time:62271ms step_avg:96.39ms
step:647/1770 train_time:62369ms step_avg:96.40ms
step:648/1770 train_time:62468ms step_avg:96.40ms
step:649/1770 train_time:62567ms step_avg:96.41ms
step:650/1770 train_time:62666ms step_avg:96.41ms
step:651/1770 train_time:62763ms step_avg:96.41ms
step:652/1770 train_time:62862ms step_avg:96.41ms
step:653/1770 train_time:62960ms step_avg:96.42ms
step:654/1770 train_time:63059ms step_avg:96.42ms
step:655/1770 train_time:63158ms step_avg:96.42ms
step:656/1770 train_time:63257ms step_avg:96.43ms
step:657/1770 train_time:63357ms step_avg:96.43ms
step:658/1770 train_time:63459ms step_avg:96.44ms
step:659/1770 train_time:63562ms step_avg:96.45ms
step:660/1770 train_time:63663ms step_avg:96.46ms
step:661/1770 train_time:63763ms step_avg:96.46ms
step:662/1770 train_time:63863ms step_avg:96.47ms
step:663/1770 train_time:63962ms step_avg:96.47ms
step:664/1770 train_time:64063ms step_avg:96.48ms
step:665/1770 train_time:64163ms step_avg:96.49ms
step:666/1770 train_time:64263ms step_avg:96.49ms
step:667/1770 train_time:64364ms step_avg:96.50ms
step:668/1770 train_time:64465ms step_avg:96.50ms
step:669/1770 train_time:64565ms step_avg:96.51ms
step:670/1770 train_time:64666ms step_avg:96.52ms
step:671/1770 train_time:64766ms step_avg:96.52ms
step:672/1770 train_time:64865ms step_avg:96.53ms
step:673/1770 train_time:64964ms step_avg:96.53ms
step:674/1770 train_time:65064ms step_avg:96.53ms
step:675/1770 train_time:65164ms step_avg:96.54ms
step:676/1770 train_time:65264ms step_avg:96.54ms
step:677/1770 train_time:65364ms step_avg:96.55ms
step:678/1770 train_time:65464ms step_avg:96.55ms
step:679/1770 train_time:65565ms step_avg:96.56ms
step:680/1770 train_time:65665ms step_avg:96.57ms
step:681/1770 train_time:65764ms step_avg:96.57ms
step:682/1770 train_time:65864ms step_avg:96.58ms
step:683/1770 train_time:65964ms step_avg:96.58ms
step:684/1770 train_time:66064ms step_avg:96.59ms
step:685/1770 train_time:66164ms step_avg:96.59ms
step:686/1770 train_time:66264ms step_avg:96.60ms
step:687/1770 train_time:66364ms step_avg:96.60ms
step:688/1770 train_time:66465ms step_avg:96.61ms
step:689/1770 train_time:66565ms step_avg:96.61ms
step:690/1770 train_time:66666ms step_avg:96.62ms
step:691/1770 train_time:66766ms step_avg:96.62ms
step:692/1770 train_time:66866ms step_avg:96.63ms
step:693/1770 train_time:66965ms step_avg:96.63ms
step:694/1770 train_time:67066ms step_avg:96.64ms
step:695/1770 train_time:67166ms step_avg:96.64ms
step:696/1770 train_time:67266ms step_avg:96.65ms
step:697/1770 train_time:67366ms step_avg:96.65ms
step:698/1770 train_time:67465ms step_avg:96.66ms
step:699/1770 train_time:67566ms step_avg:96.66ms
step:700/1770 train_time:67665ms step_avg:96.66ms
step:701/1770 train_time:67765ms step_avg:96.67ms
step:702/1770 train_time:67864ms step_avg:96.67ms
step:703/1770 train_time:67965ms step_avg:96.68ms
step:704/1770 train_time:68065ms step_avg:96.68ms
step:705/1770 train_time:68165ms step_avg:96.69ms
step:706/1770 train_time:68266ms step_avg:96.69ms
step:707/1770 train_time:68366ms step_avg:96.70ms
step:708/1770 train_time:68467ms step_avg:96.70ms
step:709/1770 train_time:68567ms step_avg:96.71ms
step:710/1770 train_time:68667ms step_avg:96.71ms
step:711/1770 train_time:68767ms step_avg:96.72ms
step:712/1770 train_time:68866ms step_avg:96.72ms
step:713/1770 train_time:68966ms step_avg:96.73ms
step:714/1770 train_time:69065ms step_avg:96.73ms
step:715/1770 train_time:69165ms step_avg:96.73ms
step:716/1770 train_time:69265ms step_avg:96.74ms
step:717/1770 train_time:69365ms step_avg:96.74ms
step:718/1770 train_time:69465ms step_avg:96.75ms
step:719/1770 train_time:69566ms step_avg:96.75ms
step:720/1770 train_time:69666ms step_avg:96.76ms
step:721/1770 train_time:69765ms step_avg:96.76ms
step:722/1770 train_time:69865ms step_avg:96.77ms
step:723/1770 train_time:69966ms step_avg:96.77ms
step:724/1770 train_time:70066ms step_avg:96.78ms
step:725/1770 train_time:70166ms step_avg:96.78ms
step:726/1770 train_time:70266ms step_avg:96.78ms
step:727/1770 train_time:70365ms step_avg:96.79ms
step:728/1770 train_time:70465ms step_avg:96.79ms
step:729/1770 train_time:70565ms step_avg:96.80ms
step:730/1770 train_time:70665ms step_avg:96.80ms
step:731/1770 train_time:70765ms step_avg:96.81ms
step:732/1770 train_time:70865ms step_avg:96.81ms
step:733/1770 train_time:70965ms step_avg:96.81ms
step:734/1770 train_time:71064ms step_avg:96.82ms
step:735/1770 train_time:71164ms step_avg:96.82ms
step:736/1770 train_time:71264ms step_avg:96.83ms
step:737/1770 train_time:71364ms step_avg:96.83ms
step:738/1770 train_time:71464ms step_avg:96.83ms
step:739/1770 train_time:71565ms step_avg:96.84ms
step:740/1770 train_time:71666ms step_avg:96.85ms
step:741/1770 train_time:71766ms step_avg:96.85ms
step:742/1770 train_time:71867ms step_avg:96.86ms
step:743/1770 train_time:71967ms step_avg:96.86ms
step:744/1770 train_time:72066ms step_avg:96.86ms
step:745/1770 train_time:72166ms step_avg:96.87ms
step:746/1770 train_time:72266ms step_avg:96.87ms
step:747/1770 train_time:72365ms step_avg:96.87ms
step:748/1770 train_time:72465ms step_avg:96.88ms
step:749/1770 train_time:72565ms step_avg:96.88ms
step:750/1770 train_time:72666ms step_avg:96.89ms
step:750/1770 val_loss:3.5991 train_time:72765ms step_avg:97.02ms
step:751/1770 train_time:72782ms step_avg:96.91ms
step:752/1770 train_time:72873ms step_avg:96.91ms
step:753/1770 train_time:72974ms step_avg:96.91ms
step:754/1770 train_time:73074ms step_avg:96.91ms
step:755/1770 train_time:73173ms step_avg:96.92ms
step:756/1770 train_time:73272ms step_avg:96.92ms
step:757/1770 train_time:73371ms step_avg:96.92ms
step:758/1770 train_time:73469ms step_avg:96.93ms
step:759/1770 train_time:73568ms step_avg:96.93ms
step:760/1770 train_time:73668ms step_avg:96.93ms
step:761/1770 train_time:73769ms step_avg:96.94ms
step:762/1770 train_time:73871ms step_avg:96.94ms
step:763/1770 train_time:73972ms step_avg:96.95ms
step:764/1770 train_time:74072ms step_avg:96.95ms
step:765/1770 train_time:74172ms step_avg:96.96ms
step:766/1770 train_time:74270ms step_avg:96.96ms
step:767/1770 train_time:74370ms step_avg:96.96ms
step:768/1770 train_time:74469ms step_avg:96.97ms
step:769/1770 train_time:74569ms step_avg:96.97ms
step:770/1770 train_time:74669ms step_avg:96.97ms
step:771/1770 train_time:74769ms step_avg:96.98ms
step:772/1770 train_time:74869ms step_avg:96.98ms
step:773/1770 train_time:74970ms step_avg:96.99ms
step:774/1770 train_time:75070ms step_avg:96.99ms
step:775/1770 train_time:75169ms step_avg:96.99ms
step:776/1770 train_time:75269ms step_avg:97.00ms
step:777/1770 train_time:75369ms step_avg:97.00ms
step:778/1770 train_time:75469ms step_avg:97.00ms
step:779/1770 train_time:75568ms step_avg:97.01ms
step:780/1770 train_time:75669ms step_avg:97.01ms
step:781/1770 train_time:75769ms step_avg:97.02ms
step:782/1770 train_time:75869ms step_avg:97.02ms
step:783/1770 train_time:75970ms step_avg:97.02ms
step:784/1770 train_time:76071ms step_avg:97.03ms
step:785/1770 train_time:76171ms step_avg:97.03ms
step:786/1770 train_time:76271ms step_avg:97.04ms
step:787/1770 train_time:76370ms step_avg:97.04ms
step:788/1770 train_time:76470ms step_avg:97.04ms
step:789/1770 train_time:76569ms step_avg:97.05ms
step:790/1770 train_time:76669ms step_avg:97.05ms
step:791/1770 train_time:76769ms step_avg:97.05ms
step:792/1770 train_time:76869ms step_avg:97.06ms
step:793/1770 train_time:76969ms step_avg:97.06ms
step:794/1770 train_time:77069ms step_avg:97.06ms
step:795/1770 train_time:77169ms step_avg:97.07ms
step:796/1770 train_time:77269ms step_avg:97.07ms
step:797/1770 train_time:77369ms step_avg:97.08ms
step:798/1770 train_time:77470ms step_avg:97.08ms
step:799/1770 train_time:77569ms step_avg:97.08ms
step:800/1770 train_time:77670ms step_avg:97.09ms
step:801/1770 train_time:77770ms step_avg:97.09ms
step:802/1770 train_time:77870ms step_avg:97.09ms
step:803/1770 train_time:77969ms step_avg:97.10ms
step:804/1770 train_time:78069ms step_avg:97.10ms
step:805/1770 train_time:78169ms step_avg:97.10ms
step:806/1770 train_time:78269ms step_avg:97.11ms
step:807/1770 train_time:78369ms step_avg:97.11ms
step:808/1770 train_time:78469ms step_avg:97.12ms
step:809/1770 train_time:78569ms step_avg:97.12ms
step:810/1770 train_time:78670ms step_avg:97.12ms
step:811/1770 train_time:78770ms step_avg:97.13ms
step:812/1770 train_time:78870ms step_avg:97.13ms
step:813/1770 train_time:78970ms step_avg:97.13ms
step:814/1770 train_time:79070ms step_avg:97.14ms
step:815/1770 train_time:79169ms step_avg:97.14ms
step:816/1770 train_time:79269ms step_avg:97.14ms
step:817/1770 train_time:79369ms step_avg:97.15ms
step:818/1770 train_time:79469ms step_avg:97.15ms
step:819/1770 train_time:79569ms step_avg:97.15ms
step:820/1770 train_time:79669ms step_avg:97.16ms
step:821/1770 train_time:79769ms step_avg:97.16ms
step:822/1770 train_time:79870ms step_avg:97.17ms
step:823/1770 train_time:79970ms step_avg:97.17ms
step:824/1770 train_time:80071ms step_avg:97.17ms
step:825/1770 train_time:80170ms step_avg:97.18ms
step:826/1770 train_time:80270ms step_avg:97.18ms
step:827/1770 train_time:80369ms step_avg:97.18ms
step:828/1770 train_time:80469ms step_avg:97.19ms
step:829/1770 train_time:80569ms step_avg:97.19ms
step:830/1770 train_time:80669ms step_avg:97.19ms
step:831/1770 train_time:80769ms step_avg:97.20ms
step:832/1770 train_time:80869ms step_avg:97.20ms
step:833/1770 train_time:80970ms step_avg:97.20ms
step:834/1770 train_time:81071ms step_avg:97.21ms
step:835/1770 train_time:81171ms step_avg:97.21ms
step:836/1770 train_time:81272ms step_avg:97.21ms
step:837/1770 train_time:81371ms step_avg:97.22ms
step:838/1770 train_time:81471ms step_avg:97.22ms
step:839/1770 train_time:81571ms step_avg:97.22ms
step:840/1770 train_time:81671ms step_avg:97.23ms
step:841/1770 train_time:81771ms step_avg:97.23ms
step:842/1770 train_time:81871ms step_avg:97.23ms
step:843/1770 train_time:81971ms step_avg:97.24ms
step:844/1770 train_time:82070ms step_avg:97.24ms
step:845/1770 train_time:82170ms step_avg:97.24ms
step:846/1770 train_time:82270ms step_avg:97.25ms
step:847/1770 train_time:82370ms step_avg:97.25ms
step:848/1770 train_time:82470ms step_avg:97.25ms
step:849/1770 train_time:82569ms step_avg:97.26ms
step:850/1770 train_time:82670ms step_avg:97.26ms
step:851/1770 train_time:82770ms step_avg:97.26ms
step:852/1770 train_time:82871ms step_avg:97.27ms
step:853/1770 train_time:82971ms step_avg:97.27ms
step:854/1770 train_time:83071ms step_avg:97.27ms
step:855/1770 train_time:83170ms step_avg:97.27ms
step:856/1770 train_time:83269ms step_avg:97.28ms
step:857/1770 train_time:83369ms step_avg:97.28ms
step:858/1770 train_time:83469ms step_avg:97.28ms
step:859/1770 train_time:83569ms step_avg:97.29ms
step:860/1770 train_time:83669ms step_avg:97.29ms
step:861/1770 train_time:83769ms step_avg:97.29ms
step:862/1770 train_time:83869ms step_avg:97.30ms
step:863/1770 train_time:83970ms step_avg:97.30ms
step:864/1770 train_time:84070ms step_avg:97.30ms
step:865/1770 train_time:84170ms step_avg:97.31ms
step:866/1770 train_time:84270ms step_avg:97.31ms
step:867/1770 train_time:84370ms step_avg:97.31ms
step:868/1770 train_time:84469ms step_avg:97.32ms
step:869/1770 train_time:84569ms step_avg:97.32ms
step:870/1770 train_time:84670ms step_avg:97.32ms
step:871/1770 train_time:84770ms step_avg:97.32ms
step:872/1770 train_time:84870ms step_avg:97.33ms
step:873/1770 train_time:84970ms step_avg:97.33ms
step:874/1770 train_time:85070ms step_avg:97.33ms
step:875/1770 train_time:85171ms step_avg:97.34ms
step:875/1770 val_loss:3.5500 train_time:85270ms step_avg:97.45ms
step:876/1770 train_time:85287ms step_avg:97.36ms
step:877/1770 train_time:85379ms step_avg:97.35ms
step:878/1770 train_time:85481ms step_avg:97.36ms
step:879/1770 train_time:85581ms step_avg:97.36ms
step:880/1770 train_time:85681ms step_avg:97.36ms
step:881/1770 train_time:85781ms step_avg:97.37ms
step:882/1770 train_time:85880ms step_avg:97.37ms
step:883/1770 train_time:85980ms step_avg:97.37ms
step:884/1770 train_time:86080ms step_avg:97.38ms
step:885/1770 train_time:86179ms step_avg:97.38ms
step:886/1770 train_time:86279ms step_avg:97.38ms
step:887/1770 train_time:86380ms step_avg:97.38ms
step:888/1770 train_time:86481ms step_avg:97.39ms
step:889/1770 train_time:86581ms step_avg:97.39ms
step:890/1770 train_time:86681ms step_avg:97.39ms
step:891/1770 train_time:86781ms step_avg:97.40ms
step:892/1770 train_time:86883ms step_avg:97.40ms
step:893/1770 train_time:86983ms step_avg:97.41ms
step:894/1770 train_time:87084ms step_avg:97.41ms
step:895/1770 train_time:87184ms step_avg:97.41ms
step:896/1770 train_time:87284ms step_avg:97.42ms
step:897/1770 train_time:87385ms step_avg:97.42ms
step:898/1770 train_time:87486ms step_avg:97.42ms
step:899/1770 train_time:87587ms step_avg:97.43ms
step:900/1770 train_time:87689ms step_avg:97.43ms
step:901/1770 train_time:87790ms step_avg:97.44ms
step:902/1770 train_time:87891ms step_avg:97.44ms
step:903/1770 train_time:87992ms step_avg:97.44ms
step:904/1770 train_time:88093ms step_avg:97.45ms
step:905/1770 train_time:88193ms step_avg:97.45ms
step:906/1770 train_time:88294ms step_avg:97.45ms
step:907/1770 train_time:88394ms step_avg:97.46ms
step:908/1770 train_time:88496ms step_avg:97.46ms
step:909/1770 train_time:88596ms step_avg:97.47ms
step:910/1770 train_time:88697ms step_avg:97.47ms
step:911/1770 train_time:88797ms step_avg:97.47ms
step:912/1770 train_time:88897ms step_avg:97.48ms
step:913/1770 train_time:88997ms step_avg:97.48ms
step:914/1770 train_time:89097ms step_avg:97.48ms
step:915/1770 train_time:89197ms step_avg:97.48ms
step:916/1770 train_time:89296ms step_avg:97.49ms
step:917/1770 train_time:89397ms step_avg:97.49ms
step:918/1770 train_time:89497ms step_avg:97.49ms
step:919/1770 train_time:89597ms step_avg:97.49ms
step:920/1770 train_time:89699ms step_avg:97.50ms
step:921/1770 train_time:89801ms step_avg:97.50ms
step:922/1770 train_time:89903ms step_avg:97.51ms
step:923/1770 train_time:90005ms step_avg:97.51ms
step:924/1770 train_time:90107ms step_avg:97.52ms
step:925/1770 train_time:90209ms step_avg:97.52ms
step:926/1770 train_time:90312ms step_avg:97.53ms
step:927/1770 train_time:90415ms step_avg:97.53ms
step:928/1770 train_time:90517ms step_avg:97.54ms
step:929/1770 train_time:90618ms step_avg:97.54ms
step:930/1770 train_time:90720ms step_avg:97.55ms
step:931/1770 train_time:90821ms step_avg:97.55ms
step:932/1770 train_time:90922ms step_avg:97.56ms
step:933/1770 train_time:91023ms step_avg:97.56ms
step:934/1770 train_time:91124ms step_avg:97.56ms
step:935/1770 train_time:91226ms step_avg:97.57ms
step:936/1770 train_time:91328ms step_avg:97.57ms
step:937/1770 train_time:91432ms step_avg:97.58ms
step:938/1770 train_time:91535ms step_avg:97.59ms
step:939/1770 train_time:91637ms step_avg:97.59ms
step:940/1770 train_time:91738ms step_avg:97.59ms
step:941/1770 train_time:91839ms step_avg:97.60ms
step:942/1770 train_time:91941ms step_avg:97.60ms
step:943/1770 train_time:92042ms step_avg:97.61ms
step:944/1770 train_time:92142ms step_avg:97.61ms
step:945/1770 train_time:92244ms step_avg:97.61ms
step:946/1770 train_time:92347ms step_avg:97.62ms
step:947/1770 train_time:92451ms step_avg:97.62ms
step:948/1770 train_time:92553ms step_avg:97.63ms
step:949/1770 train_time:92656ms step_avg:97.64ms
step:950/1770 train_time:92758ms step_avg:97.64ms
step:951/1770 train_time:92860ms step_avg:97.64ms
step:952/1770 train_time:92961ms step_avg:97.65ms
step:953/1770 train_time:93062ms step_avg:97.65ms
step:954/1770 train_time:93162ms step_avg:97.65ms
step:955/1770 train_time:93263ms step_avg:97.66ms
step:956/1770 train_time:93365ms step_avg:97.66ms
step:957/1770 train_time:93469ms step_avg:97.67ms
step:958/1770 train_time:93571ms step_avg:97.67ms
step:959/1770 train_time:93675ms step_avg:97.68ms
step:960/1770 train_time:93777ms step_avg:97.68ms
step:961/1770 train_time:93880ms step_avg:97.69ms
step:962/1770 train_time:93981ms step_avg:97.69ms
step:963/1770 train_time:94082ms step_avg:97.70ms
step:964/1770 train_time:94184ms step_avg:97.70ms
step:965/1770 train_time:94285ms step_avg:97.70ms
step:966/1770 train_time:94387ms step_avg:97.71ms
step:967/1770 train_time:94488ms step_avg:97.71ms
step:968/1770 train_time:94591ms step_avg:97.72ms
step:969/1770 train_time:94694ms step_avg:97.72ms
step:970/1770 train_time:94797ms step_avg:97.73ms
step:971/1770 train_time:94899ms step_avg:97.73ms
step:972/1770 train_time:95001ms step_avg:97.74ms
step:973/1770 train_time:95102ms step_avg:97.74ms
step:974/1770 train_time:95203ms step_avg:97.74ms
step:975/1770 train_time:95304ms step_avg:97.75ms
step:976/1770 train_time:95406ms step_avg:97.75ms
step:977/1770 train_time:95508ms step_avg:97.76ms
step:978/1770 train_time:95610ms step_avg:97.76ms
step:979/1770 train_time:95714ms step_avg:97.77ms
step:980/1770 train_time:95816ms step_avg:97.77ms
step:981/1770 train_time:95918ms step_avg:97.78ms
step:982/1770 train_time:96019ms step_avg:97.78ms
step:983/1770 train_time:96121ms step_avg:97.78ms
step:984/1770 train_time:96222ms step_avg:97.79ms
step:985/1770 train_time:96323ms step_avg:97.79ms
step:986/1770 train_time:96423ms step_avg:97.79ms
step:987/1770 train_time:96526ms step_avg:97.80ms
step:988/1770 train_time:96628ms step_avg:97.80ms
step:989/1770 train_time:96732ms step_avg:97.81ms
step:990/1770 train_time:96834ms step_avg:97.81ms
step:991/1770 train_time:96937ms step_avg:97.82ms
step:992/1770 train_time:97039ms step_avg:97.82ms
step:993/1770 train_time:97141ms step_avg:97.83ms
step:994/1770 train_time:97243ms step_avg:97.83ms
step:995/1770 train_time:97344ms step_avg:97.83ms
step:996/1770 train_time:97445ms step_avg:97.84ms
step:997/1770 train_time:97547ms step_avg:97.84ms
step:998/1770 train_time:97649ms step_avg:97.84ms
step:999/1770 train_time:97752ms step_avg:97.85ms
step:1000/1770 train_time:97854ms step_avg:97.85ms
step:1000/1770 val_loss:3.5128 train_time:97956ms step_avg:97.96ms
step:1001/1770 train_time:97973ms step_avg:97.88ms
step:1002/1770 train_time:98066ms step_avg:97.87ms
step:1003/1770 train_time:98170ms step_avg:97.88ms
step:1004/1770 train_time:98273ms step_avg:97.88ms
step:1005/1770 train_time:98374ms step_avg:97.88ms
step:1006/1770 train_time:98475ms step_avg:97.89ms
step:1007/1770 train_time:98576ms step_avg:97.89ms
step:1008/1770 train_time:98677ms step_avg:97.89ms
step:1009/1770 train_time:98777ms step_avg:97.90ms
step:1010/1770 train_time:98879ms step_avg:97.90ms
step:1011/1770 train_time:98982ms step_avg:97.90ms
step:1012/1770 train_time:99084ms step_avg:97.91ms
step:1013/1770 train_time:99187ms step_avg:97.91ms
step:1014/1770 train_time:99289ms step_avg:97.92ms
step:1015/1770 train_time:99392ms step_avg:97.92ms
step:1016/1770 train_time:99494ms step_avg:97.93ms
step:1017/1770 train_time:99595ms step_avg:97.93ms
step:1018/1770 train_time:99696ms step_avg:97.93ms
step:1019/1770 train_time:99797ms step_avg:97.94ms
step:1020/1770 train_time:99899ms step_avg:97.94ms
step:1021/1770 train_time:100001ms step_avg:97.94ms
step:1022/1770 train_time:100102ms step_avg:97.95ms
step:1023/1770 train_time:100203ms step_avg:97.95ms
step:1024/1770 train_time:100306ms step_avg:97.96ms
step:1025/1770 train_time:100409ms step_avg:97.96ms
step:1026/1770 train_time:100512ms step_avg:97.97ms
step:1027/1770 train_time:100615ms step_avg:97.97ms
step:1028/1770 train_time:100717ms step_avg:97.97ms
step:1029/1770 train_time:100819ms step_avg:97.98ms
step:1030/1770 train_time:100920ms step_avg:97.98ms
step:1031/1770 train_time:101022ms step_avg:97.98ms
step:1032/1770 train_time:101123ms step_avg:97.99ms
step:1033/1770 train_time:101225ms step_avg:97.99ms
step:1034/1770 train_time:101329ms step_avg:98.00ms
step:1035/1770 train_time:101431ms step_avg:98.00ms
step:1036/1770 train_time:101533ms step_avg:98.00ms
step:1037/1770 train_time:101636ms step_avg:98.01ms
step:1038/1770 train_time:101737ms step_avg:98.01ms
step:1039/1770 train_time:101839ms step_avg:98.02ms
step:1040/1770 train_time:101940ms step_avg:98.02ms
step:1041/1770 train_time:102041ms step_avg:98.02ms
step:1042/1770 train_time:102143ms step_avg:98.03ms
step:1043/1770 train_time:102245ms step_avg:98.03ms
step:1044/1770 train_time:102348ms step_avg:98.03ms
step:1045/1770 train_time:102451ms step_avg:98.04ms
step:1046/1770 train_time:102554ms step_avg:98.04ms
step:1047/1770 train_time:102656ms step_avg:98.05ms
step:1048/1770 train_time:102758ms step_avg:98.05ms
step:1049/1770 train_time:102860ms step_avg:98.05ms
step:1050/1770 train_time:102960ms step_avg:98.06ms
step:1051/1770 train_time:103061ms step_avg:98.06ms
step:1052/1770 train_time:103162ms step_avg:98.06ms
step:1053/1770 train_time:103264ms step_avg:98.07ms
step:1054/1770 train_time:103367ms step_avg:98.07ms
step:1055/1770 train_time:103471ms step_avg:98.08ms
step:1056/1770 train_time:103573ms step_avg:98.08ms
step:1057/1770 train_time:103675ms step_avg:98.08ms
step:1058/1770 train_time:103777ms step_avg:98.09ms
step:1059/1770 train_time:103878ms step_avg:98.09ms
step:1060/1770 train_time:103980ms step_avg:98.09ms
step:1061/1770 train_time:104081ms step_avg:98.10ms
step:1062/1770 train_time:104183ms step_avg:98.10ms
step:1063/1770 train_time:104287ms step_avg:98.11ms
step:1064/1770 train_time:104389ms step_avg:98.11ms
step:1065/1770 train_time:104492ms step_avg:98.11ms
step:1066/1770 train_time:104595ms step_avg:98.12ms
step:1067/1770 train_time:104698ms step_avg:98.12ms
step:1068/1770 train_time:104801ms step_avg:98.13ms
step:1069/1770 train_time:104903ms step_avg:98.13ms
step:1070/1770 train_time:105004ms step_avg:98.13ms
step:1071/1770 train_time:105107ms step_avg:98.14ms
step:1072/1770 train_time:105209ms step_avg:98.14ms
step:1073/1770 train_time:105311ms step_avg:98.15ms
step:1074/1770 train_time:105413ms step_avg:98.15ms
step:1075/1770 train_time:105515ms step_avg:98.15ms
step:1076/1770 train_time:105618ms step_avg:98.16ms
step:1077/1770 train_time:105720ms step_avg:98.16ms
step:1078/1770 train_time:105821ms step_avg:98.16ms
step:1079/1770 train_time:105923ms step_avg:98.17ms
step:1080/1770 train_time:106025ms step_avg:98.17ms
step:1081/1770 train_time:106128ms step_avg:98.18ms
step:1082/1770 train_time:106230ms step_avg:98.18ms
step:1083/1770 train_time:106332ms step_avg:98.18ms
step:1084/1770 train_time:106434ms step_avg:98.19ms
step:1085/1770 train_time:106537ms step_avg:98.19ms
step:1086/1770 train_time:106638ms step_avg:98.19ms
step:1087/1770 train_time:106740ms step_avg:98.20ms
step:1088/1770 train_time:106841ms step_avg:98.20ms
step:1089/1770 train_time:106943ms step_avg:98.20ms
step:1090/1770 train_time:107045ms step_avg:98.21ms
step:1091/1770 train_time:107148ms step_avg:98.21ms
step:1092/1770 train_time:107250ms step_avg:98.21ms
step:1093/1770 train_time:107354ms step_avg:98.22ms
step:1094/1770 train_time:107457ms step_avg:98.22ms
step:1095/1770 train_time:107558ms step_avg:98.23ms
step:1096/1770 train_time:107660ms step_avg:98.23ms
step:1097/1770 train_time:107761ms step_avg:98.23ms
step:1098/1770 train_time:107862ms step_avg:98.23ms
step:1099/1770 train_time:107964ms step_avg:98.24ms
step:1100/1770 train_time:108066ms step_avg:98.24ms
step:1101/1770 train_time:108168ms step_avg:98.25ms
step:1102/1770 train_time:108271ms step_avg:98.25ms
step:1103/1770 train_time:108375ms step_avg:98.25ms
step:1104/1770 train_time:108477ms step_avg:98.26ms
step:1105/1770 train_time:108578ms step_avg:98.26ms
step:1106/1770 train_time:108680ms step_avg:98.26ms
step:1107/1770 train_time:108781ms step_avg:98.27ms
step:1108/1770 train_time:108884ms step_avg:98.27ms
step:1109/1770 train_time:108986ms step_avg:98.27ms
step:1110/1770 train_time:109088ms step_avg:98.28ms
step:1111/1770 train_time:109190ms step_avg:98.28ms
step:1112/1770 train_time:109293ms step_avg:98.29ms
step:1113/1770 train_time:109395ms step_avg:98.29ms
step:1114/1770 train_time:109497ms step_avg:98.29ms
step:1115/1770 train_time:109599ms step_avg:98.30ms
step:1116/1770 train_time:109701ms step_avg:98.30ms
step:1117/1770 train_time:109803ms step_avg:98.30ms
step:1118/1770 train_time:109905ms step_avg:98.30ms
step:1119/1770 train_time:110007ms step_avg:98.31ms
step:1120/1770 train_time:110109ms step_avg:98.31ms
step:1121/1770 train_time:110212ms step_avg:98.32ms
step:1122/1770 train_time:110315ms step_avg:98.32ms
step:1123/1770 train_time:110417ms step_avg:98.32ms
step:1124/1770 train_time:110519ms step_avg:98.33ms
step:1125/1770 train_time:110620ms step_avg:98.33ms
step:1125/1770 val_loss:3.4720 train_time:110720ms step_avg:98.42ms
step:1126/1770 train_time:110738ms step_avg:98.35ms
step:1127/1770 train_time:110828ms step_avg:98.34ms
step:1128/1770 train_time:110930ms step_avg:98.34ms
step:1129/1770 train_time:111031ms step_avg:98.34ms
step:1130/1770 train_time:111133ms step_avg:98.35ms
step:1131/1770 train_time:111234ms step_avg:98.35ms
step:1132/1770 train_time:111335ms step_avg:98.35ms
step:1133/1770 train_time:111436ms step_avg:98.35ms
step:1134/1770 train_time:111537ms step_avg:98.36ms
step:1135/1770 train_time:111639ms step_avg:98.36ms
step:1136/1770 train_time:111744ms step_avg:98.37ms
step:1137/1770 train_time:111848ms step_avg:98.37ms
step:1138/1770 train_time:111951ms step_avg:98.38ms
step:1139/1770 train_time:112053ms step_avg:98.38ms
step:1140/1770 train_time:112155ms step_avg:98.38ms
step:1141/1770 train_time:112256ms step_avg:98.38ms
step:1142/1770 train_time:112357ms step_avg:98.39ms
step:1143/1770 train_time:112459ms step_avg:98.39ms
step:1144/1770 train_time:112561ms step_avg:98.39ms
step:1145/1770 train_time:112664ms step_avg:98.40ms
step:1146/1770 train_time:112768ms step_avg:98.40ms
step:1147/1770 train_time:112870ms step_avg:98.40ms
step:1148/1770 train_time:112972ms step_avg:98.41ms
step:1149/1770 train_time:113074ms step_avg:98.41ms
step:1150/1770 train_time:113176ms step_avg:98.41ms
step:1151/1770 train_time:113277ms step_avg:98.42ms
step:1152/1770 train_time:113380ms step_avg:98.42ms
step:1153/1770 train_time:113481ms step_avg:98.42ms
step:1154/1770 train_time:113584ms step_avg:98.43ms
step:1155/1770 train_time:113686ms step_avg:98.43ms
step:1156/1770 train_time:113788ms step_avg:98.43ms
step:1157/1770 train_time:113891ms step_avg:98.44ms
step:1158/1770 train_time:113993ms step_avg:98.44ms
step:1159/1770 train_time:114094ms step_avg:98.44ms
step:1160/1770 train_time:114196ms step_avg:98.44ms
step:1161/1770 train_time:114298ms step_avg:98.45ms
step:1162/1770 train_time:114400ms step_avg:98.45ms
step:1163/1770 train_time:114503ms step_avg:98.45ms
step:1164/1770 train_time:114605ms step_avg:98.46ms
step:1165/1770 train_time:114708ms step_avg:98.46ms
step:1166/1770 train_time:114810ms step_avg:98.46ms
step:1167/1770 train_time:114911ms step_avg:98.47ms
step:1168/1770 train_time:115013ms step_avg:98.47ms
step:1169/1770 train_time:115114ms step_avg:98.47ms
step:1170/1770 train_time:115216ms step_avg:98.48ms
step:1171/1770 train_time:115318ms step_avg:98.48ms
step:1172/1770 train_time:115420ms step_avg:98.48ms
step:1173/1770 train_time:115522ms step_avg:98.48ms
step:1174/1770 train_time:115626ms step_avg:98.49ms
step:1175/1770 train_time:115729ms step_avg:98.49ms
step:1176/1770 train_time:115831ms step_avg:98.50ms
step:1177/1770 train_time:115933ms step_avg:98.50ms
step:1178/1770 train_time:116034ms step_avg:98.50ms
step:1179/1770 train_time:116136ms step_avg:98.50ms
step:1180/1770 train_time:116238ms step_avg:98.51ms
step:1181/1770 train_time:116339ms step_avg:98.51ms
step:1182/1770 train_time:116442ms step_avg:98.51ms
step:1183/1770 train_time:116546ms step_avg:98.52ms
step:1184/1770 train_time:116651ms step_avg:98.52ms
step:1185/1770 train_time:116754ms step_avg:98.53ms
step:1186/1770 train_time:116858ms step_avg:98.53ms
step:1187/1770 train_time:116963ms step_avg:98.54ms
step:1188/1770 train_time:117067ms step_avg:98.54ms
step:1189/1770 train_time:117170ms step_avg:98.55ms
step:1190/1770 train_time:117272ms step_avg:98.55ms
step:1191/1770 train_time:117375ms step_avg:98.55ms
step:1192/1770 train_time:117478ms step_avg:98.56ms
step:1193/1770 train_time:117583ms step_avg:98.56ms
step:1194/1770 train_time:117686ms step_avg:98.56ms
step:1195/1770 train_time:117790ms step_avg:98.57ms
step:1196/1770 train_time:117894ms step_avg:98.57ms
step:1197/1770 train_time:117996ms step_avg:98.58ms
step:1198/1770 train_time:118100ms step_avg:98.58ms
step:1199/1770 train_time:118204ms step_avg:98.59ms
step:1200/1770 train_time:118308ms step_avg:98.59ms
step:1201/1770 train_time:118412ms step_avg:98.59ms
step:1202/1770 train_time:118514ms step_avg:98.60ms
step:1203/1770 train_time:118617ms step_avg:98.60ms
step:1204/1770 train_time:118722ms step_avg:98.61ms
step:1205/1770 train_time:118825ms step_avg:98.61ms
step:1206/1770 train_time:118929ms step_avg:98.61ms
step:1207/1770 train_time:119032ms step_avg:98.62ms
step:1208/1770 train_time:119134ms step_avg:98.62ms
step:1209/1770 train_time:119237ms step_avg:98.62ms
step:1210/1770 train_time:119341ms step_avg:98.63ms
step:1211/1770 train_time:119444ms step_avg:98.63ms
step:1212/1770 train_time:119548ms step_avg:98.64ms
step:1213/1770 train_time:119651ms step_avg:98.64ms
step:1214/1770 train_time:119753ms step_avg:98.64ms
step:1215/1770 train_time:119856ms step_avg:98.65ms
step:1216/1770 train_time:119962ms step_avg:98.65ms
step:1217/1770 train_time:120066ms step_avg:98.66ms
step:1218/1770 train_time:120169ms step_avg:98.66ms
step:1219/1770 train_time:120272ms step_avg:98.66ms
step:1220/1770 train_time:120375ms step_avg:98.67ms
step:1221/1770 train_time:120478ms step_avg:98.67ms
step:1222/1770 train_time:120583ms step_avg:98.68ms
step:1223/1770 train_time:120686ms step_avg:98.68ms
step:1224/1770 train_time:120789ms step_avg:98.68ms
step:1225/1770 train_time:120893ms step_avg:98.69ms
step:1226/1770 train_time:120995ms step_avg:98.69ms
step:1227/1770 train_time:121099ms step_avg:98.70ms
step:1228/1770 train_time:121204ms step_avg:98.70ms
step:1229/1770 train_time:121307ms step_avg:98.70ms
step:1230/1770 train_time:121410ms step_avg:98.71ms
step:1231/1770 train_time:121513ms step_avg:98.71ms
step:1232/1770 train_time:121615ms step_avg:98.71ms
step:1233/1770 train_time:121718ms step_avg:98.72ms
step:1234/1770 train_time:121821ms step_avg:98.72ms
step:1235/1770 train_time:121925ms step_avg:98.72ms
step:1236/1770 train_time:122029ms step_avg:98.73ms
step:1237/1770 train_time:122132ms step_avg:98.73ms
step:1238/1770 train_time:122235ms step_avg:98.74ms
step:1239/1770 train_time:122339ms step_avg:98.74ms
step:1240/1770 train_time:122442ms step_avg:98.74ms
step:1241/1770 train_time:122547ms step_avg:98.75ms
step:1242/1770 train_time:122650ms step_avg:98.75ms
step:1243/1770 train_time:122753ms step_avg:98.76ms
step:1244/1770 train_time:122855ms step_avg:98.76ms
step:1245/1770 train_time:122959ms step_avg:98.76ms
step:1246/1770 train_time:123062ms step_avg:98.77ms
step:1247/1770 train_time:123166ms step_avg:98.77ms
step:1248/1770 train_time:123270ms step_avg:98.77ms
step:1249/1770 train_time:123372ms step_avg:98.78ms
step:1250/1770 train_time:123476ms step_avg:98.78ms
step:1250/1770 val_loss:3.4249 train_time:123579ms step_avg:98.86ms
step:1251/1770 train_time:123597ms step_avg:98.80ms
step:1252/1770 train_time:123690ms step_avg:98.79ms
step:1253/1770 train_time:123794ms step_avg:98.80ms
step:1254/1770 train_time:123896ms step_avg:98.80ms
step:1255/1770 train_time:124001ms step_avg:98.81ms
step:1256/1770 train_time:124104ms step_avg:98.81ms
step:1257/1770 train_time:124206ms step_avg:98.81ms
step:1258/1770 train_time:124308ms step_avg:98.81ms
step:1259/1770 train_time:124411ms step_avg:98.82ms
step:1260/1770 train_time:124514ms step_avg:98.82ms
step:1261/1770 train_time:124621ms step_avg:98.83ms
step:1262/1770 train_time:124725ms step_avg:98.83ms
step:1263/1770 train_time:124828ms step_avg:98.83ms
step:1264/1770 train_time:124931ms step_avg:98.84ms
step:1265/1770 train_time:125034ms step_avg:98.84ms
step:1266/1770 train_time:125138ms step_avg:98.85ms
step:1267/1770 train_time:125242ms step_avg:98.85ms
step:1268/1770 train_time:125346ms step_avg:98.85ms
step:1269/1770 train_time:125449ms step_avg:98.86ms
step:1270/1770 train_time:125553ms step_avg:98.86ms
step:1271/1770 train_time:125657ms step_avg:98.86ms
step:1272/1770 train_time:125760ms step_avg:98.87ms
step:1273/1770 train_time:125866ms step_avg:98.87ms
step:1274/1770 train_time:125969ms step_avg:98.88ms
step:1275/1770 train_time:126070ms step_avg:98.88ms
step:1276/1770 train_time:126173ms step_avg:98.88ms
step:1277/1770 train_time:126277ms step_avg:98.89ms
step:1278/1770 train_time:126381ms step_avg:98.89ms
step:1279/1770 train_time:126484ms step_avg:98.89ms
step:1280/1770 train_time:126588ms step_avg:98.90ms
step:1281/1770 train_time:126690ms step_avg:98.90ms
step:1282/1770 train_time:126795ms step_avg:98.90ms
step:1283/1770 train_time:126900ms step_avg:98.91ms
step:1284/1770 train_time:127004ms step_avg:98.91ms
step:1285/1770 train_time:127107ms step_avg:98.92ms
step:1286/1770 train_time:127211ms step_avg:98.92ms
step:1287/1770 train_time:127316ms step_avg:98.92ms
step:1288/1770 train_time:127421ms step_avg:98.93ms
step:1289/1770 train_time:127524ms step_avg:98.93ms
step:1290/1770 train_time:127627ms step_avg:98.94ms
step:1291/1770 train_time:127729ms step_avg:98.94ms
step:1292/1770 train_time:127832ms step_avg:98.94ms
step:1293/1770 train_time:127936ms step_avg:98.94ms
step:1294/1770 train_time:128040ms step_avg:98.95ms
step:1295/1770 train_time:128144ms step_avg:98.95ms
step:1296/1770 train_time:128248ms step_avg:98.96ms
step:1297/1770 train_time:128350ms step_avg:98.96ms
step:1298/1770 train_time:128454ms step_avg:98.96ms
step:1299/1770 train_time:128557ms step_avg:98.97ms
step:1300/1770 train_time:128661ms step_avg:98.97ms
step:1301/1770 train_time:128766ms step_avg:98.97ms
step:1302/1770 train_time:128869ms step_avg:98.98ms
step:1303/1770 train_time:128972ms step_avg:98.98ms
step:1304/1770 train_time:129076ms step_avg:98.98ms
step:1305/1770 train_time:129179ms step_avg:98.99ms
step:1306/1770 train_time:129283ms step_avg:98.99ms
step:1307/1770 train_time:129387ms step_avg:99.00ms
step:1308/1770 train_time:129489ms step_avg:99.00ms
step:1309/1770 train_time:129592ms step_avg:99.00ms
step:1310/1770 train_time:129696ms step_avg:99.00ms
step:1311/1770 train_time:129799ms step_avg:99.01ms
step:1312/1770 train_time:129903ms step_avg:99.01ms
step:1313/1770 train_time:130005ms step_avg:99.01ms
step:1314/1770 train_time:130108ms step_avg:99.02ms
step:1315/1770 train_time:130212ms step_avg:99.02ms
step:1316/1770 train_time:130316ms step_avg:99.02ms
step:1317/1770 train_time:130420ms step_avg:99.03ms
step:1318/1770 train_time:130526ms step_avg:99.03ms
step:1319/1770 train_time:130629ms step_avg:99.04ms
step:1320/1770 train_time:130731ms step_avg:99.04ms
step:1321/1770 train_time:130835ms step_avg:99.04ms
step:1322/1770 train_time:130939ms step_avg:99.05ms
step:1323/1770 train_time:131044ms step_avg:99.05ms
step:1324/1770 train_time:131148ms step_avg:99.05ms
step:1325/1770 train_time:131252ms step_avg:99.06ms
step:1326/1770 train_time:131355ms step_avg:99.06ms
step:1327/1770 train_time:131461ms step_avg:99.07ms
step:1328/1770 train_time:131565ms step_avg:99.07ms
step:1329/1770 train_time:131667ms step_avg:99.07ms
step:1330/1770 train_time:131769ms step_avg:99.07ms
step:1331/1770 train_time:131872ms step_avg:99.08ms
step:1332/1770 train_time:131975ms step_avg:99.08ms
step:1333/1770 train_time:132079ms step_avg:99.08ms
step:1334/1770 train_time:132183ms step_avg:99.09ms
step:1335/1770 train_time:132286ms step_avg:99.09ms
step:1336/1770 train_time:132389ms step_avg:99.09ms
step:1337/1770 train_time:132492ms step_avg:99.10ms
step:1338/1770 train_time:132596ms step_avg:99.10ms
step:1339/1770 train_time:132699ms step_avg:99.10ms
step:1340/1770 train_time:132804ms step_avg:99.11ms
step:1341/1770 train_time:132907ms step_avg:99.11ms
step:1342/1770 train_time:133010ms step_avg:99.11ms
step:1343/1770 train_time:133114ms step_avg:99.12ms
step:1344/1770 train_time:133217ms step_avg:99.12ms
step:1345/1770 train_time:133321ms step_avg:99.12ms
step:1346/1770 train_time:133424ms step_avg:99.13ms
step:1347/1770 train_time:133528ms step_avg:99.13ms
step:1348/1770 train_time:133633ms step_avg:99.13ms
step:1349/1770 train_time:133737ms step_avg:99.14ms
step:1350/1770 train_time:133840ms step_avg:99.14ms
step:1351/1770 train_time:133945ms step_avg:99.15ms
step:1352/1770 train_time:134049ms step_avg:99.15ms
step:1353/1770 train_time:134153ms step_avg:99.15ms
step:1354/1770 train_time:134255ms step_avg:99.15ms
step:1355/1770 train_time:134360ms step_avg:99.16ms
step:1356/1770 train_time:134464ms step_avg:99.16ms
step:1357/1770 train_time:134568ms step_avg:99.17ms
step:1358/1770 train_time:134671ms step_avg:99.17ms
step:1359/1770 train_time:134774ms step_avg:99.17ms
step:1360/1770 train_time:134879ms step_avg:99.18ms
step:1361/1770 train_time:134983ms step_avg:99.18ms
step:1362/1770 train_time:135087ms step_avg:99.18ms
step:1363/1770 train_time:135190ms step_avg:99.19ms
step:1364/1770 train_time:135294ms step_avg:99.19ms
step:1365/1770 train_time:135398ms step_avg:99.19ms
step:1366/1770 train_time:135502ms step_avg:99.20ms
step:1367/1770 train_time:135607ms step_avg:99.20ms
step:1368/1770 train_time:135709ms step_avg:99.20ms
step:1369/1770 train_time:135813ms step_avg:99.21ms
step:1370/1770 train_time:135916ms step_avg:99.21ms
step:1371/1770 train_time:136020ms step_avg:99.21ms
step:1372/1770 train_time:136124ms step_avg:99.22ms
step:1373/1770 train_time:136228ms step_avg:99.22ms
step:1374/1770 train_time:136332ms step_avg:99.22ms
step:1375/1770 train_time:136436ms step_avg:99.23ms
step:1375/1770 val_loss:3.3821 train_time:136540ms step_avg:99.30ms
step:1376/1770 train_time:136557ms step_avg:99.24ms
step:1377/1770 train_time:136655ms step_avg:99.24ms
step:1378/1770 train_time:136757ms step_avg:99.24ms
step:1379/1770 train_time:136860ms step_avg:99.25ms
step:1380/1770 train_time:136963ms step_avg:99.25ms
step:1381/1770 train_time:137067ms step_avg:99.25ms
step:1382/1770 train_time:137170ms step_avg:99.25ms
step:1383/1770 train_time:137273ms step_avg:99.26ms
step:1384/1770 train_time:137375ms step_avg:99.26ms
step:1385/1770 train_time:137479ms step_avg:99.26ms
step:1386/1770 train_time:137588ms step_avg:99.27ms
step:1387/1770 train_time:137692ms step_avg:99.27ms
step:1388/1770 train_time:137795ms step_avg:99.28ms
step:1389/1770 train_time:137898ms step_avg:99.28ms
step:1390/1770 train_time:138003ms step_avg:99.28ms
step:1391/1770 train_time:138106ms step_avg:99.29ms
step:1392/1770 train_time:138209ms step_avg:99.29ms
step:1393/1770 train_time:138311ms step_avg:99.29ms
step:1394/1770 train_time:138414ms step_avg:99.29ms
step:1395/1770 train_time:138518ms step_avg:99.30ms
step:1396/1770 train_time:138623ms step_avg:99.30ms
step:1397/1770 train_time:138728ms step_avg:99.30ms
step:1398/1770 train_time:138831ms step_avg:99.31ms
step:1399/1770 train_time:138933ms step_avg:99.31ms
step:1400/1770 train_time:139037ms step_avg:99.31ms
step:1401/1770 train_time:139141ms step_avg:99.32ms
step:1402/1770 train_time:139244ms step_avg:99.32ms
step:1403/1770 train_time:139348ms step_avg:99.32ms
step:1404/1770 train_time:139451ms step_avg:99.32ms
step:1405/1770 train_time:139554ms step_avg:99.33ms
step:1406/1770 train_time:139658ms step_avg:99.33ms
step:1407/1770 train_time:139762ms step_avg:99.33ms
step:1408/1770 train_time:139866ms step_avg:99.34ms
step:1409/1770 train_time:139969ms step_avg:99.34ms
step:1410/1770 train_time:140073ms step_avg:99.34ms
step:1411/1770 train_time:140175ms step_avg:99.34ms
step:1412/1770 train_time:140279ms step_avg:99.35ms
step:1413/1770 train_time:140383ms step_avg:99.35ms
step:1414/1770 train_time:140487ms step_avg:99.35ms
step:1415/1770 train_time:140592ms step_avg:99.36ms
step:1416/1770 train_time:140695ms step_avg:99.36ms
step:1417/1770 train_time:140799ms step_avg:99.36ms
step:1418/1770 train_time:140904ms step_avg:99.37ms
step:1419/1770 train_time:141008ms step_avg:99.37ms
step:1420/1770 train_time:141111ms step_avg:99.37ms
step:1421/1770 train_time:141213ms step_avg:99.38ms
step:1422/1770 train_time:141316ms step_avg:99.38ms
step:1423/1770 train_time:141420ms step_avg:99.38ms
step:1424/1770 train_time:141523ms step_avg:99.38ms
step:1425/1770 train_time:141627ms step_avg:99.39ms
step:1426/1770 train_time:141731ms step_avg:99.39ms
step:1427/1770 train_time:141835ms step_avg:99.39ms
step:1428/1770 train_time:141939ms step_avg:99.40ms
step:1429/1770 train_time:142042ms step_avg:99.40ms
step:1430/1770 train_time:142147ms step_avg:99.40ms
step:1431/1770 train_time:142251ms step_avg:99.41ms
step:1432/1770 train_time:142354ms step_avg:99.41ms
step:1433/1770 train_time:142456ms step_avg:99.41ms
step:1434/1770 train_time:142560ms step_avg:99.41ms
step:1435/1770 train_time:142666ms step_avg:99.42ms
step:1436/1770 train_time:142770ms step_avg:99.42ms
step:1437/1770 train_time:142873ms step_avg:99.42ms
step:1438/1770 train_time:142976ms step_avg:99.43ms
step:1439/1770 train_time:143080ms step_avg:99.43ms
step:1440/1770 train_time:143184ms step_avg:99.43ms
step:1441/1770 train_time:143289ms step_avg:99.44ms
step:1442/1770 train_time:143392ms step_avg:99.44ms
step:1443/1770 train_time:143494ms step_avg:99.44ms
step:1444/1770 train_time:143599ms step_avg:99.45ms
step:1445/1770 train_time:143704ms step_avg:99.45ms
step:1446/1770 train_time:143808ms step_avg:99.45ms
step:1447/1770 train_time:143912ms step_avg:99.46ms
step:1448/1770 train_time:144016ms step_avg:99.46ms
step:1449/1770 train_time:144122ms step_avg:99.46ms
step:1450/1770 train_time:144226ms step_avg:99.47ms
step:1451/1770 train_time:144332ms step_avg:99.47ms
step:1452/1770 train_time:144437ms step_avg:99.47ms
step:1453/1770 train_time:144541ms step_avg:99.48ms
step:1454/1770 train_time:144646ms step_avg:99.48ms
step:1455/1770 train_time:144752ms step_avg:99.49ms
step:1456/1770 train_time:144858ms step_avg:99.49ms
step:1457/1770 train_time:144964ms step_avg:99.49ms
step:1458/1770 train_time:145069ms step_avg:99.50ms
step:1459/1770 train_time:145174ms step_avg:99.50ms
step:1460/1770 train_time:145278ms step_avg:99.51ms
step:1461/1770 train_time:145384ms step_avg:99.51ms
step:1462/1770 train_time:145488ms step_avg:99.51ms
step:1463/1770 train_time:145592ms step_avg:99.52ms
step:1464/1770 train_time:145697ms step_avg:99.52ms
step:1465/1770 train_time:145802ms step_avg:99.52ms
step:1466/1770 train_time:145908ms step_avg:99.53ms
step:1467/1770 train_time:146014ms step_avg:99.53ms
step:1468/1770 train_time:146119ms step_avg:99.54ms
step:1469/1770 train_time:146224ms step_avg:99.54ms
step:1470/1770 train_time:146330ms step_avg:99.54ms
step:1471/1770 train_time:146434ms step_avg:99.55ms
step:1472/1770 train_time:146539ms step_avg:99.55ms
step:1473/1770 train_time:146644ms step_avg:99.55ms
step:1474/1770 train_time:146749ms step_avg:99.56ms
step:1475/1770 train_time:146853ms step_avg:99.56ms
step:1476/1770 train_time:146957ms step_avg:99.56ms
step:1477/1770 train_time:147064ms step_avg:99.57ms
step:1478/1770 train_time:147169ms step_avg:99.57ms
step:1479/1770 train_time:147273ms step_avg:99.58ms
step:1480/1770 train_time:147378ms step_avg:99.58ms
step:1481/1770 train_time:147486ms step_avg:99.59ms
step:1482/1770 train_time:147589ms step_avg:99.59ms
step:1483/1770 train_time:147693ms step_avg:99.59ms
step:1484/1770 train_time:147797ms step_avg:99.59ms
step:1485/1770 train_time:147901ms step_avg:99.60ms
step:1486/1770 train_time:148006ms step_avg:99.60ms
step:1487/1770 train_time:148111ms step_avg:99.60ms
step:1488/1770 train_time:148215ms step_avg:99.61ms
step:1489/1770 train_time:148321ms step_avg:99.61ms
step:1490/1770 train_time:148427ms step_avg:99.62ms
step:1491/1770 train_time:148532ms step_avg:99.62ms
step:1492/1770 train_time:148637ms step_avg:99.62ms
step:1493/1770 train_time:148744ms step_avg:99.63ms
step:1494/1770 train_time:148853ms step_avg:99.63ms
step:1495/1770 train_time:148956ms step_avg:99.64ms
step:1496/1770 train_time:149060ms step_avg:99.64ms
step:1497/1770 train_time:149165ms step_avg:99.64ms
step:1498/1770 train_time:149269ms step_avg:99.65ms
step:1499/1770 train_time:149373ms step_avg:99.65ms
step:1500/1770 train_time:149477ms step_avg:99.65ms
step:1500/1770 val_loss:3.3445 train_time:149581ms step_avg:99.72ms
step:1501/1770 train_time:149599ms step_avg:99.67ms
step:1502/1770 train_time:149697ms step_avg:99.67ms
step:1503/1770 train_time:149800ms step_avg:99.67ms
step:1504/1770 train_time:149904ms step_avg:99.67ms
step:1505/1770 train_time:150011ms step_avg:99.67ms
step:1506/1770 train_time:150116ms step_avg:99.68ms
step:1507/1770 train_time:150221ms step_avg:99.68ms
step:1508/1770 train_time:150326ms step_avg:99.69ms
step:1509/1770 train_time:150429ms step_avg:99.69ms
step:1510/1770 train_time:150534ms step_avg:99.69ms
step:1511/1770 train_time:150640ms step_avg:99.70ms
step:1512/1770 train_time:150746ms step_avg:99.70ms
step:1513/1770 train_time:150852ms step_avg:99.70ms
step:1514/1770 train_time:150957ms step_avg:99.71ms
step:1515/1770 train_time:151062ms step_avg:99.71ms
step:1516/1770 train_time:151167ms step_avg:99.71ms
step:1517/1770 train_time:151271ms step_avg:99.72ms
step:1518/1770 train_time:151376ms step_avg:99.72ms
step:1519/1770 train_time:151480ms step_avg:99.72ms
step:1520/1770 train_time:151586ms step_avg:99.73ms
step:1521/1770 train_time:151691ms step_avg:99.73ms
step:1522/1770 train_time:151797ms step_avg:99.73ms
step:1523/1770 train_time:151903ms step_avg:99.74ms
step:1524/1770 train_time:152007ms step_avg:99.74ms
step:1525/1770 train_time:152111ms step_avg:99.74ms
step:1526/1770 train_time:152216ms step_avg:99.75ms
step:1527/1770 train_time:152320ms step_avg:99.75ms
step:1528/1770 train_time:152427ms step_avg:99.76ms
step:1529/1770 train_time:152531ms step_avg:99.76ms
step:1530/1770 train_time:152636ms step_avg:99.76ms
step:1531/1770 train_time:152740ms step_avg:99.77ms
step:1532/1770 train_time:152846ms step_avg:99.77ms
step:1533/1770 train_time:152951ms step_avg:99.77ms
step:1534/1770 train_time:153056ms step_avg:99.78ms
step:1535/1770 train_time:153159ms step_avg:99.78ms
step:1536/1770 train_time:153264ms step_avg:99.78ms
step:1537/1770 train_time:153368ms step_avg:99.78ms
step:1538/1770 train_time:153474ms step_avg:99.79ms
step:1539/1770 train_time:153578ms step_avg:99.79ms
step:1540/1770 train_time:153686ms step_avg:99.80ms
step:1541/1770 train_time:153792ms step_avg:99.80ms
step:1542/1770 train_time:153897ms step_avg:99.80ms
step:1543/1770 train_time:154000ms step_avg:99.81ms
step:1544/1770 train_time:154106ms step_avg:99.81ms
step:1545/1770 train_time:154211ms step_avg:99.81ms
step:1546/1770 train_time:154316ms step_avg:99.82ms
step:1547/1770 train_time:154421ms step_avg:99.82ms
step:1548/1770 train_time:154525ms step_avg:99.82ms
step:1549/1770 train_time:154630ms step_avg:99.83ms
step:1550/1770 train_time:154735ms step_avg:99.83ms
step:1551/1770 train_time:154840ms step_avg:99.83ms
step:1552/1770 train_time:154945ms step_avg:99.84ms
step:1553/1770 train_time:155049ms step_avg:99.84ms
step:1554/1770 train_time:155153ms step_avg:99.84ms
step:1555/1770 train_time:155259ms step_avg:99.85ms
step:1556/1770 train_time:155363ms step_avg:99.85ms
step:1557/1770 train_time:155468ms step_avg:99.85ms
step:1558/1770 train_time:155572ms step_avg:99.85ms
step:1559/1770 train_time:155677ms step_avg:99.86ms
step:1560/1770 train_time:155781ms step_avg:99.86ms
step:1561/1770 train_time:155888ms step_avg:99.86ms
step:1562/1770 train_time:155993ms step_avg:99.87ms
step:1563/1770 train_time:156099ms step_avg:99.87ms
step:1564/1770 train_time:156203ms step_avg:99.87ms
step:1565/1770 train_time:156307ms step_avg:99.88ms
step:1566/1770 train_time:156412ms step_avg:99.88ms
step:1567/1770 train_time:156518ms step_avg:99.88ms
step:1568/1770 train_time:156622ms step_avg:99.89ms
step:1569/1770 train_time:156730ms step_avg:99.89ms
step:1570/1770 train_time:156834ms step_avg:99.89ms
step:1571/1770 train_time:156939ms step_avg:99.90ms
step:1572/1770 train_time:157044ms step_avg:99.90ms
step:1573/1770 train_time:157151ms step_avg:99.91ms
step:1574/1770 train_time:157256ms step_avg:99.91ms
step:1575/1770 train_time:157361ms step_avg:99.91ms
step:1576/1770 train_time:157465ms step_avg:99.91ms
step:1577/1770 train_time:157572ms step_avg:99.92ms
step:1578/1770 train_time:157678ms step_avg:99.92ms
step:1579/1770 train_time:157781ms step_avg:99.92ms
step:1580/1770 train_time:157885ms step_avg:99.93ms
step:1581/1770 train_time:157992ms step_avg:99.93ms
step:1582/1770 train_time:158098ms step_avg:99.94ms
step:1583/1770 train_time:158203ms step_avg:99.94ms
step:1584/1770 train_time:158309ms step_avg:99.94ms
step:1585/1770 train_time:158415ms step_avg:99.95ms
step:1586/1770 train_time:158523ms step_avg:99.95ms
step:1587/1770 train_time:158628ms step_avg:99.95ms
step:1588/1770 train_time:158732ms step_avg:99.96ms
step:1589/1770 train_time:158839ms step_avg:99.96ms
step:1590/1770 train_time:158944ms step_avg:99.96ms
step:1591/1770 train_time:159047ms step_avg:99.97ms
step:1592/1770 train_time:159153ms step_avg:99.97ms
step:1593/1770 train_time:159258ms step_avg:99.97ms
step:1594/1770 train_time:159364ms step_avg:99.98ms
step:1595/1770 train_time:159468ms step_avg:99.98ms
step:1596/1770 train_time:159575ms step_avg:99.98ms
step:1597/1770 train_time:159679ms step_avg:99.99ms
step:1598/1770 train_time:159784ms step_avg:99.99ms
step:1599/1770 train_time:159889ms step_avg:99.99ms
step:1600/1770 train_time:159997ms step_avg:100.00ms
step:1601/1770 train_time:160101ms step_avg:100.00ms
step:1602/1770 train_time:160207ms step_avg:100.00ms
step:1603/1770 train_time:160313ms step_avg:100.01ms
step:1604/1770 train_time:160417ms step_avg:100.01ms
step:1605/1770 train_time:160521ms step_avg:100.01ms
step:1606/1770 train_time:160625ms step_avg:100.02ms
step:1607/1770 train_time:160733ms step_avg:100.02ms
step:1608/1770 train_time:160838ms step_avg:100.02ms
step:1609/1770 train_time:160942ms step_avg:100.03ms
step:1610/1770 train_time:161048ms step_avg:100.03ms
step:1611/1770 train_time:161154ms step_avg:100.03ms
step:1612/1770 train_time:161260ms step_avg:100.04ms
step:1613/1770 train_time:161364ms step_avg:100.04ms
step:1614/1770 train_time:161468ms step_avg:100.04ms
step:1615/1770 train_time:161572ms step_avg:100.04ms
step:1616/1770 train_time:161677ms step_avg:100.05ms
step:1617/1770 train_time:161783ms step_avg:100.05ms
step:1618/1770 train_time:161890ms step_avg:100.06ms
step:1619/1770 train_time:161996ms step_avg:100.06ms
step:1620/1770 train_time:162102ms step_avg:100.06ms
step:1621/1770 train_time:162207ms step_avg:100.07ms
step:1622/1770 train_time:162313ms step_avg:100.07ms
step:1623/1770 train_time:162420ms step_avg:100.07ms
step:1624/1770 train_time:162524ms step_avg:100.08ms
step:1625/1770 train_time:162628ms step_avg:100.08ms
step:1625/1770 val_loss:3.3098 train_time:162733ms step_avg:100.14ms
step:1626/1770 train_time:162750ms step_avg:100.09ms
step:1627/1770 train_time:162845ms step_avg:100.09ms
step:1628/1770 train_time:162950ms step_avg:100.09ms
step:1629/1770 train_time:163054ms step_avg:100.09ms
step:1630/1770 train_time:163158ms step_avg:100.10ms
step:1631/1770 train_time:163262ms step_avg:100.10ms
step:1632/1770 train_time:163366ms step_avg:100.10ms
step:1633/1770 train_time:163471ms step_avg:100.10ms
step:1634/1770 train_time:163575ms step_avg:100.11ms
step:1635/1770 train_time:163682ms step_avg:100.11ms
step:1636/1770 train_time:163788ms step_avg:100.12ms
step:1637/1770 train_time:163894ms step_avg:100.12ms
step:1638/1770 train_time:163999ms step_avg:100.12ms
step:1639/1770 train_time:164103ms step_avg:100.12ms
step:1640/1770 train_time:164208ms step_avg:100.13ms
step:1641/1770 train_time:164312ms step_avg:100.13ms
step:1642/1770 train_time:164416ms step_avg:100.13ms
step:1643/1770 train_time:164520ms step_avg:100.13ms
step:1644/1770 train_time:164625ms step_avg:100.14ms
step:1645/1770 train_time:164729ms step_avg:100.14ms
step:1646/1770 train_time:164837ms step_avg:100.14ms
step:1647/1770 train_time:164943ms step_avg:100.15ms
step:1648/1770 train_time:165047ms step_avg:100.15ms
step:1649/1770 train_time:165152ms step_avg:100.15ms
step:1650/1770 train_time:165258ms step_avg:100.16ms
step:1651/1770 train_time:165362ms step_avg:100.16ms
step:1652/1770 train_time:165466ms step_avg:100.16ms
step:1653/1770 train_time:165571ms step_avg:100.16ms
step:1654/1770 train_time:165679ms step_avg:100.17ms
step:1655/1770 train_time:165786ms step_avg:100.17ms
step:1656/1770 train_time:165891ms step_avg:100.18ms
step:1657/1770 train_time:165998ms step_avg:100.18ms
step:1658/1770 train_time:166101ms step_avg:100.18ms
step:1659/1770 train_time:166208ms step_avg:100.19ms
step:1660/1770 train_time:166313ms step_avg:100.19ms
step:1661/1770 train_time:166419ms step_avg:100.19ms
step:1662/1770 train_time:166524ms step_avg:100.19ms
step:1663/1770 train_time:166628ms step_avg:100.20ms
step:1664/1770 train_time:166734ms step_avg:100.20ms
step:1665/1770 train_time:166838ms step_avg:100.20ms
step:1666/1770 train_time:166943ms step_avg:100.21ms
step:1667/1770 train_time:167048ms step_avg:100.21ms
step:1668/1770 train_time:167153ms step_avg:100.21ms
step:1669/1770 train_time:167257ms step_avg:100.21ms
step:1670/1770 train_time:167362ms step_avg:100.22ms
step:1671/1770 train_time:167467ms step_avg:100.22ms
step:1672/1770 train_time:167573ms step_avg:100.22ms
step:1673/1770 train_time:167679ms step_avg:100.23ms
step:1674/1770 train_time:167784ms step_avg:100.23ms
step:1675/1770 train_time:167888ms step_avg:100.23ms
step:1676/1770 train_time:167995ms step_avg:100.24ms
step:1677/1770 train_time:168103ms step_avg:100.24ms
step:1678/1770 train_time:168206ms step_avg:100.24ms
step:1679/1770 train_time:168312ms step_avg:100.25ms
step:1680/1770 train_time:168417ms step_avg:100.25ms
step:1681/1770 train_time:168523ms step_avg:100.25ms
step:1682/1770 train_time:168629ms step_avg:100.26ms
step:1683/1770 train_time:168735ms step_avg:100.26ms
step:1684/1770 train_time:168839ms step_avg:100.26ms
step:1685/1770 train_time:168943ms step_avg:100.26ms
step:1686/1770 train_time:169050ms step_avg:100.27ms
step:1687/1770 train_time:169157ms step_avg:100.27ms
step:1688/1770 train_time:169262ms step_avg:100.27ms
step:1689/1770 train_time:169367ms step_avg:100.28ms
step:1690/1770 train_time:169471ms step_avg:100.28ms
step:1691/1770 train_time:169578ms step_avg:100.28ms
step:1692/1770 train_time:169682ms step_avg:100.29ms
step:1693/1770 train_time:169788ms step_avg:100.29ms
step:1694/1770 train_time:169893ms step_avg:100.29ms
step:1695/1770 train_time:169999ms step_avg:100.29ms
step:1696/1770 train_time:170106ms step_avg:100.30ms
step:1697/1770 train_time:170211ms step_avg:100.30ms
step:1698/1770 train_time:170318ms step_avg:100.31ms
step:1699/1770 train_time:170422ms step_avg:100.31ms
step:1700/1770 train_time:170525ms step_avg:100.31ms
step:1701/1770 train_time:170629ms step_avg:100.31ms
step:1702/1770 train_time:170734ms step_avg:100.31ms
step:1703/1770 train_time:170840ms step_avg:100.32ms
step:1704/1770 train_time:170944ms step_avg:100.32ms
step:1705/1770 train_time:171049ms step_avg:100.32ms
step:1706/1770 train_time:171153ms step_avg:100.32ms
step:1707/1770 train_time:171259ms step_avg:100.33ms
step:1708/1770 train_time:171365ms step_avg:100.33ms
step:1709/1770 train_time:171471ms step_avg:100.33ms
step:1710/1770 train_time:171579ms step_avg:100.34ms
step:1711/1770 train_time:171687ms step_avg:100.34ms
step:1712/1770 train_time:171791ms step_avg:100.35ms
step:1713/1770 train_time:171896ms step_avg:100.35ms
step:1714/1770 train_time:172003ms step_avg:100.35ms
step:1715/1770 train_time:172107ms step_avg:100.35ms
step:1716/1770 train_time:172213ms step_avg:100.36ms
step:1717/1770 train_time:172318ms step_avg:100.36ms
step:1718/1770 train_time:172425ms step_avg:100.36ms
step:1719/1770 train_time:172530ms step_avg:100.37ms
step:1720/1770 train_time:172638ms step_avg:100.37ms
step:1721/1770 train_time:172742ms step_avg:100.37ms
step:1722/1770 train_time:172850ms step_avg:100.38ms
step:1723/1770 train_time:172957ms step_avg:100.38ms
step:1724/1770 train_time:173065ms step_avg:100.39ms
step:1725/1770 train_time:173173ms step_avg:100.39ms
step:1726/1770 train_time:173279ms step_avg:100.39ms
step:1727/1770 train_time:173385ms step_avg:100.40ms
step:1728/1770 train_time:173491ms step_avg:100.40ms
step:1729/1770 train_time:173597ms step_avg:100.40ms
step:1730/1770 train_time:173704ms step_avg:100.41ms
step:1731/1770 train_time:173811ms step_avg:100.41ms
step:1732/1770 train_time:173916ms step_avg:100.41ms
step:1733/1770 train_time:174022ms step_avg:100.42ms
step:1734/1770 train_time:174127ms step_avg:100.42ms
step:1735/1770 train_time:174234ms step_avg:100.42ms
step:1736/1770 train_time:174339ms step_avg:100.43ms
step:1737/1770 train_time:174444ms step_avg:100.43ms
step:1738/1770 train_time:174550ms step_avg:100.43ms
step:1739/1770 train_time:174656ms step_avg:100.43ms
step:1740/1770 train_time:174761ms step_avg:100.44ms
step:1741/1770 train_time:174869ms step_avg:100.44ms
step:1742/1770 train_time:174977ms step_avg:100.45ms
step:1743/1770 train_time:175082ms step_avg:100.45ms
step:1744/1770 train_time:175187ms step_avg:100.45ms
step:1745/1770 train_time:175294ms step_avg:100.45ms
step:1746/1770 train_time:175402ms step_avg:100.46ms
step:1747/1770 train_time:175506ms step_avg:100.46ms
step:1748/1770 train_time:175614ms step_avg:100.47ms
step:1749/1770 train_time:175722ms step_avg:100.47ms
step:1750/1770 train_time:175827ms step_avg:100.47ms
step:1750/1770 val_loss:3.2834 train_time:175933ms step_avg:100.53ms
step:1751/1770 train_time:175951ms step_avg:100.49ms
step:1752/1770 train_time:176048ms step_avg:100.48ms
step:1753/1770 train_time:176154ms step_avg:100.49ms
step:1754/1770 train_time:176260ms step_avg:100.49ms
step:1755/1770 train_time:176364ms step_avg:100.49ms
step:1756/1770 train_time:176470ms step_avg:100.50ms
step:1757/1770 train_time:176577ms step_avg:100.50ms
step:1758/1770 train_time:176682ms step_avg:100.50ms
step:1759/1770 train_time:176788ms step_avg:100.50ms
step:1760/1770 train_time:176892ms step_avg:100.51ms
step:1761/1770 train_time:177003ms step_avg:100.51ms
step:1762/1770 train_time:177111ms step_avg:100.52ms
step:1763/1770 train_time:177216ms step_avg:100.52ms
step:1764/1770 train_time:177322ms step_avg:100.52ms
step:1765/1770 train_time:177426ms step_avg:100.52ms
step:1766/1770 train_time:177536ms step_avg:100.53ms
step:1767/1770 train_time:177640ms step_avg:100.53ms
step:1768/1770 train_time:177745ms step_avg:100.53ms
step:1769/1770 train_time:177848ms step_avg:100.54ms
step:1770/1770 train_time:177954ms step_avg:100.54ms
step:1770/1770 val_loss:3.2804 train_time:178061ms step_avg:100.60ms
peak memory allocated: 30724 MiB reserved: 46492 MiB
