import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
import copy
import glob
from dataclasses import dataclass
from functools import lru_cache, partial # Added partial for hook registration
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
#torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params: list[Tensor] = [*params]
        param_groups = []
        for size in {p.numel() for p in params}:
            b = torch.empty(world_size, size, dtype=torch.bfloat16, device="cuda")
            group = dict(params=[p for p in params if p.numel() == size],
                         update_buffer=b, update_buffer_views=[b[i] for i in range(world_size)])
            param_groups.append(group)
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            update_buffer: Tensor = group["update_buffer"]
            update_buffer_views: list[Tensor] = group["update_buffer_views"]
            # generate weight updates in distributed fashion
            params: list[Tensor] = group["params"]
            handle = None
            params_world = None
            def update_prev(): # optimized Muon implementation contributed by @YouJiacheng
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffer_views):
                    p_world.add_(g_world.view_as(p_world),
                                 alpha=-group["lr"] * max(1, p_world.size(-2) / p_world.size(-1))**0.5)
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if "momentum_buffer" not in state:
                        state["momentum_buffer"] = torch.zeros_like(g)
                    buf: Tensor = state["momentum_buffer"]
                    buf.lerp_(g, 1 - group["momentum"])
                    g = g.lerp_(buf, group["momentum"]) if group["nesterov"] else buf
                    g = zeropower_via_newtonschulz5(g, steps=group["ns_steps"]).flatten()
                else:
                    g = update_buffer_views[self.rank]
                if base_i > 0:
                    update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=0.12).transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, block_mask: BlockMask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, max_seq_len, i) for i in range(num_layers)])
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128),
                                    use_fp8=True, x_s=(model_dim**0.5)/448, w_s=24/448, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        self.skip_weights = nn.Parameter(torch.ones(num_layers//2))

    def create_blockmasks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        docs = (input_seq == 50256).cumsum(0)

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_blockmask: Tensor):
            num_blocks = dense_blockmask.sum(dim=-1, dtype=torch.int32)
            indices = dense_blockmask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
        causal_blockmask_any = block_idx[:, None] >= block_idx
        causal_blockmask_all = block_idx[:, None] > block_idx
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
        document_blockmask_any = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
        document_blockmask_all = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
        blockmask_any = causal_blockmask_any & document_blockmask_any
        blockmask_all = causal_blockmask_all & document_blockmask_all
        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(blockmask_any & ~blockmask_all)
        full_kv_num_blocks, full_kv_indices = dense_to_ordered(blockmask_all)
        def build_bm(window_size_blocks: Tensor) -> BlockMask:
            return BlockMask.from_kv_blocks(
                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(window_size_blocks - full_kv_num_blocks, 1)),
                partial_kv_indices,
                torch.clamp_max(full_kv_num_blocks, window_size_blocks - 1),
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        long_bm, short_bm = self.create_blockmasks(input_seq, sliding_window_num_blocks)
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, long_bm, short_bm, short_bm, short_bm, long_bm]
        assert len(block_masks) == len(self.blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977

        # U-net design by @brendanh0gan
        skip_connections = []
        n = len(self.skip_weights)
        for i in range(len(self.blocks)):
            if i >= n:
                x = x + self.skip_weights[i - n] * skip_connections.pop()
            x = self.blocks[i](x, ve[i], x0, block_masks[i])
            if i < n:
                skip_connections.append(x)

        x = norm(x)
        logits = self.lm_head(x).float()
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / (7.5 * x.size(-1)**0.5))
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq, reduction='sum' if self.training else 'mean')
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn't helpful.
        pos += batch_size
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_seq_len = 48*1024 # FlexAttention sequence length
    val_seq_len = 4*64*1024 # FlexAttention sequence length for validation
    # optimization
    num_iterations = 1770 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    # architecture
    vocab_size = 50257
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert world_size == 8 # this code is designed for 8xH100
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs("logs_bucket", exist_ok=True)
    logfile = f"logs_bucket/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

########################################
#    Construct model and optimizer     #
########################################

model: nn.Module = GPT(vocab_size=args.vocab_size, num_layers=12, num_heads=6, model_dim=768,
                       max_seq_len=max(args.train_seq_len, args.val_seq_len)).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
adam_params = [dict(params=head_params, lr=0.22), dict(params=embed_params, lr=0.6), dict(params=scalar_params, lr=0.04)]
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = torch.optim.Adam(adam_params, betas=(0.8, 0.95), eps=1e-10, fused=True)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then decay
def get_lr(step: int):
    x = step / args.num_iterations # progress in training
    assert 0 <= x < 1
    if x < 1 - args.cooldown_frac:
        return 1.0
    else:
        w = (1 - x) / args.cooldown_frac
        return w * 1.0 + (1 - w) * 0.1

# attention window size schedule: linearly increase
@lru_cache(1)
def get_window_size_blocks_helper(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)
def get_window_size_blocks(step: int):
    x = step / args.num_iterations # progress in training
    assert 0 <= x <= 1
    # Linearly increase the block-wise sliding window size over training 128 -> 1792
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * x, n=128)
    return get_window_size_blocks_helper(window_size)

model: nn.Module = torch.compile(model, dynamic=False)

########################################
#     Warmup kernel and Layerwise Bucketing   #
########################################

# Track the gradient readiness order during warmup
gradient_ready_order = []

def record_grad_order_hook(param):
    def hook(*_):
        if param.grad is not None and all(param is not p for p in gradient_ready_order):
            gradient_ready_order.append(param)
    return hook

# Register temp hooks to track when each parameter's gradient is ready
tmp_handles = []
for p in model.parameters():
    if p.requires_grad:
        handle = p.register_post_accumulate_grad_hook(record_grad_order_hook(p))
        tmp_handles.append(handle)

warmup_steps = 10
initial_state = dict(
    model=copy.deepcopy(model.state_dict()),
    optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]
)

for step in range(warmup_steps):
    # Create a local generator with fixed, step-dependent seed
    g = torch.Generator(device="cuda")
    g.manual_seed(1337 + step)

    # Use this generator to sample inputs
    inputs = targets = torch.randint(
        low=0,
        high=args.vocab_size,
        size=(args.train_seq_len,),
        device="cuda",
        dtype=torch.long,
        generator=g,  # <- Local generator ensures deterministic and isolated RNG
    )
    model(inputs.to(torch.int32), targets, get_window_size_blocks(0)).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)

# Remove hooks after warmup
for h in tmp_handles:
    h.remove()

# Restore model/optimizer state to ensure a clean start
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del initial_state

print0(f"Collected {len(gradient_ready_order)} parameters in gradient-ready order")

########################################
#      Overlap Communication Setup     #
########################################

# Layer-aware bucketing using gradient-ready order
def create_buckets_layerwise(ordered_params, bucket_size_mb=25):
    """Group parameters into buckets of ~bucket_size_mb MB, in gradient ready order"""
    buckets = []
    current_bucket = []
    current_size = 0

    for param in ordered_params:
        if not param.requires_grad:
            continue
        param_size_mb = param.numel() * param.element_size() / (1024 * 1024)

        if current_size + param_size_mb > bucket_size_mb and current_bucket:
            buckets.append(current_bucket)
            current_bucket = [param]
            current_size = param_size_mb
        else:
            current_bucket.append(param)
            current_size += param_size_mb

    if current_bucket:
        buckets.append(current_bucket)

    return buckets

# Create buckets for all parameters
all_params = [p for p in model.parameters() if p.requires_grad]
param_buckets = create_buckets_layerwise(gradient_ready_order)
print0(f"Created {len(param_buckets)} gradient buckets")
for i, bucket in enumerate(param_buckets):
    total_size = sum(p.numel() * p.element_size() for p in bucket) / (1024 * 1024)
    print0(f"Bucket {i}: {len(bucket)} params, {total_size:.1f} MB")

# Bucket state tracking
bucket_ready_count = [0] * len(param_buckets)
bucket_handles = [None] * len(param_buckets)
param_to_bucket = {}

# Map each parameter to its bucket index
for bucket_idx, bucket in enumerate(param_buckets):
    for param in bucket:
        param_to_bucket[param] = bucket_idx

def _gradient_hook(param: Tensor):
    """Called when a parameter's gradient is ready"""
    if param.grad is None:
        return

    bucket_idx = param_to_bucket[param]
    bucket_ready_count[bucket_idx] += 1

    # Check if all parameters in this bucket are ready
    if bucket_ready_count[bucket_idx] == len(param_buckets[bucket_idx]):
        # All-reduce this bucket
        bucket_grads = [p.grad for p in param_buckets[bucket_idx]]

        # For multi-tensor operations, we can reduce them together
        if len(bucket_grads) == 1:
            handle = dist.all_reduce(bucket_grads[0], op=dist.ReduceOp.AVG, async_op=True)
        else:
            # Use multi-tensor all-reduce for efficiency
            handle = dist.all_reduce_coalesced(bucket_grads, op=dist.ReduceOp.AVG, async_op=True)

        bucket_handles[bucket_idx] = handle

# Register hooks for all parameters
print0("Registering bucketed gradient hooks...")
for param in all_params:
    param.register_post_accumulate_grad_hook(_gradient_hook)

def wait_for_gradients():
    """Wait for all gradient reductions to complete and reset bucket state"""
    for handle in bucket_handles:
        if handle is not None:
            handle.wait()

    # Reset state for next iteration
    for i in range(len(bucket_ready_count)):
        bucket_ready_count[i] = 0
        bucket_handles[i] = None

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, world_size * args.train_seq_len, rank, world_size)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_batch_size = world_size * args.val_seq_len
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        val_loader = distributed_data_generator(args.val_files, val_batch_size, rank, world_size)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets = next(val_loader)
                val_loss += model(inputs, targets, get_window_size_blocks(step))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs_bucket/{run_id}", exist_ok=True)
            torch.save(log, f"logs_bucket/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    inputs, targets = next(train_loader)
    model(inputs, targets, get_window_size_blocks(step)).backward()
    #for param in model.parameters():
    #    dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
    wait_for_gradients() # does the same thing as commented two lines above, but faster

    # set optimization hyperparameters
    for opt in optimizers:
        for group in opt.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)
    for group in optimizer2.param_groups:
        frac = min(step / 300, 1) # momentum warmup for muon
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers
    for opt in optimizers:
        opt.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Feb  4 2025, 14:57:36) [GCC 11.4.0]
Running PyTorch 2.8.0.dev20250612+cu126 compiled for CUDA 12.6
Fri Jun 13 04:44:35 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.08             Driver Version: 550.127.08     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   41C    P0            130W /  700W |    5824MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   33C    P0            127W /  700W |    1514MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   32C    P0            121W /  700W |    1514MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   38C    P0            124W /  700W |    1514MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   39C    P0            121W /  700W |    1514MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   32C    P0            120W /  700W |    1514MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   38C    P0            125W /  700W |    1514MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   31C    P0            114W /  700W |    1514MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
Collected 75 parameters in gradient-ready order
Created 21 gradient buckets
Bucket 0: 1 params, 73.6 MB
Bucket 1: 1 params, 73.6 MB
Bucket 2: 4 params, 15.8 MB
Bucket 3: 1 params, 147.4 MB
Bucket 4: 4 params, 20.3 MB
Bucket 5: 4 params, 24.8 MB
Bucket 6: 4 params, 18.0 MB
Bucket 7: 5 params, 18.0 MB
Bucket 8: 4 params, 20.3 MB
Bucket 9: 5 params, 24.8 MB
Bucket 10: 4 params, 20.3 MB
Bucket 11: 4 params, 24.8 MB
Bucket 12: 5 params, 18.0 MB
Bucket 13: 5 params, 20.3 MB
Bucket 14: 5 params, 18.0 MB
Bucket 15: 4 params, 20.3 MB
Bucket 16: 5 params, 24.8 MB
Bucket 17: 4 params, 24.8 MB
Bucket 18: 4 params, 2.3 MB
Bucket 19: 1 params, 73.6 MB
Bucket 20: 1 params, 73.6 MB
Registering bucketed gradient hooks...
step:0/1770 val_loss:10.8258 train_time:0ms step_avg:0.02ms
step:1/1770 train_time:79ms step_avg:78.75ms
step:2/1770 train_time:149ms step_avg:74.37ms
step:3/1770 train_time:237ms step_avg:79.02ms
step:4/1770 train_time:330ms step_avg:82.46ms
step:5/1770 train_time:425ms step_avg:84.92ms
step:6/1770 train_time:518ms step_avg:86.38ms
step:7/1770 train_time:613ms step_avg:87.50ms
step:8/1770 train_time:707ms step_avg:88.34ms
step:9/1770 train_time:802ms step_avg:89.07ms
step:10/1770 train_time:896ms step_avg:89.57ms
step:11/1770 train_time:989ms step_avg:89.93ms
step:12/1770 train_time:1085ms step_avg:90.41ms
step:13/1770 train_time:1182ms step_avg:90.89ms
step:14/1770 train_time:1278ms step_avg:91.29ms
step:15/1770 train_time:1372ms step_avg:91.49ms
step:16/1770 train_time:1466ms step_avg:91.62ms
step:17/1770 train_time:1560ms step_avg:91.78ms
step:18/1770 train_time:1654ms step_avg:91.91ms
step:19/1770 train_time:1749ms step_avg:92.03ms
step:20/1770 train_time:1843ms step_avg:92.15ms
step:21/1770 train_time:1937ms step_avg:92.25ms
step:22/1770 train_time:2032ms step_avg:92.35ms
step:23/1770 train_time:2128ms step_avg:92.51ms
step:24/1770 train_time:2224ms step_avg:92.67ms
step:25/1770 train_time:2320ms step_avg:92.78ms
step:26/1770 train_time:2415ms step_avg:92.90ms
step:27/1770 train_time:2510ms step_avg:92.95ms
step:28/1770 train_time:2604ms step_avg:92.99ms
step:29/1770 train_time:2699ms step_avg:93.05ms
step:30/1770 train_time:2793ms step_avg:93.08ms
step:31/1770 train_time:2886ms step_avg:93.11ms
step:32/1770 train_time:2982ms step_avg:93.18ms
step:33/1770 train_time:3077ms step_avg:93.24ms
step:34/1770 train_time:3172ms step_avg:93.30ms
step:35/1770 train_time:3267ms step_avg:93.33ms
step:36/1770 train_time:3363ms step_avg:93.43ms
step:37/1770 train_time:3459ms step_avg:93.48ms
step:38/1770 train_time:3553ms step_avg:93.50ms
step:39/1770 train_time:3648ms step_avg:93.54ms
step:40/1770 train_time:3742ms step_avg:93.55ms
step:41/1770 train_time:3836ms step_avg:93.56ms
step:42/1770 train_time:3931ms step_avg:93.59ms
step:43/1770 train_time:4026ms step_avg:93.62ms
step:44/1770 train_time:4120ms step_avg:93.64ms
step:45/1770 train_time:4216ms step_avg:93.69ms
step:46/1770 train_time:4310ms step_avg:93.71ms
step:47/1770 train_time:4405ms step_avg:93.73ms
step:48/1770 train_time:4500ms step_avg:93.76ms
step:49/1770 train_time:4594ms step_avg:93.76ms
step:50/1770 train_time:4688ms step_avg:93.77ms
step:51/1770 train_time:4783ms step_avg:93.78ms
step:52/1770 train_time:4877ms step_avg:93.78ms
step:53/1770 train_time:4971ms step_avg:93.80ms
step:54/1770 train_time:5065ms step_avg:93.80ms
step:55/1770 train_time:5160ms step_avg:93.82ms
step:56/1770 train_time:5255ms step_avg:93.84ms
step:57/1770 train_time:5350ms step_avg:93.86ms
step:58/1770 train_time:5444ms step_avg:93.86ms
step:59/1770 train_time:5540ms step_avg:93.89ms
step:60/1770 train_time:5635ms step_avg:93.92ms
step:61/1770 train_time:5729ms step_avg:93.92ms
step:62/1770 train_time:5823ms step_avg:93.92ms
step:63/1770 train_time:5918ms step_avg:93.93ms
step:64/1770 train_time:6013ms step_avg:93.95ms
step:65/1770 train_time:6107ms step_avg:93.96ms
step:66/1770 train_time:6202ms step_avg:93.97ms
step:67/1770 train_time:6296ms step_avg:93.97ms
step:68/1770 train_time:6390ms step_avg:93.98ms
step:69/1770 train_time:6485ms step_avg:93.99ms
step:70/1770 train_time:6580ms step_avg:94.00ms
step:71/1770 train_time:6674ms step_avg:94.00ms
step:72/1770 train_time:6768ms step_avg:94.00ms
step:73/1770 train_time:6863ms step_avg:94.01ms
step:74/1770 train_time:6957ms step_avg:94.01ms
step:75/1770 train_time:7051ms step_avg:94.02ms
step:76/1770 train_time:7146ms step_avg:94.02ms
step:77/1770 train_time:7241ms step_avg:94.04ms
step:78/1770 train_time:7337ms step_avg:94.07ms
step:79/1770 train_time:7431ms step_avg:94.07ms
step:80/1770 train_time:7526ms step_avg:94.07ms
step:81/1770 train_time:7621ms step_avg:94.08ms
step:82/1770 train_time:7715ms step_avg:94.09ms
step:83/1770 train_time:7810ms step_avg:94.09ms
step:84/1770 train_time:7904ms step_avg:94.10ms
step:85/1770 train_time:7999ms step_avg:94.10ms
step:86/1770 train_time:8094ms step_avg:94.11ms
step:87/1770 train_time:8188ms step_avg:94.12ms
step:88/1770 train_time:8283ms step_avg:94.13ms
step:89/1770 train_time:8379ms step_avg:94.14ms
step:90/1770 train_time:8473ms step_avg:94.15ms
step:91/1770 train_time:8568ms step_avg:94.15ms
step:92/1770 train_time:8663ms step_avg:94.16ms
step:93/1770 train_time:8757ms step_avg:94.16ms
step:94/1770 train_time:8852ms step_avg:94.17ms
step:95/1770 train_time:8945ms step_avg:94.16ms
step:96/1770 train_time:9040ms step_avg:94.17ms
step:97/1770 train_time:9135ms step_avg:94.17ms
step:98/1770 train_time:9230ms step_avg:94.18ms
step:99/1770 train_time:9325ms step_avg:94.19ms
step:100/1770 train_time:9420ms step_avg:94.20ms
step:101/1770 train_time:9514ms step_avg:94.20ms
step:102/1770 train_time:9608ms step_avg:94.19ms
step:103/1770 train_time:9703ms step_avg:94.20ms
step:104/1770 train_time:9798ms step_avg:94.21ms
step:105/1770 train_time:9893ms step_avg:94.22ms
step:106/1770 train_time:9987ms step_avg:94.22ms
step:107/1770 train_time:10082ms step_avg:94.22ms
step:108/1770 train_time:10176ms step_avg:94.22ms
step:109/1770 train_time:10270ms step_avg:94.22ms
step:110/1770 train_time:10365ms step_avg:94.23ms
step:111/1770 train_time:10460ms step_avg:94.23ms
step:112/1770 train_time:10554ms step_avg:94.23ms
step:113/1770 train_time:10648ms step_avg:94.23ms
step:114/1770 train_time:10742ms step_avg:94.23ms
step:115/1770 train_time:10837ms step_avg:94.23ms
step:116/1770 train_time:10931ms step_avg:94.24ms
step:117/1770 train_time:11025ms step_avg:94.23ms
step:118/1770 train_time:11120ms step_avg:94.24ms
step:119/1770 train_time:11215ms step_avg:94.25ms
step:120/1770 train_time:11310ms step_avg:94.25ms
step:121/1770 train_time:11405ms step_avg:94.25ms
step:122/1770 train_time:11500ms step_avg:94.26ms
step:123/1770 train_time:11595ms step_avg:94.27ms
step:124/1770 train_time:11689ms step_avg:94.26ms
step:125/1770 train_time:11783ms step_avg:94.26ms
step:125/1770 val_loss:4.6335 train_time:11877ms step_avg:95.02ms
step:126/1770 train_time:11898ms step_avg:94.43ms
step:127/1770 train_time:11978ms step_avg:94.31ms
step:128/1770 train_time:12083ms step_avg:94.40ms
step:129/1770 train_time:12179ms step_avg:94.41ms
step:130/1770 train_time:12273ms step_avg:94.41ms
step:131/1770 train_time:12367ms step_avg:94.40ms
step:132/1770 train_time:12460ms step_avg:94.40ms
step:133/1770 train_time:12555ms step_avg:94.40ms
step:134/1770 train_time:12649ms step_avg:94.40ms
step:135/1770 train_time:12743ms step_avg:94.39ms
step:136/1770 train_time:12838ms step_avg:94.39ms
step:137/1770 train_time:12932ms step_avg:94.40ms
step:138/1770 train_time:13030ms step_avg:94.42ms
step:139/1770 train_time:13128ms step_avg:94.45ms
step:140/1770 train_time:13225ms step_avg:94.46ms
step:141/1770 train_time:13320ms step_avg:94.47ms
step:142/1770 train_time:13414ms step_avg:94.47ms
step:143/1770 train_time:13509ms step_avg:94.47ms
step:144/1770 train_time:13604ms step_avg:94.48ms
step:145/1770 train_time:13699ms step_avg:94.48ms
step:146/1770 train_time:13793ms step_avg:94.48ms
step:147/1770 train_time:13888ms step_avg:94.47ms
step:148/1770 train_time:13983ms step_avg:94.48ms
step:149/1770 train_time:14079ms step_avg:94.49ms
step:150/1770 train_time:14174ms step_avg:94.50ms
step:151/1770 train_time:14270ms step_avg:94.50ms
step:152/1770 train_time:14367ms step_avg:94.52ms
step:153/1770 train_time:14462ms step_avg:94.52ms
step:154/1770 train_time:14556ms step_avg:94.52ms
step:155/1770 train_time:14651ms step_avg:94.52ms
step:156/1770 train_time:14746ms step_avg:94.53ms
step:157/1770 train_time:14841ms step_avg:94.53ms
step:158/1770 train_time:14936ms step_avg:94.53ms
step:159/1770 train_time:15031ms step_avg:94.54ms
step:160/1770 train_time:15128ms step_avg:94.55ms
step:161/1770 train_time:15223ms step_avg:94.55ms
step:162/1770 train_time:15319ms step_avg:94.56ms
step:163/1770 train_time:15414ms step_avg:94.56ms
step:164/1770 train_time:15510ms step_avg:94.57ms
step:165/1770 train_time:15605ms step_avg:94.58ms
step:166/1770 train_time:15700ms step_avg:94.58ms
step:167/1770 train_time:15795ms step_avg:94.58ms
step:168/1770 train_time:15890ms step_avg:94.58ms
step:169/1770 train_time:15985ms step_avg:94.59ms
step:170/1770 train_time:16081ms step_avg:94.59ms
step:171/1770 train_time:16176ms step_avg:94.60ms
step:172/1770 train_time:16273ms step_avg:94.61ms
step:173/1770 train_time:16369ms step_avg:94.62ms
step:174/1770 train_time:16465ms step_avg:94.62ms
step:175/1770 train_time:16560ms step_avg:94.63ms
step:176/1770 train_time:16655ms step_avg:94.63ms
step:177/1770 train_time:16750ms step_avg:94.63ms
step:178/1770 train_time:16845ms step_avg:94.64ms
step:179/1770 train_time:16940ms step_avg:94.64ms
step:180/1770 train_time:17035ms step_avg:94.64ms
step:181/1770 train_time:17130ms step_avg:94.64ms
step:182/1770 train_time:17226ms step_avg:94.65ms
step:183/1770 train_time:17321ms step_avg:94.65ms
step:184/1770 train_time:17417ms step_avg:94.66ms
step:185/1770 train_time:17512ms step_avg:94.66ms
step:186/1770 train_time:17608ms step_avg:94.67ms
step:187/1770 train_time:17702ms step_avg:94.67ms
step:188/1770 train_time:17797ms step_avg:94.67ms
step:189/1770 train_time:17891ms step_avg:94.66ms
step:190/1770 train_time:17986ms step_avg:94.66ms
step:191/1770 train_time:18082ms step_avg:94.67ms
step:192/1770 train_time:18177ms step_avg:94.67ms
step:193/1770 train_time:18272ms step_avg:94.67ms
step:194/1770 train_time:18368ms step_avg:94.68ms
step:195/1770 train_time:18462ms step_avg:94.68ms
step:196/1770 train_time:18558ms step_avg:94.68ms
step:197/1770 train_time:18654ms step_avg:94.69ms
step:198/1770 train_time:18748ms step_avg:94.69ms
step:199/1770 train_time:18843ms step_avg:94.69ms
step:200/1770 train_time:18938ms step_avg:94.69ms
step:201/1770 train_time:19032ms step_avg:94.69ms
step:202/1770 train_time:19128ms step_avg:94.69ms
step:203/1770 train_time:19223ms step_avg:94.69ms
step:204/1770 train_time:19318ms step_avg:94.70ms
step:205/1770 train_time:19413ms step_avg:94.70ms
step:206/1770 train_time:19509ms step_avg:94.70ms
step:207/1770 train_time:19604ms step_avg:94.71ms
step:208/1770 train_time:19699ms step_avg:94.71ms
step:209/1770 train_time:19795ms step_avg:94.71ms
step:210/1770 train_time:19890ms step_avg:94.71ms
step:211/1770 train_time:19985ms step_avg:94.72ms
step:212/1770 train_time:20080ms step_avg:94.72ms
step:213/1770 train_time:20174ms step_avg:94.71ms
step:214/1770 train_time:20271ms step_avg:94.73ms
step:215/1770 train_time:20367ms step_avg:94.73ms
step:216/1770 train_time:20461ms step_avg:94.73ms
step:217/1770 train_time:20557ms step_avg:94.73ms
step:218/1770 train_time:20652ms step_avg:94.73ms
step:219/1770 train_time:20747ms step_avg:94.73ms
step:220/1770 train_time:20842ms step_avg:94.74ms
step:221/1770 train_time:20937ms step_avg:94.74ms
step:222/1770 train_time:21032ms step_avg:94.74ms
step:223/1770 train_time:21128ms step_avg:94.74ms
step:224/1770 train_time:21224ms step_avg:94.75ms
step:225/1770 train_time:21319ms step_avg:94.75ms
step:226/1770 train_time:21415ms step_avg:94.76ms
step:227/1770 train_time:21510ms step_avg:94.76ms
step:228/1770 train_time:21606ms step_avg:94.76ms
step:229/1770 train_time:21701ms step_avg:94.76ms
step:230/1770 train_time:21795ms step_avg:94.76ms
step:231/1770 train_time:21890ms step_avg:94.76ms
step:232/1770 train_time:21985ms step_avg:94.76ms
step:233/1770 train_time:22080ms step_avg:94.76ms
step:234/1770 train_time:22174ms step_avg:94.76ms
step:235/1770 train_time:22269ms step_avg:94.76ms
step:236/1770 train_time:22365ms step_avg:94.77ms
step:237/1770 train_time:22459ms step_avg:94.77ms
step:238/1770 train_time:22555ms step_avg:94.77ms
step:239/1770 train_time:22650ms step_avg:94.77ms
step:240/1770 train_time:22745ms step_avg:94.77ms
step:241/1770 train_time:22840ms step_avg:94.77ms
step:242/1770 train_time:22935ms step_avg:94.77ms
step:243/1770 train_time:23030ms step_avg:94.77ms
step:244/1770 train_time:23125ms step_avg:94.78ms
step:245/1770 train_time:23220ms step_avg:94.78ms
step:246/1770 train_time:23316ms step_avg:94.78ms
step:247/1770 train_time:23411ms step_avg:94.78ms
step:248/1770 train_time:23508ms step_avg:94.79ms
step:249/1770 train_time:23602ms step_avg:94.79ms
step:250/1770 train_time:23698ms step_avg:94.79ms
step:250/1770 val_loss:4.1097 train_time:23792ms step_avg:95.17ms
step:251/1770 train_time:23811ms step_avg:94.87ms
step:252/1770 train_time:23892ms step_avg:94.81ms
step:253/1770 train_time:23991ms step_avg:94.83ms
step:254/1770 train_time:24086ms step_avg:94.83ms
step:255/1770 train_time:24181ms step_avg:94.83ms
step:256/1770 train_time:24275ms step_avg:94.83ms
step:257/1770 train_time:24370ms step_avg:94.82ms
step:258/1770 train_time:24463ms step_avg:94.82ms
step:259/1770 train_time:24558ms step_avg:94.82ms
step:260/1770 train_time:24653ms step_avg:94.82ms
step:261/1770 train_time:24747ms step_avg:94.82ms
step:262/1770 train_time:24843ms step_avg:94.82ms
step:263/1770 train_time:24940ms step_avg:94.83ms
step:264/1770 train_time:25037ms step_avg:94.84ms
step:265/1770 train_time:25133ms step_avg:94.84ms
step:266/1770 train_time:25228ms step_avg:94.84ms
step:267/1770 train_time:25324ms step_avg:94.84ms
step:268/1770 train_time:25419ms step_avg:94.85ms
step:269/1770 train_time:25514ms step_avg:94.85ms
step:270/1770 train_time:25609ms step_avg:94.85ms
step:271/1770 train_time:25704ms step_avg:94.85ms
step:272/1770 train_time:25799ms step_avg:94.85ms
step:273/1770 train_time:25896ms step_avg:94.86ms
step:274/1770 train_time:25993ms step_avg:94.86ms
step:275/1770 train_time:26089ms step_avg:94.87ms
step:276/1770 train_time:26184ms step_avg:94.87ms
step:277/1770 train_time:26281ms step_avg:94.88ms
step:278/1770 train_time:26376ms step_avg:94.88ms
step:279/1770 train_time:26471ms step_avg:94.88ms
step:280/1770 train_time:26567ms step_avg:94.88ms
step:281/1770 train_time:26662ms step_avg:94.88ms
step:282/1770 train_time:26757ms step_avg:94.88ms
step:283/1770 train_time:26853ms step_avg:94.89ms
step:284/1770 train_time:26949ms step_avg:94.89ms
step:285/1770 train_time:27044ms step_avg:94.89ms
step:286/1770 train_time:27140ms step_avg:94.90ms
step:287/1770 train_time:27236ms step_avg:94.90ms
step:288/1770 train_time:27331ms step_avg:94.90ms
step:289/1770 train_time:27427ms step_avg:94.90ms
step:290/1770 train_time:27522ms step_avg:94.90ms
step:291/1770 train_time:27618ms step_avg:94.91ms
step:292/1770 train_time:27714ms step_avg:94.91ms
step:293/1770 train_time:27809ms step_avg:94.91ms
step:294/1770 train_time:27904ms step_avg:94.91ms
step:295/1770 train_time:28000ms step_avg:94.92ms
step:296/1770 train_time:28096ms step_avg:94.92ms
step:297/1770 train_time:28192ms step_avg:94.92ms
step:298/1770 train_time:28287ms step_avg:94.92ms
step:299/1770 train_time:28382ms step_avg:94.92ms
step:300/1770 train_time:28478ms step_avg:94.93ms
step:301/1770 train_time:28573ms step_avg:94.93ms
step:302/1770 train_time:28669ms step_avg:94.93ms
step:303/1770 train_time:28765ms step_avg:94.93ms
step:304/1770 train_time:28861ms step_avg:94.94ms
step:305/1770 train_time:28957ms step_avg:94.94ms
step:306/1770 train_time:29052ms step_avg:94.94ms
step:307/1770 train_time:29148ms step_avg:94.94ms
step:308/1770 train_time:29243ms step_avg:94.95ms
step:309/1770 train_time:29339ms step_avg:94.95ms
step:310/1770 train_time:29435ms step_avg:94.95ms
step:311/1770 train_time:29531ms step_avg:94.95ms
step:312/1770 train_time:29626ms step_avg:94.96ms
step:313/1770 train_time:29721ms step_avg:94.96ms
step:314/1770 train_time:29817ms step_avg:94.96ms
step:315/1770 train_time:29913ms step_avg:94.96ms
step:316/1770 train_time:30009ms step_avg:94.96ms
step:317/1770 train_time:30104ms step_avg:94.96ms
step:318/1770 train_time:30199ms step_avg:94.97ms
step:319/1770 train_time:30295ms step_avg:94.97ms
step:320/1770 train_time:30390ms step_avg:94.97ms
step:321/1770 train_time:30486ms step_avg:94.97ms
step:322/1770 train_time:30581ms step_avg:94.97ms
step:323/1770 train_time:30677ms step_avg:94.98ms
step:324/1770 train_time:30773ms step_avg:94.98ms
step:325/1770 train_time:30868ms step_avg:94.98ms
step:326/1770 train_time:30963ms step_avg:94.98ms
step:327/1770 train_time:31059ms step_avg:94.98ms
step:328/1770 train_time:31155ms step_avg:94.98ms
step:329/1770 train_time:31251ms step_avg:94.99ms
step:330/1770 train_time:31346ms step_avg:94.99ms
step:331/1770 train_time:31441ms step_avg:94.99ms
step:332/1770 train_time:31537ms step_avg:94.99ms
step:333/1770 train_time:31633ms step_avg:94.99ms
step:334/1770 train_time:31729ms step_avg:95.00ms
step:335/1770 train_time:31825ms step_avg:95.00ms
step:336/1770 train_time:31921ms step_avg:95.00ms
step:337/1770 train_time:32017ms step_avg:95.01ms
step:338/1770 train_time:32112ms step_avg:95.01ms
step:339/1770 train_time:32208ms step_avg:95.01ms
step:340/1770 train_time:32303ms step_avg:95.01ms
step:341/1770 train_time:32398ms step_avg:95.01ms
step:342/1770 train_time:32495ms step_avg:95.01ms
step:343/1770 train_time:32590ms step_avg:95.01ms
step:344/1770 train_time:32686ms step_avg:95.02ms
step:345/1770 train_time:32782ms step_avg:95.02ms
step:346/1770 train_time:32878ms step_avg:95.02ms
step:347/1770 train_time:32974ms step_avg:95.03ms
step:348/1770 train_time:33070ms step_avg:95.03ms
step:349/1770 train_time:33165ms step_avg:95.03ms
step:350/1770 train_time:33261ms step_avg:95.03ms
step:351/1770 train_time:33356ms step_avg:95.03ms
step:352/1770 train_time:33452ms step_avg:95.03ms
step:353/1770 train_time:33547ms step_avg:95.04ms
step:354/1770 train_time:33643ms step_avg:95.04ms
step:355/1770 train_time:33740ms step_avg:95.04ms
step:356/1770 train_time:33836ms step_avg:95.05ms
step:357/1770 train_time:33932ms step_avg:95.05ms
step:358/1770 train_time:34027ms step_avg:95.05ms
step:359/1770 train_time:34122ms step_avg:95.05ms
step:360/1770 train_time:34218ms step_avg:95.05ms
step:361/1770 train_time:34314ms step_avg:95.05ms
step:362/1770 train_time:34410ms step_avg:95.05ms
step:363/1770 train_time:34505ms step_avg:95.06ms
step:364/1770 train_time:34601ms step_avg:95.06ms
step:365/1770 train_time:34697ms step_avg:95.06ms
step:366/1770 train_time:34792ms step_avg:95.06ms
step:367/1770 train_time:34887ms step_avg:95.06ms
step:368/1770 train_time:34983ms step_avg:95.06ms
step:369/1770 train_time:35079ms step_avg:95.07ms
step:370/1770 train_time:35175ms step_avg:95.07ms
step:371/1770 train_time:35271ms step_avg:95.07ms
step:372/1770 train_time:35366ms step_avg:95.07ms
step:373/1770 train_time:35463ms step_avg:95.07ms
step:374/1770 train_time:35558ms step_avg:95.08ms
step:375/1770 train_time:35654ms step_avg:95.08ms
step:375/1770 val_loss:3.9038 train_time:35748ms step_avg:95.33ms
step:376/1770 train_time:35766ms step_avg:95.12ms
step:377/1770 train_time:35851ms step_avg:95.09ms
step:378/1770 train_time:35951ms step_avg:95.11ms
step:379/1770 train_time:36047ms step_avg:95.11ms
step:380/1770 train_time:36143ms step_avg:95.11ms
step:381/1770 train_time:36238ms step_avg:95.11ms
step:382/1770 train_time:36333ms step_avg:95.11ms
step:383/1770 train_time:36428ms step_avg:95.11ms
step:384/1770 train_time:36523ms step_avg:95.11ms
step:385/1770 train_time:36618ms step_avg:95.11ms
step:386/1770 train_time:36712ms step_avg:95.11ms
step:387/1770 train_time:36809ms step_avg:95.11ms
step:388/1770 train_time:36906ms step_avg:95.12ms
step:389/1770 train_time:37003ms step_avg:95.12ms
step:390/1770 train_time:37099ms step_avg:95.13ms
step:391/1770 train_time:37195ms step_avg:95.13ms
step:392/1770 train_time:37290ms step_avg:95.13ms
step:393/1770 train_time:37386ms step_avg:95.13ms
step:394/1770 train_time:37481ms step_avg:95.13ms
step:395/1770 train_time:37576ms step_avg:95.13ms
step:396/1770 train_time:37673ms step_avg:95.13ms
step:397/1770 train_time:37771ms step_avg:95.14ms
step:398/1770 train_time:37869ms step_avg:95.15ms
step:399/1770 train_time:37967ms step_avg:95.16ms
step:400/1770 train_time:38067ms step_avg:95.17ms
step:401/1770 train_time:38165ms step_avg:95.18ms
step:402/1770 train_time:38263ms step_avg:95.18ms
step:403/1770 train_time:38361ms step_avg:95.19ms
step:404/1770 train_time:38459ms step_avg:95.19ms
step:405/1770 train_time:38557ms step_avg:95.20ms
step:406/1770 train_time:38654ms step_avg:95.21ms
step:407/1770 train_time:38752ms step_avg:95.21ms
step:408/1770 train_time:38849ms step_avg:95.22ms
step:409/1770 train_time:38947ms step_avg:95.22ms
step:410/1770 train_time:39045ms step_avg:95.23ms
step:411/1770 train_time:39143ms step_avg:95.24ms
step:412/1770 train_time:39241ms step_avg:95.24ms
step:413/1770 train_time:39339ms step_avg:95.25ms
step:414/1770 train_time:39437ms step_avg:95.26ms
step:415/1770 train_time:39535ms step_avg:95.26ms
step:416/1770 train_time:39632ms step_avg:95.27ms
step:417/1770 train_time:39730ms step_avg:95.28ms
step:418/1770 train_time:39829ms step_avg:95.28ms
step:419/1770 train_time:39927ms step_avg:95.29ms
step:420/1770 train_time:40025ms step_avg:95.30ms
step:421/1770 train_time:40122ms step_avg:95.30ms
step:422/1770 train_time:40220ms step_avg:95.31ms
step:423/1770 train_time:40318ms step_avg:95.31ms
step:424/1770 train_time:40416ms step_avg:95.32ms
step:425/1770 train_time:40513ms step_avg:95.32ms
step:426/1770 train_time:40612ms step_avg:95.33ms
step:427/1770 train_time:40709ms step_avg:95.34ms
step:428/1770 train_time:40808ms step_avg:95.35ms
step:429/1770 train_time:40907ms step_avg:95.35ms
step:430/1770 train_time:41004ms step_avg:95.36ms
step:431/1770 train_time:41102ms step_avg:95.37ms
step:432/1770 train_time:41200ms step_avg:95.37ms
step:433/1770 train_time:41298ms step_avg:95.38ms
step:434/1770 train_time:41395ms step_avg:95.38ms
step:435/1770 train_time:41492ms step_avg:95.38ms
step:436/1770 train_time:41590ms step_avg:95.39ms
step:437/1770 train_time:41688ms step_avg:95.39ms
step:438/1770 train_time:41785ms step_avg:95.40ms
step:439/1770 train_time:41883ms step_avg:95.40ms
step:440/1770 train_time:41981ms step_avg:95.41ms
step:441/1770 train_time:42079ms step_avg:95.42ms
step:442/1770 train_time:42177ms step_avg:95.42ms
step:443/1770 train_time:42274ms step_avg:95.43ms
step:444/1770 train_time:42372ms step_avg:95.43ms
step:445/1770 train_time:42470ms step_avg:95.44ms
step:446/1770 train_time:42569ms step_avg:95.45ms
step:447/1770 train_time:42667ms step_avg:95.45ms
step:448/1770 train_time:42765ms step_avg:95.46ms
step:449/1770 train_time:42863ms step_avg:95.46ms
step:450/1770 train_time:42961ms step_avg:95.47ms
step:451/1770 train_time:43059ms step_avg:95.47ms
step:452/1770 train_time:43157ms step_avg:95.48ms
step:453/1770 train_time:43255ms step_avg:95.48ms
step:454/1770 train_time:43352ms step_avg:95.49ms
step:455/1770 train_time:43450ms step_avg:95.49ms
step:456/1770 train_time:43548ms step_avg:95.50ms
step:457/1770 train_time:43647ms step_avg:95.51ms
step:458/1770 train_time:43745ms step_avg:95.51ms
step:459/1770 train_time:43842ms step_avg:95.52ms
step:460/1770 train_time:43941ms step_avg:95.52ms
step:461/1770 train_time:44038ms step_avg:95.53ms
step:462/1770 train_time:44136ms step_avg:95.53ms
step:463/1770 train_time:44234ms step_avg:95.54ms
step:464/1770 train_time:44332ms step_avg:95.54ms
step:465/1770 train_time:44429ms step_avg:95.55ms
step:466/1770 train_time:44527ms step_avg:95.55ms
step:467/1770 train_time:44625ms step_avg:95.56ms
step:468/1770 train_time:44722ms step_avg:95.56ms
step:469/1770 train_time:44821ms step_avg:95.57ms
step:470/1770 train_time:44919ms step_avg:95.57ms
step:471/1770 train_time:45017ms step_avg:95.58ms
step:472/1770 train_time:45115ms step_avg:95.58ms
step:473/1770 train_time:45213ms step_avg:95.59ms
step:474/1770 train_time:45311ms step_avg:95.59ms
step:475/1770 train_time:45408ms step_avg:95.60ms
step:476/1770 train_time:45507ms step_avg:95.60ms
step:477/1770 train_time:45604ms step_avg:95.61ms
step:478/1770 train_time:45702ms step_avg:95.61ms
step:479/1770 train_time:45801ms step_avg:95.62ms
step:480/1770 train_time:45900ms step_avg:95.62ms
step:481/1770 train_time:45998ms step_avg:95.63ms
step:482/1770 train_time:46095ms step_avg:95.63ms
step:483/1770 train_time:46193ms step_avg:95.64ms
step:484/1770 train_time:46291ms step_avg:95.64ms
step:485/1770 train_time:46389ms step_avg:95.65ms
step:486/1770 train_time:46486ms step_avg:95.65ms
step:487/1770 train_time:46584ms step_avg:95.65ms
step:488/1770 train_time:46682ms step_avg:95.66ms
step:489/1770 train_time:46780ms step_avg:95.66ms
step:490/1770 train_time:46878ms step_avg:95.67ms
step:491/1770 train_time:46975ms step_avg:95.67ms
step:492/1770 train_time:47073ms step_avg:95.68ms
step:493/1770 train_time:47171ms step_avg:95.68ms
step:494/1770 train_time:47269ms step_avg:95.69ms
step:495/1770 train_time:47367ms step_avg:95.69ms
step:496/1770 train_time:47465ms step_avg:95.70ms
step:497/1770 train_time:47562ms step_avg:95.70ms
step:498/1770 train_time:47659ms step_avg:95.70ms
step:499/1770 train_time:47758ms step_avg:95.71ms
step:500/1770 train_time:47856ms step_avg:95.71ms
step:500/1770 val_loss:3.7528 train_time:47953ms step_avg:95.91ms
step:501/1770 train_time:47971ms step_avg:95.75ms
step:502/1770 train_time:48060ms step_avg:95.74ms
step:503/1770 train_time:48162ms step_avg:95.75ms
step:504/1770 train_time:48260ms step_avg:95.75ms
step:505/1770 train_time:48358ms step_avg:95.76ms
step:506/1770 train_time:48456ms step_avg:95.76ms
step:507/1770 train_time:48554ms step_avg:95.77ms
step:508/1770 train_time:48651ms step_avg:95.77ms
step:509/1770 train_time:48748ms step_avg:95.77ms
step:510/1770 train_time:48845ms step_avg:95.77ms
step:511/1770 train_time:48943ms step_avg:95.78ms
step:512/1770 train_time:49043ms step_avg:95.79ms
step:513/1770 train_time:49142ms step_avg:95.79ms
step:514/1770 train_time:49241ms step_avg:95.80ms
step:515/1770 train_time:49339ms step_avg:95.80ms
step:516/1770 train_time:49437ms step_avg:95.81ms
step:517/1770 train_time:49536ms step_avg:95.81ms
step:518/1770 train_time:49634ms step_avg:95.82ms
step:519/1770 train_time:49731ms step_avg:95.82ms
step:520/1770 train_time:49828ms step_avg:95.82ms
step:521/1770 train_time:49926ms step_avg:95.83ms
step:522/1770 train_time:50024ms step_avg:95.83ms
step:523/1770 train_time:50122ms step_avg:95.84ms
step:524/1770 train_time:50221ms step_avg:95.84ms
step:525/1770 train_time:50319ms step_avg:95.85ms
step:526/1770 train_time:50417ms step_avg:95.85ms
step:527/1770 train_time:50516ms step_avg:95.86ms
step:528/1770 train_time:50614ms step_avg:95.86ms
step:529/1770 train_time:50713ms step_avg:95.87ms
step:530/1770 train_time:50811ms step_avg:95.87ms
step:531/1770 train_time:50908ms step_avg:95.87ms
step:532/1770 train_time:51006ms step_avg:95.88ms
step:533/1770 train_time:51105ms step_avg:95.88ms
step:534/1770 train_time:51204ms step_avg:95.89ms
step:535/1770 train_time:51303ms step_avg:95.89ms
step:536/1770 train_time:51402ms step_avg:95.90ms
step:537/1770 train_time:51501ms step_avg:95.90ms
step:538/1770 train_time:51600ms step_avg:95.91ms
step:539/1770 train_time:51700ms step_avg:95.92ms
step:540/1770 train_time:51800ms step_avg:95.93ms
step:541/1770 train_time:51899ms step_avg:95.93ms
step:542/1770 train_time:51998ms step_avg:95.94ms
step:543/1770 train_time:52097ms step_avg:95.94ms
step:544/1770 train_time:52195ms step_avg:95.95ms
step:545/1770 train_time:52293ms step_avg:95.95ms
step:546/1770 train_time:52392ms step_avg:95.96ms
step:547/1770 train_time:52490ms step_avg:95.96ms
step:548/1770 train_time:52588ms step_avg:95.96ms
step:549/1770 train_time:52686ms step_avg:95.97ms
step:550/1770 train_time:52785ms step_avg:95.97ms
step:551/1770 train_time:52883ms step_avg:95.98ms
step:552/1770 train_time:52983ms step_avg:95.98ms
step:553/1770 train_time:53081ms step_avg:95.99ms
step:554/1770 train_time:53180ms step_avg:95.99ms
step:555/1770 train_time:53278ms step_avg:96.00ms
step:556/1770 train_time:53377ms step_avg:96.00ms
step:557/1770 train_time:53476ms step_avg:96.01ms
step:558/1770 train_time:53574ms step_avg:96.01ms
step:559/1770 train_time:53673ms step_avg:96.02ms
step:560/1770 train_time:53772ms step_avg:96.02ms
step:561/1770 train_time:53871ms step_avg:96.03ms
step:562/1770 train_time:53970ms step_avg:96.03ms
step:563/1770 train_time:54068ms step_avg:96.03ms
step:564/1770 train_time:54166ms step_avg:96.04ms
step:565/1770 train_time:54264ms step_avg:96.04ms
step:566/1770 train_time:54362ms step_avg:96.05ms
step:567/1770 train_time:54460ms step_avg:96.05ms
step:568/1770 train_time:54559ms step_avg:96.05ms
step:569/1770 train_time:54658ms step_avg:96.06ms
step:570/1770 train_time:54757ms step_avg:96.06ms
step:571/1770 train_time:54855ms step_avg:96.07ms
step:572/1770 train_time:54954ms step_avg:96.07ms
step:573/1770 train_time:55052ms step_avg:96.08ms
step:574/1770 train_time:55150ms step_avg:96.08ms
step:575/1770 train_time:55249ms step_avg:96.08ms
step:576/1770 train_time:55347ms step_avg:96.09ms
step:577/1770 train_time:55446ms step_avg:96.09ms
step:578/1770 train_time:55545ms step_avg:96.10ms
step:579/1770 train_time:55644ms step_avg:96.10ms
step:580/1770 train_time:55743ms step_avg:96.11ms
step:581/1770 train_time:55843ms step_avg:96.11ms
step:582/1770 train_time:55943ms step_avg:96.12ms
step:583/1770 train_time:56042ms step_avg:96.13ms
step:584/1770 train_time:56141ms step_avg:96.13ms
step:585/1770 train_time:56240ms step_avg:96.14ms
step:586/1770 train_time:56339ms step_avg:96.14ms
step:587/1770 train_time:56437ms step_avg:96.14ms
step:588/1770 train_time:56536ms step_avg:96.15ms
step:589/1770 train_time:56634ms step_avg:96.15ms
step:590/1770 train_time:56732ms step_avg:96.16ms
step:591/1770 train_time:56831ms step_avg:96.16ms
step:592/1770 train_time:56930ms step_avg:96.17ms
step:593/1770 train_time:57028ms step_avg:96.17ms
step:594/1770 train_time:57126ms step_avg:96.17ms
step:595/1770 train_time:57225ms step_avg:96.18ms
step:596/1770 train_time:57324ms step_avg:96.18ms
step:597/1770 train_time:57423ms step_avg:96.19ms
step:598/1770 train_time:57522ms step_avg:96.19ms
step:599/1770 train_time:57620ms step_avg:96.19ms
step:600/1770 train_time:57718ms step_avg:96.20ms
step:601/1770 train_time:57818ms step_avg:96.20ms
step:602/1770 train_time:57917ms step_avg:96.21ms
step:603/1770 train_time:58015ms step_avg:96.21ms
step:604/1770 train_time:58114ms step_avg:96.21ms
step:605/1770 train_time:58212ms step_avg:96.22ms
step:606/1770 train_time:58311ms step_avg:96.22ms
step:607/1770 train_time:58411ms step_avg:96.23ms
step:608/1770 train_time:58511ms step_avg:96.23ms
step:609/1770 train_time:58609ms step_avg:96.24ms
step:610/1770 train_time:58707ms step_avg:96.24ms
step:611/1770 train_time:58805ms step_avg:96.24ms
step:612/1770 train_time:58905ms step_avg:96.25ms
step:613/1770 train_time:59004ms step_avg:96.25ms
step:614/1770 train_time:59102ms step_avg:96.26ms
step:615/1770 train_time:59202ms step_avg:96.26ms
step:616/1770 train_time:59302ms step_avg:96.27ms
step:617/1770 train_time:59402ms step_avg:96.28ms
step:618/1770 train_time:59501ms step_avg:96.28ms
step:619/1770 train_time:59601ms step_avg:96.29ms
step:620/1770 train_time:59699ms step_avg:96.29ms
step:621/1770 train_time:59798ms step_avg:96.29ms
step:622/1770 train_time:59896ms step_avg:96.30ms
step:623/1770 train_time:59994ms step_avg:96.30ms
step:624/1770 train_time:60093ms step_avg:96.30ms
step:625/1770 train_time:60191ms step_avg:96.31ms
step:625/1770 val_loss:3.6660 train_time:60288ms step_avg:96.46ms
step:626/1770 train_time:60306ms step_avg:96.34ms
step:627/1770 train_time:60395ms step_avg:96.32ms
step:628/1770 train_time:60497ms step_avg:96.33ms
step:629/1770 train_time:60596ms step_avg:96.34ms
step:630/1770 train_time:60694ms step_avg:96.34ms
step:631/1770 train_time:60792ms step_avg:96.34ms
step:632/1770 train_time:60890ms step_avg:96.34ms
step:633/1770 train_time:60988ms step_avg:96.35ms
step:634/1770 train_time:61085ms step_avg:96.35ms
step:635/1770 train_time:61183ms step_avg:96.35ms
step:636/1770 train_time:61281ms step_avg:96.35ms
step:637/1770 train_time:61381ms step_avg:96.36ms
step:638/1770 train_time:61480ms step_avg:96.36ms
step:639/1770 train_time:61578ms step_avg:96.37ms
step:640/1770 train_time:61677ms step_avg:96.37ms
step:641/1770 train_time:61775ms step_avg:96.37ms
step:642/1770 train_time:61873ms step_avg:96.38ms
step:643/1770 train_time:61971ms step_avg:96.38ms
step:644/1770 train_time:62069ms step_avg:96.38ms
step:645/1770 train_time:62168ms step_avg:96.38ms
step:646/1770 train_time:62266ms step_avg:96.39ms
step:647/1770 train_time:62365ms step_avg:96.39ms
step:648/1770 train_time:62464ms step_avg:96.40ms
step:649/1770 train_time:62563ms step_avg:96.40ms
step:650/1770 train_time:62662ms step_avg:96.40ms
step:651/1770 train_time:62761ms step_avg:96.41ms
step:652/1770 train_time:62860ms step_avg:96.41ms
step:653/1770 train_time:62958ms step_avg:96.41ms
step:654/1770 train_time:63056ms step_avg:96.42ms
step:655/1770 train_time:63154ms step_avg:96.42ms
step:656/1770 train_time:63252ms step_avg:96.42ms
step:657/1770 train_time:63351ms step_avg:96.42ms
step:658/1770 train_time:63451ms step_avg:96.43ms
step:659/1770 train_time:63553ms step_avg:96.44ms
step:660/1770 train_time:63654ms step_avg:96.45ms
step:661/1770 train_time:63755ms step_avg:96.45ms
step:662/1770 train_time:63855ms step_avg:96.46ms
step:663/1770 train_time:63957ms step_avg:96.47ms
step:664/1770 train_time:64057ms step_avg:96.47ms
step:665/1770 train_time:64156ms step_avg:96.48ms
step:666/1770 train_time:64256ms step_avg:96.48ms
step:667/1770 train_time:64356ms step_avg:96.49ms
step:668/1770 train_time:64456ms step_avg:96.49ms
step:669/1770 train_time:64558ms step_avg:96.50ms
step:670/1770 train_time:64658ms step_avg:96.51ms
step:671/1770 train_time:64758ms step_avg:96.51ms
step:672/1770 train_time:64858ms step_avg:96.51ms
step:673/1770 train_time:64958ms step_avg:96.52ms
step:674/1770 train_time:65057ms step_avg:96.52ms
step:675/1770 train_time:65157ms step_avg:96.53ms
step:676/1770 train_time:65257ms step_avg:96.53ms
step:677/1770 train_time:65357ms step_avg:96.54ms
step:678/1770 train_time:65456ms step_avg:96.54ms
step:679/1770 train_time:65556ms step_avg:96.55ms
step:680/1770 train_time:65657ms step_avg:96.55ms
step:681/1770 train_time:65758ms step_avg:96.56ms
step:682/1770 train_time:65858ms step_avg:96.57ms
step:683/1770 train_time:65958ms step_avg:96.57ms
step:684/1770 train_time:66058ms step_avg:96.58ms
step:685/1770 train_time:66158ms step_avg:96.58ms
step:686/1770 train_time:66258ms step_avg:96.59ms
step:687/1770 train_time:66358ms step_avg:96.59ms
step:688/1770 train_time:66458ms step_avg:96.60ms
step:689/1770 train_time:66559ms step_avg:96.60ms
step:690/1770 train_time:66659ms step_avg:96.61ms
step:691/1770 train_time:66760ms step_avg:96.61ms
step:692/1770 train_time:66861ms step_avg:96.62ms
step:693/1770 train_time:66962ms step_avg:96.63ms
step:694/1770 train_time:67062ms step_avg:96.63ms
step:695/1770 train_time:67163ms step_avg:96.64ms
step:696/1770 train_time:67264ms step_avg:96.64ms
step:697/1770 train_time:67365ms step_avg:96.65ms
step:698/1770 train_time:67466ms step_avg:96.66ms
step:699/1770 train_time:67566ms step_avg:96.66ms
step:700/1770 train_time:67666ms step_avg:96.67ms
step:701/1770 train_time:67766ms step_avg:96.67ms
step:702/1770 train_time:67867ms step_avg:96.68ms
step:703/1770 train_time:67967ms step_avg:96.68ms
step:704/1770 train_time:68067ms step_avg:96.69ms
step:705/1770 train_time:68168ms step_avg:96.69ms
step:706/1770 train_time:68269ms step_avg:96.70ms
step:707/1770 train_time:68370ms step_avg:96.70ms
step:708/1770 train_time:68471ms step_avg:96.71ms
step:709/1770 train_time:68572ms step_avg:96.72ms
step:710/1770 train_time:68673ms step_avg:96.72ms
step:711/1770 train_time:68774ms step_avg:96.73ms
step:712/1770 train_time:68875ms step_avg:96.73ms
step:713/1770 train_time:68975ms step_avg:96.74ms
step:714/1770 train_time:69075ms step_avg:96.74ms
step:715/1770 train_time:69175ms step_avg:96.75ms
step:716/1770 train_time:69276ms step_avg:96.75ms
step:717/1770 train_time:69376ms step_avg:96.76ms
step:718/1770 train_time:69475ms step_avg:96.76ms
step:719/1770 train_time:69576ms step_avg:96.77ms
step:720/1770 train_time:69676ms step_avg:96.77ms
step:721/1770 train_time:69777ms step_avg:96.78ms
step:722/1770 train_time:69877ms step_avg:96.78ms
step:723/1770 train_time:69977ms step_avg:96.79ms
step:724/1770 train_time:70076ms step_avg:96.79ms
step:725/1770 train_time:70177ms step_avg:96.80ms
step:726/1770 train_time:70276ms step_avg:96.80ms
step:727/1770 train_time:70377ms step_avg:96.80ms
step:728/1770 train_time:70476ms step_avg:96.81ms
step:729/1770 train_time:70577ms step_avg:96.81ms
step:730/1770 train_time:70677ms step_avg:96.82ms
step:731/1770 train_time:70777ms step_avg:96.82ms
step:732/1770 train_time:70878ms step_avg:96.83ms
step:733/1770 train_time:70978ms step_avg:96.83ms
step:734/1770 train_time:71077ms step_avg:96.84ms
step:735/1770 train_time:71177ms step_avg:96.84ms
step:736/1770 train_time:71277ms step_avg:96.84ms
step:737/1770 train_time:71378ms step_avg:96.85ms
step:738/1770 train_time:71478ms step_avg:96.85ms
step:739/1770 train_time:71578ms step_avg:96.86ms
step:740/1770 train_time:71677ms step_avg:96.86ms
step:741/1770 train_time:71778ms step_avg:96.87ms
step:742/1770 train_time:71877ms step_avg:96.87ms
step:743/1770 train_time:71977ms step_avg:96.87ms
step:744/1770 train_time:72077ms step_avg:96.88ms
step:745/1770 train_time:72177ms step_avg:96.88ms
step:746/1770 train_time:72277ms step_avg:96.89ms
step:747/1770 train_time:72376ms step_avg:96.89ms
step:748/1770 train_time:72477ms step_avg:96.89ms
step:749/1770 train_time:72577ms step_avg:96.90ms
step:750/1770 train_time:72677ms step_avg:96.90ms
step:750/1770 val_loss:3.6006 train_time:72776ms step_avg:97.03ms
step:751/1770 train_time:72795ms step_avg:96.93ms
step:752/1770 train_time:72885ms step_avg:96.92ms
step:753/1770 train_time:72986ms step_avg:96.93ms
step:754/1770 train_time:73086ms step_avg:96.93ms
step:755/1770 train_time:73186ms step_avg:96.93ms
step:756/1770 train_time:73285ms step_avg:96.94ms
step:757/1770 train_time:73385ms step_avg:96.94ms
step:758/1770 train_time:73484ms step_avg:96.95ms
step:759/1770 train_time:73584ms step_avg:96.95ms
step:760/1770 train_time:73684ms step_avg:96.95ms
step:761/1770 train_time:73787ms step_avg:96.96ms
step:762/1770 train_time:73889ms step_avg:96.97ms
step:763/1770 train_time:73990ms step_avg:96.97ms
step:764/1770 train_time:74090ms step_avg:96.98ms
step:765/1770 train_time:74189ms step_avg:96.98ms
step:766/1770 train_time:74289ms step_avg:96.98ms
step:767/1770 train_time:74388ms step_avg:96.99ms
step:768/1770 train_time:74488ms step_avg:96.99ms
step:769/1770 train_time:74587ms step_avg:96.99ms
step:770/1770 train_time:74687ms step_avg:97.00ms
step:771/1770 train_time:74788ms step_avg:97.00ms
step:772/1770 train_time:74890ms step_avg:97.01ms
step:773/1770 train_time:74991ms step_avg:97.01ms
step:774/1770 train_time:75092ms step_avg:97.02ms
step:775/1770 train_time:75193ms step_avg:97.02ms
step:776/1770 train_time:75294ms step_avg:97.03ms
step:777/1770 train_time:75394ms step_avg:97.03ms
step:778/1770 train_time:75494ms step_avg:97.04ms
step:779/1770 train_time:75595ms step_avg:97.04ms
step:780/1770 train_time:75696ms step_avg:97.05ms
step:781/1770 train_time:75797ms step_avg:97.05ms
step:782/1770 train_time:75898ms step_avg:97.06ms
step:783/1770 train_time:75998ms step_avg:97.06ms
step:784/1770 train_time:76098ms step_avg:97.06ms
step:785/1770 train_time:76198ms step_avg:97.07ms
step:786/1770 train_time:76298ms step_avg:97.07ms
step:787/1770 train_time:76398ms step_avg:97.07ms
step:788/1770 train_time:76498ms step_avg:97.08ms
step:789/1770 train_time:76598ms step_avg:97.08ms
step:790/1770 train_time:76699ms step_avg:97.09ms
step:791/1770 train_time:76802ms step_avg:97.09ms
step:792/1770 train_time:76904ms step_avg:97.10ms
step:793/1770 train_time:77005ms step_avg:97.11ms
step:794/1770 train_time:77106ms step_avg:97.11ms
step:795/1770 train_time:77207ms step_avg:97.12ms
step:796/1770 train_time:77308ms step_avg:97.12ms
step:797/1770 train_time:77408ms step_avg:97.12ms
step:798/1770 train_time:77509ms step_avg:97.13ms
step:799/1770 train_time:77610ms step_avg:97.13ms
step:800/1770 train_time:77711ms step_avg:97.14ms
step:801/1770 train_time:77812ms step_avg:97.14ms
step:802/1770 train_time:77913ms step_avg:97.15ms
step:803/1770 train_time:78014ms step_avg:97.15ms
step:804/1770 train_time:78115ms step_avg:97.16ms
step:805/1770 train_time:78216ms step_avg:97.16ms
step:806/1770 train_time:78317ms step_avg:97.17ms
step:807/1770 train_time:78417ms step_avg:97.17ms
step:808/1770 train_time:78516ms step_avg:97.17ms
step:809/1770 train_time:78617ms step_avg:97.18ms
step:810/1770 train_time:78718ms step_avg:97.18ms
step:811/1770 train_time:78818ms step_avg:97.19ms
step:812/1770 train_time:78918ms step_avg:97.19ms
step:813/1770 train_time:79019ms step_avg:97.19ms
step:814/1770 train_time:79120ms step_avg:97.20ms
step:815/1770 train_time:79221ms step_avg:97.20ms
step:816/1770 train_time:79322ms step_avg:97.21ms
step:817/1770 train_time:79423ms step_avg:97.21ms
step:818/1770 train_time:79525ms step_avg:97.22ms
step:819/1770 train_time:79625ms step_avg:97.22ms
step:820/1770 train_time:79727ms step_avg:97.23ms
step:821/1770 train_time:79828ms step_avg:97.23ms
step:822/1770 train_time:79928ms step_avg:97.24ms
step:823/1770 train_time:80028ms step_avg:97.24ms
step:824/1770 train_time:80129ms step_avg:97.24ms
step:825/1770 train_time:80230ms step_avg:97.25ms
step:826/1770 train_time:80331ms step_avg:97.25ms
step:827/1770 train_time:80431ms step_avg:97.26ms
step:828/1770 train_time:80531ms step_avg:97.26ms
step:829/1770 train_time:80632ms step_avg:97.26ms
step:830/1770 train_time:80734ms step_avg:97.27ms
step:831/1770 train_time:80835ms step_avg:97.27ms
step:832/1770 train_time:80935ms step_avg:97.28ms
step:833/1770 train_time:81036ms step_avg:97.28ms
step:834/1770 train_time:81136ms step_avg:97.29ms
step:835/1770 train_time:81236ms step_avg:97.29ms
step:836/1770 train_time:81338ms step_avg:97.29ms
step:837/1770 train_time:81438ms step_avg:97.30ms
step:838/1770 train_time:81539ms step_avg:97.30ms
step:839/1770 train_time:81641ms step_avg:97.31ms
step:840/1770 train_time:81743ms step_avg:97.31ms
step:841/1770 train_time:81845ms step_avg:97.32ms
step:842/1770 train_time:81946ms step_avg:97.32ms
step:843/1770 train_time:82047ms step_avg:97.33ms
step:844/1770 train_time:82148ms step_avg:97.33ms
step:845/1770 train_time:82248ms step_avg:97.33ms
step:846/1770 train_time:82348ms step_avg:97.34ms
step:847/1770 train_time:82448ms step_avg:97.34ms
step:848/1770 train_time:82549ms step_avg:97.35ms
step:849/1770 train_time:82650ms step_avg:97.35ms
step:850/1770 train_time:82751ms step_avg:97.35ms
step:851/1770 train_time:82853ms step_avg:97.36ms
step:852/1770 train_time:82955ms step_avg:97.37ms
step:853/1770 train_time:83056ms step_avg:97.37ms
step:854/1770 train_time:83156ms step_avg:97.37ms
step:855/1770 train_time:83257ms step_avg:97.38ms
step:856/1770 train_time:83356ms step_avg:97.38ms
step:857/1770 train_time:83456ms step_avg:97.38ms
step:858/1770 train_time:83558ms step_avg:97.39ms
step:859/1770 train_time:83659ms step_avg:97.39ms
step:860/1770 train_time:83761ms step_avg:97.40ms
step:861/1770 train_time:83861ms step_avg:97.40ms
step:862/1770 train_time:83962ms step_avg:97.40ms
step:863/1770 train_time:84064ms step_avg:97.41ms
step:864/1770 train_time:84165ms step_avg:97.41ms
step:865/1770 train_time:84265ms step_avg:97.42ms
step:866/1770 train_time:84367ms step_avg:97.42ms
step:867/1770 train_time:84468ms step_avg:97.43ms
step:868/1770 train_time:84568ms step_avg:97.43ms
step:869/1770 train_time:84668ms step_avg:97.43ms
step:870/1770 train_time:84769ms step_avg:97.44ms
step:871/1770 train_time:84869ms step_avg:97.44ms
step:872/1770 train_time:84969ms step_avg:97.44ms
step:873/1770 train_time:85069ms step_avg:97.44ms
step:874/1770 train_time:85169ms step_avg:97.45ms
step:875/1770 train_time:85270ms step_avg:97.45ms
step:875/1770 val_loss:3.5510 train_time:85370ms step_avg:97.57ms
step:876/1770 train_time:85388ms step_avg:97.47ms
step:877/1770 train_time:85477ms step_avg:97.47ms
step:878/1770 train_time:85580ms step_avg:97.47ms
step:879/1770 train_time:85681ms step_avg:97.48ms
step:880/1770 train_time:85781ms step_avg:97.48ms
step:881/1770 train_time:85880ms step_avg:97.48ms
step:882/1770 train_time:85980ms step_avg:97.48ms
step:883/1770 train_time:86079ms step_avg:97.48ms
step:884/1770 train_time:86179ms step_avg:97.49ms
step:885/1770 train_time:86280ms step_avg:97.49ms
step:886/1770 train_time:86382ms step_avg:97.50ms
step:887/1770 train_time:86484ms step_avg:97.50ms
step:888/1770 train_time:86585ms step_avg:97.51ms
step:889/1770 train_time:86686ms step_avg:97.51ms
step:890/1770 train_time:86786ms step_avg:97.51ms
step:891/1770 train_time:86887ms step_avg:97.52ms
step:892/1770 train_time:86989ms step_avg:97.52ms
step:893/1770 train_time:87089ms step_avg:97.52ms
step:894/1770 train_time:87189ms step_avg:97.53ms
step:895/1770 train_time:87290ms step_avg:97.53ms
step:896/1770 train_time:87391ms step_avg:97.53ms
step:897/1770 train_time:87491ms step_avg:97.54ms
step:898/1770 train_time:87592ms step_avg:97.54ms
step:899/1770 train_time:87694ms step_avg:97.55ms
step:900/1770 train_time:87795ms step_avg:97.55ms
step:901/1770 train_time:87897ms step_avg:97.55ms
step:902/1770 train_time:87998ms step_avg:97.56ms
step:903/1770 train_time:88099ms step_avg:97.56ms
step:904/1770 train_time:88200ms step_avg:97.57ms
step:905/1770 train_time:88301ms step_avg:97.57ms
step:906/1770 train_time:88401ms step_avg:97.57ms
step:907/1770 train_time:88501ms step_avg:97.58ms
step:908/1770 train_time:88602ms step_avg:97.58ms
step:909/1770 train_time:88702ms step_avg:97.58ms
step:910/1770 train_time:88802ms step_avg:97.59ms
step:911/1770 train_time:88902ms step_avg:97.59ms
step:912/1770 train_time:89002ms step_avg:97.59ms
step:913/1770 train_time:89103ms step_avg:97.59ms
step:914/1770 train_time:89204ms step_avg:97.60ms
step:915/1770 train_time:89304ms step_avg:97.60ms
step:916/1770 train_time:89405ms step_avg:97.60ms
step:917/1770 train_time:89504ms step_avg:97.61ms
step:918/1770 train_time:89604ms step_avg:97.61ms
step:919/1770 train_time:89705ms step_avg:97.61ms
step:920/1770 train_time:89807ms step_avg:97.62ms
step:921/1770 train_time:89909ms step_avg:97.62ms
step:922/1770 train_time:90011ms step_avg:97.63ms
step:923/1770 train_time:90113ms step_avg:97.63ms
step:924/1770 train_time:90215ms step_avg:97.64ms
step:925/1770 train_time:90317ms step_avg:97.64ms
step:926/1770 train_time:90419ms step_avg:97.64ms
step:927/1770 train_time:90522ms step_avg:97.65ms
step:928/1770 train_time:90624ms step_avg:97.65ms
step:929/1770 train_time:90725ms step_avg:97.66ms
step:930/1770 train_time:90826ms step_avg:97.66ms
step:931/1770 train_time:90928ms step_avg:97.67ms
step:932/1770 train_time:91030ms step_avg:97.67ms
step:933/1770 train_time:91132ms step_avg:97.68ms
step:934/1770 train_time:91234ms step_avg:97.68ms
step:935/1770 train_time:91337ms step_avg:97.69ms
step:936/1770 train_time:91440ms step_avg:97.69ms
step:937/1770 train_time:91542ms step_avg:97.70ms
step:938/1770 train_time:91645ms step_avg:97.70ms
step:939/1770 train_time:91746ms step_avg:97.71ms
step:940/1770 train_time:91847ms step_avg:97.71ms
step:941/1770 train_time:91948ms step_avg:97.71ms
step:942/1770 train_time:92050ms step_avg:97.72ms
step:943/1770 train_time:92152ms step_avg:97.72ms
step:944/1770 train_time:92254ms step_avg:97.73ms
step:945/1770 train_time:92356ms step_avg:97.73ms
step:946/1770 train_time:92461ms step_avg:97.74ms
step:947/1770 train_time:92564ms step_avg:97.74ms
step:948/1770 train_time:92665ms step_avg:97.75ms
step:949/1770 train_time:92767ms step_avg:97.75ms
step:950/1770 train_time:92869ms step_avg:97.76ms
step:951/1770 train_time:92971ms step_avg:97.76ms
step:952/1770 train_time:93073ms step_avg:97.77ms
step:953/1770 train_time:93174ms step_avg:97.77ms
step:954/1770 train_time:93277ms step_avg:97.77ms
step:955/1770 train_time:93381ms step_avg:97.78ms
step:956/1770 train_time:93482ms step_avg:97.79ms
step:957/1770 train_time:93584ms step_avg:97.79ms
step:958/1770 train_time:93686ms step_avg:97.79ms
step:959/1770 train_time:93788ms step_avg:97.80ms
step:960/1770 train_time:93889ms step_avg:97.80ms
step:961/1770 train_time:93991ms step_avg:97.81ms
step:962/1770 train_time:94094ms step_avg:97.81ms
step:963/1770 train_time:94195ms step_avg:97.81ms
step:964/1770 train_time:94298ms step_avg:97.82ms
step:965/1770 train_time:94401ms step_avg:97.82ms
step:966/1770 train_time:94503ms step_avg:97.83ms
step:967/1770 train_time:94605ms step_avg:97.83ms
step:968/1770 train_time:94707ms step_avg:97.84ms
step:969/1770 train_time:94808ms step_avg:97.84ms
step:970/1770 train_time:94909ms step_avg:97.84ms
step:971/1770 train_time:95011ms step_avg:97.85ms
step:972/1770 train_time:95113ms step_avg:97.85ms
step:973/1770 train_time:95215ms step_avg:97.86ms
step:974/1770 train_time:95317ms step_avg:97.86ms
step:975/1770 train_time:95421ms step_avg:97.87ms
step:976/1770 train_time:95523ms step_avg:97.87ms
step:977/1770 train_time:95625ms step_avg:97.88ms
step:978/1770 train_time:95726ms step_avg:97.88ms
step:979/1770 train_time:95828ms step_avg:97.88ms
step:980/1770 train_time:95929ms step_avg:97.89ms
step:981/1770 train_time:96031ms step_avg:97.89ms
step:982/1770 train_time:96132ms step_avg:97.89ms
step:983/1770 train_time:96234ms step_avg:97.90ms
step:984/1770 train_time:96337ms step_avg:97.90ms
step:985/1770 train_time:96442ms step_avg:97.91ms
step:986/1770 train_time:96544ms step_avg:97.91ms
step:987/1770 train_time:96647ms step_avg:97.92ms
step:988/1770 train_time:96748ms step_avg:97.92ms
step:989/1770 train_time:96851ms step_avg:97.93ms
step:990/1770 train_time:96953ms step_avg:97.93ms
step:991/1770 train_time:97054ms step_avg:97.94ms
step:992/1770 train_time:97155ms step_avg:97.94ms
step:993/1770 train_time:97258ms step_avg:97.94ms
step:994/1770 train_time:97361ms step_avg:97.95ms
step:995/1770 train_time:97463ms step_avg:97.95ms
step:996/1770 train_time:97565ms step_avg:97.96ms
step:997/1770 train_time:97667ms step_avg:97.96ms
step:998/1770 train_time:97769ms step_avg:97.96ms
step:999/1770 train_time:97870ms step_avg:97.97ms
step:1000/1770 train_time:97972ms step_avg:97.97ms
step:1000/1770 val_loss:3.5132 train_time:98073ms step_avg:98.07ms
step:1001/1770 train_time:98091ms step_avg:97.99ms
step:1002/1770 train_time:98185ms step_avg:97.99ms
step:1003/1770 train_time:98286ms step_avg:97.99ms
step:1004/1770 train_time:98388ms step_avg:98.00ms
step:1005/1770 train_time:98489ms step_avg:98.00ms
step:1006/1770 train_time:98590ms step_avg:98.00ms
step:1007/1770 train_time:98691ms step_avg:98.00ms
step:1008/1770 train_time:98792ms step_avg:98.01ms
step:1009/1770 train_time:98893ms step_avg:98.01ms
step:1010/1770 train_time:98994ms step_avg:98.01ms
step:1011/1770 train_time:99100ms step_avg:98.02ms
step:1012/1770 train_time:99204ms step_avg:98.03ms
step:1013/1770 train_time:99306ms step_avg:98.03ms
step:1014/1770 train_time:99407ms step_avg:98.03ms
step:1015/1770 train_time:99508ms step_avg:98.04ms
step:1016/1770 train_time:99611ms step_avg:98.04ms
step:1017/1770 train_time:99712ms step_avg:98.05ms
step:1018/1770 train_time:99813ms step_avg:98.05ms
step:1019/1770 train_time:99914ms step_avg:98.05ms
step:1020/1770 train_time:100017ms step_avg:98.06ms
step:1021/1770 train_time:100121ms step_avg:98.06ms
step:1022/1770 train_time:100226ms step_avg:98.07ms
step:1023/1770 train_time:100328ms step_avg:98.07ms
step:1024/1770 train_time:100430ms step_avg:98.08ms
step:1025/1770 train_time:100532ms step_avg:98.08ms
step:1026/1770 train_time:100634ms step_avg:98.08ms
step:1027/1770 train_time:100735ms step_avg:98.09ms
step:1028/1770 train_time:100836ms step_avg:98.09ms
step:1029/1770 train_time:100938ms step_avg:98.09ms
step:1030/1770 train_time:101041ms step_avg:98.10ms
step:1031/1770 train_time:101143ms step_avg:98.10ms
step:1032/1770 train_time:101245ms step_avg:98.11ms
step:1033/1770 train_time:101347ms step_avg:98.11ms
step:1034/1770 train_time:101449ms step_avg:98.11ms
step:1035/1770 train_time:101550ms step_avg:98.12ms
step:1036/1770 train_time:101652ms step_avg:98.12ms
step:1037/1770 train_time:101755ms step_avg:98.12ms
step:1038/1770 train_time:101857ms step_avg:98.13ms
step:1039/1770 train_time:101959ms step_avg:98.13ms
step:1040/1770 train_time:102060ms step_avg:98.13ms
step:1041/1770 train_time:102163ms step_avg:98.14ms
step:1042/1770 train_time:102266ms step_avg:98.14ms
step:1043/1770 train_time:102367ms step_avg:98.15ms
step:1044/1770 train_time:102469ms step_avg:98.15ms
step:1045/1770 train_time:102570ms step_avg:98.15ms
step:1046/1770 train_time:102672ms step_avg:98.16ms
step:1047/1770 train_time:102773ms step_avg:98.16ms
step:1048/1770 train_time:102876ms step_avg:98.16ms
step:1049/1770 train_time:102978ms step_avg:98.17ms
step:1050/1770 train_time:103082ms step_avg:98.17ms
step:1051/1770 train_time:103184ms step_avg:98.18ms
step:1052/1770 train_time:103286ms step_avg:98.18ms
step:1053/1770 train_time:103387ms step_avg:98.18ms
step:1054/1770 train_time:103489ms step_avg:98.19ms
step:1055/1770 train_time:103591ms step_avg:98.19ms
step:1056/1770 train_time:103692ms step_avg:98.19ms
step:1057/1770 train_time:103794ms step_avg:98.20ms
step:1058/1770 train_time:103898ms step_avg:98.20ms
step:1059/1770 train_time:104002ms step_avg:98.21ms
step:1060/1770 train_time:104104ms step_avg:98.21ms
step:1061/1770 train_time:104206ms step_avg:98.22ms
step:1062/1770 train_time:104308ms step_avg:98.22ms
step:1063/1770 train_time:104411ms step_avg:98.22ms
step:1064/1770 train_time:104514ms step_avg:98.23ms
step:1065/1770 train_time:104616ms step_avg:98.23ms
step:1066/1770 train_time:104717ms step_avg:98.23ms
step:1067/1770 train_time:104819ms step_avg:98.24ms
step:1068/1770 train_time:104922ms step_avg:98.24ms
step:1069/1770 train_time:105024ms step_avg:98.25ms
step:1070/1770 train_time:105126ms step_avg:98.25ms
step:1071/1770 train_time:105229ms step_avg:98.25ms
step:1072/1770 train_time:105331ms step_avg:98.26ms
step:1073/1770 train_time:105432ms step_avg:98.26ms
step:1074/1770 train_time:105535ms step_avg:98.26ms
step:1075/1770 train_time:105638ms step_avg:98.27ms
step:1076/1770 train_time:105740ms step_avg:98.27ms
step:1077/1770 train_time:105843ms step_avg:98.28ms
step:1078/1770 train_time:105944ms step_avg:98.28ms
step:1079/1770 train_time:106047ms step_avg:98.28ms
step:1080/1770 train_time:106149ms step_avg:98.29ms
step:1081/1770 train_time:106251ms step_avg:98.29ms
step:1082/1770 train_time:106353ms step_avg:98.29ms
step:1083/1770 train_time:106455ms step_avg:98.30ms
step:1084/1770 train_time:106557ms step_avg:98.30ms
step:1085/1770 train_time:106659ms step_avg:98.30ms
step:1086/1770 train_time:106762ms step_avg:98.31ms
step:1087/1770 train_time:106863ms step_avg:98.31ms
step:1088/1770 train_time:106964ms step_avg:98.31ms
step:1089/1770 train_time:107065ms step_avg:98.32ms
step:1090/1770 train_time:107168ms step_avg:98.32ms
step:1091/1770 train_time:107269ms step_avg:98.32ms
step:1092/1770 train_time:107372ms step_avg:98.33ms
step:1093/1770 train_time:107474ms step_avg:98.33ms
step:1094/1770 train_time:107577ms step_avg:98.33ms
step:1095/1770 train_time:107680ms step_avg:98.34ms
step:1096/1770 train_time:107783ms step_avg:98.34ms
step:1097/1770 train_time:107885ms step_avg:98.35ms
step:1098/1770 train_time:107987ms step_avg:98.35ms
step:1099/1770 train_time:108088ms step_avg:98.35ms
step:1100/1770 train_time:108190ms step_avg:98.35ms
step:1101/1770 train_time:108293ms step_avg:98.36ms
step:1102/1770 train_time:108395ms step_avg:98.36ms
step:1103/1770 train_time:108497ms step_avg:98.37ms
step:1104/1770 train_time:108600ms step_avg:98.37ms
step:1105/1770 train_time:108702ms step_avg:98.37ms
step:1106/1770 train_time:108805ms step_avg:98.38ms
step:1107/1770 train_time:108907ms step_avg:98.38ms
step:1108/1770 train_time:109008ms step_avg:98.38ms
step:1109/1770 train_time:109110ms step_avg:98.39ms
step:1110/1770 train_time:109211ms step_avg:98.39ms
step:1111/1770 train_time:109313ms step_avg:98.39ms
step:1112/1770 train_time:109416ms step_avg:98.40ms
step:1113/1770 train_time:109518ms step_avg:98.40ms
step:1114/1770 train_time:109621ms step_avg:98.40ms
step:1115/1770 train_time:109724ms step_avg:98.41ms
step:1116/1770 train_time:109826ms step_avg:98.41ms
step:1117/1770 train_time:109928ms step_avg:98.41ms
step:1118/1770 train_time:110029ms step_avg:98.42ms
step:1119/1770 train_time:110131ms step_avg:98.42ms
step:1120/1770 train_time:110233ms step_avg:98.42ms
step:1121/1770 train_time:110335ms step_avg:98.43ms
step:1122/1770 train_time:110437ms step_avg:98.43ms
step:1123/1770 train_time:110540ms step_avg:98.43ms
step:1124/1770 train_time:110643ms step_avg:98.44ms
step:1125/1770 train_time:110745ms step_avg:98.44ms
step:1125/1770 val_loss:3.4741 train_time:110845ms step_avg:98.53ms
step:1126/1770 train_time:110863ms step_avg:98.46ms
step:1127/1770 train_time:110960ms step_avg:98.46ms
step:1128/1770 train_time:111063ms step_avg:98.46ms
step:1129/1770 train_time:111165ms step_avg:98.46ms
step:1130/1770 train_time:111267ms step_avg:98.47ms
step:1131/1770 train_time:111369ms step_avg:98.47ms
step:1132/1770 train_time:111470ms step_avg:98.47ms
step:1133/1770 train_time:111571ms step_avg:98.47ms
step:1134/1770 train_time:111672ms step_avg:98.48ms
step:1135/1770 train_time:111774ms step_avg:98.48ms
step:1136/1770 train_time:111879ms step_avg:98.48ms
step:1137/1770 train_time:111983ms step_avg:98.49ms
step:1138/1770 train_time:112086ms step_avg:98.49ms
step:1139/1770 train_time:112188ms step_avg:98.50ms
step:1140/1770 train_time:112290ms step_avg:98.50ms
step:1141/1770 train_time:112392ms step_avg:98.50ms
step:1142/1770 train_time:112494ms step_avg:98.51ms
step:1143/1770 train_time:112596ms step_avg:98.51ms
step:1144/1770 train_time:112697ms step_avg:98.51ms
step:1145/1770 train_time:112800ms step_avg:98.52ms
step:1146/1770 train_time:112903ms step_avg:98.52ms
step:1147/1770 train_time:113006ms step_avg:98.52ms
step:1148/1770 train_time:113108ms step_avg:98.53ms
step:1149/1770 train_time:113210ms step_avg:98.53ms
step:1150/1770 train_time:113311ms step_avg:98.53ms
step:1151/1770 train_time:113415ms step_avg:98.54ms
step:1152/1770 train_time:113518ms step_avg:98.54ms
step:1153/1770 train_time:113620ms step_avg:98.54ms
step:1154/1770 train_time:113722ms step_avg:98.55ms
step:1155/1770 train_time:113824ms step_avg:98.55ms
step:1156/1770 train_time:113927ms step_avg:98.55ms
step:1157/1770 train_time:114030ms step_avg:98.56ms
step:1158/1770 train_time:114132ms step_avg:98.56ms
step:1159/1770 train_time:114235ms step_avg:98.56ms
step:1160/1770 train_time:114336ms step_avg:98.57ms
step:1161/1770 train_time:114439ms step_avg:98.57ms
step:1162/1770 train_time:114542ms step_avg:98.57ms
step:1163/1770 train_time:114643ms step_avg:98.58ms
step:1164/1770 train_time:114744ms step_avg:98.58ms
step:1165/1770 train_time:114846ms step_avg:98.58ms
step:1166/1770 train_time:114948ms step_avg:98.58ms
step:1167/1770 train_time:115050ms step_avg:98.59ms
step:1168/1770 train_time:115153ms step_avg:98.59ms
step:1169/1770 train_time:115255ms step_avg:98.59ms
step:1170/1770 train_time:115357ms step_avg:98.60ms
step:1171/1770 train_time:115459ms step_avg:98.60ms
step:1172/1770 train_time:115562ms step_avg:98.60ms
step:1173/1770 train_time:115663ms step_avg:98.60ms
step:1174/1770 train_time:115765ms step_avg:98.61ms
step:1175/1770 train_time:115867ms step_avg:98.61ms
step:1176/1770 train_time:115969ms step_avg:98.61ms
step:1177/1770 train_time:116072ms step_avg:98.62ms
step:1178/1770 train_time:116175ms step_avg:98.62ms
step:1179/1770 train_time:116277ms step_avg:98.62ms
step:1180/1770 train_time:116380ms step_avg:98.63ms
step:1181/1770 train_time:116482ms step_avg:98.63ms
step:1182/1770 train_time:116585ms step_avg:98.63ms
step:1183/1770 train_time:116687ms step_avg:98.64ms
step:1184/1770 train_time:116792ms step_avg:98.64ms
step:1185/1770 train_time:116895ms step_avg:98.65ms
step:1186/1770 train_time:117000ms step_avg:98.65ms
step:1187/1770 train_time:117105ms step_avg:98.66ms
step:1188/1770 train_time:117207ms step_avg:98.66ms
step:1189/1770 train_time:117310ms step_avg:98.66ms
step:1190/1770 train_time:117414ms step_avg:98.67ms
step:1191/1770 train_time:117519ms step_avg:98.67ms
step:1192/1770 train_time:117623ms step_avg:98.68ms
step:1193/1770 train_time:117725ms step_avg:98.68ms
step:1194/1770 train_time:117828ms step_avg:98.68ms
step:1195/1770 train_time:117931ms step_avg:98.69ms
step:1196/1770 train_time:118035ms step_avg:98.69ms
step:1197/1770 train_time:118140ms step_avg:98.70ms
step:1198/1770 train_time:118243ms step_avg:98.70ms
step:1199/1770 train_time:118347ms step_avg:98.70ms
step:1200/1770 train_time:118452ms step_avg:98.71ms
step:1201/1770 train_time:118556ms step_avg:98.71ms
step:1202/1770 train_time:118659ms step_avg:98.72ms
step:1203/1770 train_time:118762ms step_avg:98.72ms
step:1204/1770 train_time:118866ms step_avg:98.73ms
step:1205/1770 train_time:118969ms step_avg:98.73ms
step:1206/1770 train_time:119073ms step_avg:98.73ms
step:1207/1770 train_time:119177ms step_avg:98.74ms
step:1208/1770 train_time:119281ms step_avg:98.74ms
step:1209/1770 train_time:119385ms step_avg:98.75ms
step:1210/1770 train_time:119487ms step_avg:98.75ms
step:1211/1770 train_time:119591ms step_avg:98.75ms
step:1212/1770 train_time:119696ms step_avg:98.76ms
step:1213/1770 train_time:119799ms step_avg:98.76ms
step:1214/1770 train_time:119903ms step_avg:98.77ms
step:1215/1770 train_time:120005ms step_avg:98.77ms
step:1216/1770 train_time:120110ms step_avg:98.77ms
step:1217/1770 train_time:120213ms step_avg:98.78ms
step:1218/1770 train_time:120317ms step_avg:98.78ms
step:1219/1770 train_time:120421ms step_avg:98.79ms
step:1220/1770 train_time:120525ms step_avg:98.79ms
step:1221/1770 train_time:120628ms step_avg:98.79ms
step:1222/1770 train_time:120733ms step_avg:98.80ms
step:1223/1770 train_time:120835ms step_avg:98.80ms
step:1224/1770 train_time:120941ms step_avg:98.81ms
step:1225/1770 train_time:121045ms step_avg:98.81ms
step:1226/1770 train_time:121148ms step_avg:98.82ms
step:1227/1770 train_time:121253ms step_avg:98.82ms
step:1228/1770 train_time:121359ms step_avg:98.83ms
step:1229/1770 train_time:121462ms step_avg:98.83ms
step:1230/1770 train_time:121564ms step_avg:98.83ms
step:1231/1770 train_time:121667ms step_avg:98.84ms
step:1232/1770 train_time:121770ms step_avg:98.84ms
step:1233/1770 train_time:121873ms step_avg:98.84ms
step:1234/1770 train_time:121977ms step_avg:98.85ms
step:1235/1770 train_time:122081ms step_avg:98.85ms
step:1236/1770 train_time:122185ms step_avg:98.86ms
step:1237/1770 train_time:122289ms step_avg:98.86ms
step:1238/1770 train_time:122394ms step_avg:98.86ms
step:1239/1770 train_time:122496ms step_avg:98.87ms
step:1240/1770 train_time:122600ms step_avg:98.87ms
step:1241/1770 train_time:122705ms step_avg:98.88ms
step:1242/1770 train_time:122807ms step_avg:98.88ms
step:1243/1770 train_time:122911ms step_avg:98.88ms
step:1244/1770 train_time:123013ms step_avg:98.89ms
step:1245/1770 train_time:123117ms step_avg:98.89ms
step:1246/1770 train_time:123222ms step_avg:98.89ms
step:1247/1770 train_time:123324ms step_avg:98.90ms
step:1248/1770 train_time:123428ms step_avg:98.90ms
step:1249/1770 train_time:123531ms step_avg:98.90ms
step:1250/1770 train_time:123634ms step_avg:98.91ms
step:1250/1770 val_loss:3.4259 train_time:123737ms step_avg:98.99ms
step:1251/1770 train_time:123755ms step_avg:98.92ms
step:1252/1770 train_time:123846ms step_avg:98.92ms
step:1253/1770 train_time:123952ms step_avg:98.92ms
step:1254/1770 train_time:124055ms step_avg:98.93ms
step:1255/1770 train_time:124160ms step_avg:98.93ms
step:1256/1770 train_time:124262ms step_avg:98.93ms
step:1257/1770 train_time:124364ms step_avg:98.94ms
step:1258/1770 train_time:124468ms step_avg:98.94ms
step:1259/1770 train_time:124571ms step_avg:98.94ms
step:1260/1770 train_time:124675ms step_avg:98.95ms
step:1261/1770 train_time:124780ms step_avg:98.95ms
step:1262/1770 train_time:124885ms step_avg:98.96ms
step:1263/1770 train_time:124989ms step_avg:98.96ms
step:1264/1770 train_time:125093ms step_avg:98.97ms
step:1265/1770 train_time:125196ms step_avg:98.97ms
step:1266/1770 train_time:125299ms step_avg:98.97ms
step:1267/1770 train_time:125403ms step_avg:98.98ms
step:1268/1770 train_time:125505ms step_avg:98.98ms
step:1269/1770 train_time:125610ms step_avg:98.98ms
step:1270/1770 train_time:125713ms step_avg:98.99ms
step:1271/1770 train_time:125816ms step_avg:98.99ms
step:1272/1770 train_time:125919ms step_avg:98.99ms
step:1273/1770 train_time:126023ms step_avg:99.00ms
step:1274/1770 train_time:126127ms step_avg:99.00ms
step:1275/1770 train_time:126231ms step_avg:99.00ms
step:1276/1770 train_time:126334ms step_avg:99.01ms
step:1277/1770 train_time:126436ms step_avg:99.01ms
step:1278/1770 train_time:126540ms step_avg:99.01ms
step:1279/1770 train_time:126643ms step_avg:99.02ms
step:1280/1770 train_time:126748ms step_avg:99.02ms
step:1281/1770 train_time:126851ms step_avg:99.03ms
step:1282/1770 train_time:126955ms step_avg:99.03ms
step:1283/1770 train_time:127058ms step_avg:99.03ms
step:1284/1770 train_time:127162ms step_avg:99.04ms
step:1285/1770 train_time:127266ms step_avg:99.04ms
step:1286/1770 train_time:127370ms step_avg:99.04ms
step:1287/1770 train_time:127474ms step_avg:99.05ms
step:1288/1770 train_time:127578ms step_avg:99.05ms
step:1289/1770 train_time:127681ms step_avg:99.05ms
step:1290/1770 train_time:127784ms step_avg:99.06ms
step:1291/1770 train_time:127888ms step_avg:99.06ms
step:1292/1770 train_time:127992ms step_avg:99.07ms
step:1293/1770 train_time:128096ms step_avg:99.07ms
step:1294/1770 train_time:128199ms step_avg:99.07ms
step:1295/1770 train_time:128303ms step_avg:99.08ms
step:1296/1770 train_time:128406ms step_avg:99.08ms
step:1297/1770 train_time:128511ms step_avg:99.08ms
step:1298/1770 train_time:128615ms step_avg:99.09ms
step:1299/1770 train_time:128718ms step_avg:99.09ms
step:1300/1770 train_time:128822ms step_avg:99.09ms
step:1301/1770 train_time:128925ms step_avg:99.10ms
step:1302/1770 train_time:129029ms step_avg:99.10ms
step:1303/1770 train_time:129133ms step_avg:99.10ms
step:1304/1770 train_time:129237ms step_avg:99.11ms
step:1305/1770 train_time:129340ms step_avg:99.11ms
step:1306/1770 train_time:129443ms step_avg:99.11ms
step:1307/1770 train_time:129547ms step_avg:99.12ms
step:1308/1770 train_time:129651ms step_avg:99.12ms
step:1309/1770 train_time:129756ms step_avg:99.13ms
step:1310/1770 train_time:129859ms step_avg:99.13ms
step:1311/1770 train_time:129962ms step_avg:99.13ms
step:1312/1770 train_time:130066ms step_avg:99.14ms
step:1313/1770 train_time:130170ms step_avg:99.14ms
step:1314/1770 train_time:130273ms step_avg:99.14ms
step:1315/1770 train_time:130376ms step_avg:99.15ms
step:1316/1770 train_time:130479ms step_avg:99.15ms
step:1317/1770 train_time:130583ms step_avg:99.15ms
step:1318/1770 train_time:130691ms step_avg:99.16ms
step:1319/1770 train_time:130795ms step_avg:99.16ms
step:1320/1770 train_time:130897ms step_avg:99.16ms
step:1321/1770 train_time:131000ms step_avg:99.17ms
step:1322/1770 train_time:131104ms step_avg:99.17ms
step:1323/1770 train_time:131208ms step_avg:99.17ms
step:1324/1770 train_time:131313ms step_avg:99.18ms
step:1325/1770 train_time:131417ms step_avg:99.18ms
step:1326/1770 train_time:131520ms step_avg:99.19ms
step:1327/1770 train_time:131625ms step_avg:99.19ms
step:1328/1770 train_time:131728ms step_avg:99.19ms
step:1329/1770 train_time:131832ms step_avg:99.20ms
step:1330/1770 train_time:131936ms step_avg:99.20ms
step:1331/1770 train_time:132039ms step_avg:99.20ms
step:1332/1770 train_time:132142ms step_avg:99.21ms
step:1333/1770 train_time:132246ms step_avg:99.21ms
step:1334/1770 train_time:132351ms step_avg:99.21ms
step:1335/1770 train_time:132454ms step_avg:99.22ms
step:1336/1770 train_time:132557ms step_avg:99.22ms
step:1337/1770 train_time:132661ms step_avg:99.22ms
step:1338/1770 train_time:132764ms step_avg:99.23ms
step:1339/1770 train_time:132870ms step_avg:99.23ms
step:1340/1770 train_time:132974ms step_avg:99.23ms
step:1341/1770 train_time:133076ms step_avg:99.24ms
step:1342/1770 train_time:133181ms step_avg:99.24ms
step:1343/1770 train_time:133286ms step_avg:99.24ms
step:1344/1770 train_time:133391ms step_avg:99.25ms
step:1345/1770 train_time:133494ms step_avg:99.25ms
step:1346/1770 train_time:133597ms step_avg:99.26ms
step:1347/1770 train_time:133700ms step_avg:99.26ms
step:1348/1770 train_time:133807ms step_avg:99.26ms
step:1349/1770 train_time:133912ms step_avg:99.27ms
step:1350/1770 train_time:134014ms step_avg:99.27ms
step:1351/1770 train_time:134118ms step_avg:99.27ms
step:1352/1770 train_time:134222ms step_avg:99.28ms
step:1353/1770 train_time:134326ms step_avg:99.28ms
step:1354/1770 train_time:134431ms step_avg:99.28ms
step:1355/1770 train_time:134534ms step_avg:99.29ms
step:1356/1770 train_time:134637ms step_avg:99.29ms
step:1357/1770 train_time:134741ms step_avg:99.29ms
step:1358/1770 train_time:134846ms step_avg:99.30ms
step:1359/1770 train_time:134949ms step_avg:99.30ms
step:1360/1770 train_time:135055ms step_avg:99.30ms
step:1361/1770 train_time:135158ms step_avg:99.31ms
step:1362/1770 train_time:135261ms step_avg:99.31ms
step:1363/1770 train_time:135365ms step_avg:99.31ms
step:1364/1770 train_time:135471ms step_avg:99.32ms
step:1365/1770 train_time:135575ms step_avg:99.32ms
step:1366/1770 train_time:135677ms step_avg:99.32ms
step:1367/1770 train_time:135782ms step_avg:99.33ms
step:1368/1770 train_time:135885ms step_avg:99.33ms
step:1369/1770 train_time:135989ms step_avg:99.33ms
step:1370/1770 train_time:136093ms step_avg:99.34ms
step:1371/1770 train_time:136196ms step_avg:99.34ms
step:1372/1770 train_time:136299ms step_avg:99.34ms
step:1373/1770 train_time:136403ms step_avg:99.35ms
step:1374/1770 train_time:136508ms step_avg:99.35ms
step:1375/1770 train_time:136612ms step_avg:99.35ms
step:1375/1770 val_loss:3.3827 train_time:136715ms step_avg:99.43ms
step:1376/1770 train_time:136733ms step_avg:99.37ms
step:1377/1770 train_time:136828ms step_avg:99.37ms
step:1378/1770 train_time:136932ms step_avg:99.37ms
step:1379/1770 train_time:137035ms step_avg:99.37ms
step:1380/1770 train_time:137138ms step_avg:99.38ms
step:1381/1770 train_time:137241ms step_avg:99.38ms
step:1382/1770 train_time:137343ms step_avg:99.38ms
step:1383/1770 train_time:137447ms step_avg:99.38ms
step:1384/1770 train_time:137550ms step_avg:99.39ms
step:1385/1770 train_time:137654ms step_avg:99.39ms
step:1386/1770 train_time:137759ms step_avg:99.39ms
step:1387/1770 train_time:137865ms step_avg:99.40ms
step:1388/1770 train_time:137968ms step_avg:99.40ms
step:1389/1770 train_time:138073ms step_avg:99.40ms
step:1390/1770 train_time:138176ms step_avg:99.41ms
step:1391/1770 train_time:138279ms step_avg:99.41ms
step:1392/1770 train_time:138382ms step_avg:99.41ms
step:1393/1770 train_time:138485ms step_avg:99.42ms
step:1394/1770 train_time:138587ms step_avg:99.42ms
step:1395/1770 train_time:138691ms step_avg:99.42ms
step:1396/1770 train_time:138797ms step_avg:99.42ms
step:1397/1770 train_time:138903ms step_avg:99.43ms
step:1398/1770 train_time:139007ms step_avg:99.43ms
step:1399/1770 train_time:139110ms step_avg:99.44ms
step:1400/1770 train_time:139215ms step_avg:99.44ms
step:1401/1770 train_time:139319ms step_avg:99.44ms
step:1402/1770 train_time:139423ms step_avg:99.45ms
step:1403/1770 train_time:139526ms step_avg:99.45ms
step:1404/1770 train_time:139630ms step_avg:99.45ms
step:1405/1770 train_time:139732ms step_avg:99.45ms
step:1406/1770 train_time:139836ms step_avg:99.46ms
step:1407/1770 train_time:139940ms step_avg:99.46ms
step:1408/1770 train_time:140045ms step_avg:99.46ms
step:1409/1770 train_time:140147ms step_avg:99.47ms
step:1410/1770 train_time:140251ms step_avg:99.47ms
step:1411/1770 train_time:140355ms step_avg:99.47ms
step:1412/1770 train_time:140459ms step_avg:99.47ms
step:1413/1770 train_time:140563ms step_avg:99.48ms
step:1414/1770 train_time:140668ms step_avg:99.48ms
step:1415/1770 train_time:140771ms step_avg:99.49ms
step:1416/1770 train_time:140876ms step_avg:99.49ms
step:1417/1770 train_time:140980ms step_avg:99.49ms
step:1418/1770 train_time:141085ms step_avg:99.50ms
step:1419/1770 train_time:141189ms step_avg:99.50ms
step:1420/1770 train_time:141292ms step_avg:99.50ms
step:1421/1770 train_time:141396ms step_avg:99.50ms
step:1422/1770 train_time:141500ms step_avg:99.51ms
step:1423/1770 train_time:141603ms step_avg:99.51ms
step:1424/1770 train_time:141706ms step_avg:99.51ms
step:1425/1770 train_time:141808ms step_avg:99.51ms
step:1426/1770 train_time:141913ms step_avg:99.52ms
step:1427/1770 train_time:142016ms step_avg:99.52ms
step:1428/1770 train_time:142120ms step_avg:99.52ms
step:1429/1770 train_time:142224ms step_avg:99.53ms
step:1430/1770 train_time:142327ms step_avg:99.53ms
step:1431/1770 train_time:142434ms step_avg:99.53ms
step:1432/1770 train_time:142537ms step_avg:99.54ms
step:1433/1770 train_time:142641ms step_avg:99.54ms
step:1434/1770 train_time:142744ms step_avg:99.54ms
step:1435/1770 train_time:142847ms step_avg:99.55ms
step:1436/1770 train_time:142953ms step_avg:99.55ms
step:1437/1770 train_time:143056ms step_avg:99.55ms
step:1438/1770 train_time:143159ms step_avg:99.55ms
step:1439/1770 train_time:143263ms step_avg:99.56ms
step:1440/1770 train_time:143367ms step_avg:99.56ms
step:1441/1770 train_time:143472ms step_avg:99.56ms
step:1442/1770 train_time:143575ms step_avg:99.57ms
step:1443/1770 train_time:143681ms step_avg:99.57ms
step:1444/1770 train_time:143785ms step_avg:99.57ms
step:1445/1770 train_time:143889ms step_avg:99.58ms
step:1446/1770 train_time:143993ms step_avg:99.58ms
step:1447/1770 train_time:144098ms step_avg:99.58ms
step:1448/1770 train_time:144203ms step_avg:99.59ms
step:1449/1770 train_time:144308ms step_avg:99.59ms
step:1450/1770 train_time:144412ms step_avg:99.59ms
step:1451/1770 train_time:144517ms step_avg:99.60ms
step:1452/1770 train_time:144621ms step_avg:99.60ms
step:1453/1770 train_time:144726ms step_avg:99.60ms
step:1454/1770 train_time:144830ms step_avg:99.61ms
step:1455/1770 train_time:144935ms step_avg:99.61ms
step:1456/1770 train_time:145040ms step_avg:99.62ms
step:1457/1770 train_time:145145ms step_avg:99.62ms
step:1458/1770 train_time:145250ms step_avg:99.62ms
step:1459/1770 train_time:145356ms step_avg:99.63ms
step:1460/1770 train_time:145460ms step_avg:99.63ms
step:1461/1770 train_time:145564ms step_avg:99.63ms
step:1462/1770 train_time:145669ms step_avg:99.64ms
step:1463/1770 train_time:145774ms step_avg:99.64ms
step:1464/1770 train_time:145879ms step_avg:99.64ms
step:1465/1770 train_time:145983ms step_avg:99.65ms
step:1466/1770 train_time:146088ms step_avg:99.65ms
step:1467/1770 train_time:146193ms step_avg:99.65ms
step:1468/1770 train_time:146298ms step_avg:99.66ms
step:1469/1770 train_time:146403ms step_avg:99.66ms
step:1470/1770 train_time:146507ms step_avg:99.66ms
step:1471/1770 train_time:146612ms step_avg:99.67ms
step:1472/1770 train_time:146717ms step_avg:99.67ms
step:1473/1770 train_time:146823ms step_avg:99.68ms
step:1474/1770 train_time:146928ms step_avg:99.68ms
step:1475/1770 train_time:147033ms step_avg:99.68ms
step:1476/1770 train_time:147138ms step_avg:99.69ms
step:1477/1770 train_time:147245ms step_avg:99.69ms
step:1478/1770 train_time:147350ms step_avg:99.70ms
step:1479/1770 train_time:147455ms step_avg:99.70ms
step:1480/1770 train_time:147560ms step_avg:99.70ms
step:1481/1770 train_time:147669ms step_avg:99.71ms
step:1482/1770 train_time:147773ms step_avg:99.71ms
step:1483/1770 train_time:147877ms step_avg:99.71ms
step:1484/1770 train_time:147982ms step_avg:99.72ms
step:1485/1770 train_time:148086ms step_avg:99.72ms
step:1486/1770 train_time:148190ms step_avg:99.72ms
step:1487/1770 train_time:148295ms step_avg:99.73ms
step:1488/1770 train_time:148401ms step_avg:99.73ms
step:1489/1770 train_time:148507ms step_avg:99.74ms
step:1490/1770 train_time:148613ms step_avg:99.74ms
step:1491/1770 train_time:148717ms step_avg:99.74ms
step:1492/1770 train_time:148822ms step_avg:99.75ms
step:1493/1770 train_time:148928ms step_avg:99.75ms
step:1494/1770 train_time:149035ms step_avg:99.76ms
step:1495/1770 train_time:149140ms step_avg:99.76ms
step:1496/1770 train_time:149244ms step_avg:99.76ms
step:1497/1770 train_time:149349ms step_avg:99.77ms
step:1498/1770 train_time:149453ms step_avg:99.77ms
step:1499/1770 train_time:149557ms step_avg:99.77ms
step:1500/1770 train_time:149663ms step_avg:99.78ms
step:1500/1770 val_loss:3.3444 train_time:149766ms step_avg:99.84ms
step:1501/1770 train_time:149784ms step_avg:99.79ms
step:1502/1770 train_time:149880ms step_avg:99.79ms
step:1503/1770 train_time:149984ms step_avg:99.79ms
step:1504/1770 train_time:150088ms step_avg:99.79ms
step:1505/1770 train_time:150194ms step_avg:99.80ms
step:1506/1770 train_time:150298ms step_avg:99.80ms
step:1507/1770 train_time:150403ms step_avg:99.80ms
step:1508/1770 train_time:150508ms step_avg:99.81ms
step:1509/1770 train_time:150612ms step_avg:99.81ms
step:1510/1770 train_time:150716ms step_avg:99.81ms
step:1511/1770 train_time:150823ms step_avg:99.82ms
step:1512/1770 train_time:150929ms step_avg:99.82ms
step:1513/1770 train_time:151034ms step_avg:99.82ms
step:1514/1770 train_time:151138ms step_avg:99.83ms
step:1515/1770 train_time:151242ms step_avg:99.83ms
step:1516/1770 train_time:151347ms step_avg:99.83ms
step:1517/1770 train_time:151451ms step_avg:99.84ms
step:1518/1770 train_time:151558ms step_avg:99.84ms
step:1519/1770 train_time:151661ms step_avg:99.84ms
step:1520/1770 train_time:151766ms step_avg:99.85ms
step:1521/1770 train_time:151871ms step_avg:99.85ms
step:1522/1770 train_time:151977ms step_avg:99.85ms
step:1523/1770 train_time:152082ms step_avg:99.86ms
step:1524/1770 train_time:152187ms step_avg:99.86ms
step:1525/1770 train_time:152291ms step_avg:99.86ms
step:1526/1770 train_time:152395ms step_avg:99.87ms
step:1527/1770 train_time:152499ms step_avg:99.87ms
step:1528/1770 train_time:152605ms step_avg:99.87ms
step:1529/1770 train_time:152709ms step_avg:99.88ms
step:1530/1770 train_time:152814ms step_avg:99.88ms
step:1531/1770 train_time:152919ms step_avg:99.88ms
step:1532/1770 train_time:153025ms step_avg:99.89ms
step:1533/1770 train_time:153129ms step_avg:99.89ms
step:1534/1770 train_time:153235ms step_avg:99.89ms
step:1535/1770 train_time:153339ms step_avg:99.90ms
step:1536/1770 train_time:153443ms step_avg:99.90ms
step:1537/1770 train_time:153549ms step_avg:99.90ms
step:1538/1770 train_time:153655ms step_avg:99.91ms
step:1539/1770 train_time:153760ms step_avg:99.91ms
step:1540/1770 train_time:153866ms step_avg:99.91ms
step:1541/1770 train_time:153973ms step_avg:99.92ms
step:1542/1770 train_time:154077ms step_avg:99.92ms
step:1543/1770 train_time:154182ms step_avg:99.92ms
step:1544/1770 train_time:154287ms step_avg:99.93ms
step:1545/1770 train_time:154391ms step_avg:99.93ms
step:1546/1770 train_time:154497ms step_avg:99.93ms
step:1547/1770 train_time:154602ms step_avg:99.94ms
step:1548/1770 train_time:154706ms step_avg:99.94ms
step:1549/1770 train_time:154811ms step_avg:99.94ms
step:1550/1770 train_time:154916ms step_avg:99.95ms
step:1551/1770 train_time:155020ms step_avg:99.95ms
step:1552/1770 train_time:155126ms step_avg:99.95ms
step:1553/1770 train_time:155230ms step_avg:99.95ms
step:1554/1770 train_time:155334ms step_avg:99.96ms
step:1555/1770 train_time:155440ms step_avg:99.96ms
step:1556/1770 train_time:155544ms step_avg:99.96ms
step:1557/1770 train_time:155648ms step_avg:99.97ms
step:1558/1770 train_time:155753ms step_avg:99.97ms
step:1559/1770 train_time:155858ms step_avg:99.97ms
step:1560/1770 train_time:155963ms step_avg:99.98ms
step:1561/1770 train_time:156070ms step_avg:99.98ms
step:1562/1770 train_time:156174ms step_avg:99.98ms
step:1563/1770 train_time:156279ms step_avg:99.99ms
step:1564/1770 train_time:156383ms step_avg:99.99ms
step:1565/1770 train_time:156487ms step_avg:99.99ms
step:1566/1770 train_time:156591ms step_avg:99.99ms
step:1567/1770 train_time:156695ms step_avg:100.00ms
step:1568/1770 train_time:156800ms step_avg:100.00ms
step:1569/1770 train_time:156907ms step_avg:100.00ms
step:1570/1770 train_time:157012ms step_avg:100.01ms
step:1571/1770 train_time:157116ms step_avg:100.01ms
step:1572/1770 train_time:157222ms step_avg:100.01ms
step:1573/1770 train_time:157329ms step_avg:100.02ms
step:1574/1770 train_time:157433ms step_avg:100.02ms
step:1575/1770 train_time:157537ms step_avg:100.02ms
step:1576/1770 train_time:157641ms step_avg:100.03ms
step:1577/1770 train_time:157746ms step_avg:100.03ms
step:1578/1770 train_time:157852ms step_avg:100.03ms
step:1579/1770 train_time:157955ms step_avg:100.04ms
step:1580/1770 train_time:158060ms step_avg:100.04ms
step:1581/1770 train_time:158166ms step_avg:100.04ms
step:1582/1770 train_time:158273ms step_avg:100.05ms
step:1583/1770 train_time:158377ms step_avg:100.05ms
step:1584/1770 train_time:158483ms step_avg:100.05ms
step:1585/1770 train_time:158588ms step_avg:100.06ms
step:1586/1770 train_time:158696ms step_avg:100.06ms
step:1587/1770 train_time:158801ms step_avg:100.06ms
step:1588/1770 train_time:158906ms step_avg:100.07ms
step:1589/1770 train_time:159011ms step_avg:100.07ms
step:1590/1770 train_time:159115ms step_avg:100.07ms
step:1591/1770 train_time:159221ms step_avg:100.08ms
step:1592/1770 train_time:159326ms step_avg:100.08ms
step:1593/1770 train_time:159431ms step_avg:100.08ms
step:1594/1770 train_time:159535ms step_avg:100.08ms
step:1595/1770 train_time:159641ms step_avg:100.09ms
step:1596/1770 train_time:159746ms step_avg:100.09ms
step:1597/1770 train_time:159850ms step_avg:100.09ms
step:1598/1770 train_time:159955ms step_avg:100.10ms
step:1599/1770 train_time:160061ms step_avg:100.10ms
step:1600/1770 train_time:160168ms step_avg:100.10ms
step:1601/1770 train_time:160273ms step_avg:100.11ms
step:1602/1770 train_time:160379ms step_avg:100.11ms
step:1603/1770 train_time:160484ms step_avg:100.11ms
step:1604/1770 train_time:160588ms step_avg:100.12ms
step:1605/1770 train_time:160692ms step_avg:100.12ms
step:1606/1770 train_time:160797ms step_avg:100.12ms
step:1607/1770 train_time:160905ms step_avg:100.13ms
step:1608/1770 train_time:161010ms step_avg:100.13ms
step:1609/1770 train_time:161114ms step_avg:100.13ms
step:1610/1770 train_time:161219ms step_avg:100.14ms
step:1611/1770 train_time:161326ms step_avg:100.14ms
step:1612/1770 train_time:161431ms step_avg:100.14ms
step:1613/1770 train_time:161536ms step_avg:100.15ms
step:1614/1770 train_time:161641ms step_avg:100.15ms
step:1615/1770 train_time:161746ms step_avg:100.15ms
step:1616/1770 train_time:161851ms step_avg:100.16ms
step:1617/1770 train_time:161958ms step_avg:100.16ms
step:1618/1770 train_time:162063ms step_avg:100.16ms
step:1619/1770 train_time:162169ms step_avg:100.17ms
step:1620/1770 train_time:162275ms step_avg:100.17ms
step:1621/1770 train_time:162379ms step_avg:100.17ms
step:1622/1770 train_time:162485ms step_avg:100.18ms
step:1623/1770 train_time:162592ms step_avg:100.18ms
step:1624/1770 train_time:162696ms step_avg:100.18ms
step:1625/1770 train_time:162801ms step_avg:100.19ms
step:1625/1770 val_loss:3.3098 train_time:162905ms step_avg:100.25ms
step:1626/1770 train_time:162923ms step_avg:100.20ms
step:1627/1770 train_time:163019ms step_avg:100.20ms
step:1628/1770 train_time:163122ms step_avg:100.20ms
step:1629/1770 train_time:163225ms step_avg:100.20ms
step:1630/1770 train_time:163330ms step_avg:100.20ms
step:1631/1770 train_time:163434ms step_avg:100.20ms
step:1632/1770 train_time:163538ms step_avg:100.21ms
step:1633/1770 train_time:163642ms step_avg:100.21ms
step:1634/1770 train_time:163746ms step_avg:100.21ms
step:1635/1770 train_time:163851ms step_avg:100.21ms
step:1636/1770 train_time:163960ms step_avg:100.22ms
step:1637/1770 train_time:164066ms step_avg:100.22ms
step:1638/1770 train_time:164169ms step_avg:100.23ms
step:1639/1770 train_time:164275ms step_avg:100.23ms
step:1640/1770 train_time:164380ms step_avg:100.23ms
step:1641/1770 train_time:164484ms step_avg:100.23ms
step:1642/1770 train_time:164587ms step_avg:100.24ms
step:1643/1770 train_time:164692ms step_avg:100.24ms
step:1644/1770 train_time:164799ms step_avg:100.24ms
step:1645/1770 train_time:164904ms step_avg:100.25ms
step:1646/1770 train_time:165010ms step_avg:100.25ms
step:1647/1770 train_time:165116ms step_avg:100.25ms
step:1648/1770 train_time:165220ms step_avg:100.25ms
step:1649/1770 train_time:165325ms step_avg:100.26ms
step:1650/1770 train_time:165429ms step_avg:100.26ms
step:1651/1770 train_time:165534ms step_avg:100.26ms
step:1652/1770 train_time:165638ms step_avg:100.27ms
step:1653/1770 train_time:165742ms step_avg:100.27ms
step:1654/1770 train_time:165850ms step_avg:100.27ms
step:1655/1770 train_time:165958ms step_avg:100.28ms
step:1656/1770 train_time:166062ms step_avg:100.28ms
step:1657/1770 train_time:166169ms step_avg:100.28ms
step:1658/1770 train_time:166275ms step_avg:100.29ms
step:1659/1770 train_time:166381ms step_avg:100.29ms
step:1660/1770 train_time:166486ms step_avg:100.29ms
step:1661/1770 train_time:166590ms step_avg:100.30ms
step:1662/1770 train_time:166696ms step_avg:100.30ms
step:1663/1770 train_time:166800ms step_avg:100.30ms
step:1664/1770 train_time:166904ms step_avg:100.30ms
step:1665/1770 train_time:167009ms step_avg:100.31ms
step:1666/1770 train_time:167116ms step_avg:100.31ms
step:1667/1770 train_time:167220ms step_avg:100.31ms
step:1668/1770 train_time:167324ms step_avg:100.31ms
step:1669/1770 train_time:167428ms step_avg:100.32ms
step:1670/1770 train_time:167533ms step_avg:100.32ms
step:1671/1770 train_time:167638ms step_avg:100.32ms
step:1672/1770 train_time:167743ms step_avg:100.32ms
step:1673/1770 train_time:167849ms step_avg:100.33ms
step:1674/1770 train_time:167953ms step_avg:100.33ms
step:1675/1770 train_time:168058ms step_avg:100.33ms
step:1676/1770 train_time:168162ms step_avg:100.34ms
step:1677/1770 train_time:168270ms step_avg:100.34ms
step:1678/1770 train_time:168374ms step_avg:100.34ms
step:1679/1770 train_time:168480ms step_avg:100.35ms
step:1680/1770 train_time:168585ms step_avg:100.35ms
step:1681/1770 train_time:168690ms step_avg:100.35ms
step:1682/1770 train_time:168797ms step_avg:100.36ms
step:1683/1770 train_time:168900ms step_avg:100.36ms
step:1684/1770 train_time:169005ms step_avg:100.36ms
step:1685/1770 train_time:169110ms step_avg:100.36ms
step:1686/1770 train_time:169217ms step_avg:100.37ms
step:1687/1770 train_time:169322ms step_avg:100.37ms
step:1688/1770 train_time:169426ms step_avg:100.37ms
step:1689/1770 train_time:169530ms step_avg:100.37ms
step:1690/1770 train_time:169635ms step_avg:100.38ms
step:1691/1770 train_time:169740ms step_avg:100.38ms
step:1692/1770 train_time:169846ms step_avg:100.38ms
step:1693/1770 train_time:169952ms step_avg:100.38ms
step:1694/1770 train_time:170056ms step_avg:100.39ms
step:1695/1770 train_time:170161ms step_avg:100.39ms
step:1696/1770 train_time:170266ms step_avg:100.39ms
step:1697/1770 train_time:170372ms step_avg:100.40ms
step:1698/1770 train_time:170477ms step_avg:100.40ms
step:1699/1770 train_time:170582ms step_avg:100.40ms
step:1700/1770 train_time:170687ms step_avg:100.40ms
step:1701/1770 train_time:170791ms step_avg:100.41ms
step:1702/1770 train_time:170897ms step_avg:100.41ms
step:1703/1770 train_time:171000ms step_avg:100.41ms
step:1704/1770 train_time:171105ms step_avg:100.41ms
step:1705/1770 train_time:171209ms step_avg:100.42ms
step:1706/1770 train_time:171313ms step_avg:100.42ms
step:1707/1770 train_time:171419ms step_avg:100.42ms
step:1708/1770 train_time:171524ms step_avg:100.42ms
step:1709/1770 train_time:171630ms step_avg:100.43ms
step:1710/1770 train_time:171740ms step_avg:100.43ms
step:1711/1770 train_time:171848ms step_avg:100.44ms
step:1712/1770 train_time:171956ms step_avg:100.44ms
step:1713/1770 train_time:172060ms step_avg:100.44ms
step:1714/1770 train_time:172166ms step_avg:100.45ms
step:1715/1770 train_time:172270ms step_avg:100.45ms
step:1716/1770 train_time:172376ms step_avg:100.45ms
step:1717/1770 train_time:172481ms step_avg:100.45ms
step:1718/1770 train_time:172586ms step_avg:100.46ms
step:1719/1770 train_time:172693ms step_avg:100.46ms
step:1720/1770 train_time:172800ms step_avg:100.46ms
step:1721/1770 train_time:172906ms step_avg:100.47ms
step:1722/1770 train_time:173014ms step_avg:100.47ms
step:1723/1770 train_time:173121ms step_avg:100.48ms
step:1724/1770 train_time:173228ms step_avg:100.48ms
step:1725/1770 train_time:173336ms step_avg:100.48ms
step:1726/1770 train_time:173443ms step_avg:100.49ms
step:1727/1770 train_time:173548ms step_avg:100.49ms
step:1728/1770 train_time:173655ms step_avg:100.49ms
step:1729/1770 train_time:173762ms step_avg:100.50ms
step:1730/1770 train_time:173868ms step_avg:100.50ms
step:1731/1770 train_time:173975ms step_avg:100.51ms
step:1732/1770 train_time:174081ms step_avg:100.51ms
step:1733/1770 train_time:174188ms step_avg:100.51ms
step:1734/1770 train_time:174293ms step_avg:100.52ms
step:1735/1770 train_time:174399ms step_avg:100.52ms
step:1736/1770 train_time:174505ms step_avg:100.52ms
step:1737/1770 train_time:174611ms step_avg:100.52ms
step:1738/1770 train_time:174717ms step_avg:100.53ms
step:1739/1770 train_time:174823ms step_avg:100.53ms
step:1740/1770 train_time:174928ms step_avg:100.53ms
step:1741/1770 train_time:175035ms step_avg:100.54ms
step:1742/1770 train_time:175143ms step_avg:100.54ms
step:1743/1770 train_time:175250ms step_avg:100.54ms
step:1744/1770 train_time:175356ms step_avg:100.55ms
step:1745/1770 train_time:175462ms step_avg:100.55ms
step:1746/1770 train_time:175570ms step_avg:100.56ms
step:1747/1770 train_time:175674ms step_avg:100.56ms
step:1748/1770 train_time:175781ms step_avg:100.56ms
step:1749/1770 train_time:175886ms step_avg:100.56ms
step:1750/1770 train_time:175992ms step_avg:100.57ms
step:1750/1770 val_loss:3.2830 train_time:176097ms step_avg:100.63ms
step:1751/1770 train_time:176115ms step_avg:100.58ms
step:1752/1770 train_time:176210ms step_avg:100.58ms
step:1753/1770 train_time:176314ms step_avg:100.58ms
step:1754/1770 train_time:176420ms step_avg:100.58ms
step:1755/1770 train_time:176525ms step_avg:100.58ms
step:1756/1770 train_time:176631ms step_avg:100.59ms
step:1757/1770 train_time:176736ms step_avg:100.59ms
step:1758/1770 train_time:176841ms step_avg:100.59ms
step:1759/1770 train_time:176947ms step_avg:100.59ms
step:1760/1770 train_time:177053ms step_avg:100.60ms
step:1761/1770 train_time:177160ms step_avg:100.60ms
step:1762/1770 train_time:177270ms step_avg:100.61ms
step:1763/1770 train_time:177374ms step_avg:100.61ms
step:1764/1770 train_time:177480ms step_avg:100.61ms
step:1765/1770 train_time:177585ms step_avg:100.61ms
step:1766/1770 train_time:177694ms step_avg:100.62ms
step:1767/1770 train_time:177798ms step_avg:100.62ms
step:1768/1770 train_time:177904ms step_avg:100.62ms
step:1769/1770 train_time:178009ms step_avg:100.63ms
step:1770/1770 train_time:178115ms step_avg:100.63ms
step:1770/1770 val_loss:3.2798 train_time:178222ms step_avg:100.69ms
peak memory allocated: 30724 MiB reserved: 46472 MiB
